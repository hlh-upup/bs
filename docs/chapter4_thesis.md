# 第4章 视觉注意力检测系统实现与测试

## 4.1 系统架构设计

### 4.1.1 总体架构设计

本系统以数字人讲课视频自动生成与质量评估为核心目标，采用前后端分离的分层架构，整体划分为四个功能层次：前端交互层、后端服务层、模型推理层与质量评估层。各层之间通过标准化接口协议进行数据传递，职责边界清晰，具备良好的可扩展性与可维护性。

【插图占位：图4-1 系统总体四层架构图（前端交互层 → 后端服务层 → 模型推理层 → 质量评估层）】

**前端交互层**负责用户操作界面的渲染与交互逻辑，提供数字人形象管理、声音克隆配置、视频生成控制及结果展示等功能页面。用户的所有操作请求均经由该层封装为标准化HTTP请求，发送至后端服务层。

**后端服务层**以RESTful风格的HTTP服务为核心，负责接收前端请求、调度推理任务、管理用户数据目录及协调各推理模型的运行。该层采用线程池机制（最大并发数为5）异步执行计算密集型推理任务，对外提供统一的API接口，并对所有推理异常进行拦截与标准化处理，返回结构化的错误响应。

**模型推理层**承载语音合成（VITS）、唇形驱动（Wav2Lip）和表情渲染（SadTalker）三个生成模型，负责将文本与人脸图像转换为完整的数字人说话视频。三个模型形成串行处理链路：文本经语音合成生成音频，再经唇形渲染与表情动画模块驱动人脸图像，最终输出具有自然表情与唇形同步的视频内容。

**质量评估层**集成了本系统的核心自研模型——多任务学习视频质量评估模型（以下简称"评估模型"），对生成视频进行多维度自动化质量评分，覆盖口型同步、表情自然度、音频质量、跨模态一致性及综合质量五个评估维度，评分结果以JSON格式返回前端并持久化存储，为系统的质量反馈闭环提供数据支撑。

### 4.1.2 技术选型说明

表4-1列出了各层次的核心技术选型及其在系统中的应用定位。

**表4-1 系统技术选型说明**

| 层次 | 技术选型 | 选型依据 |
|---|---|---|
| 前端交互层 | Vue 3 + TypeScript + Pinia + Vue Router | 渐进式框架，Composition API提升逻辑复用性；TypeScript增强类型安全；Pinia实现轻量级全局状态管理 |
| 前端网络通信 | Axios（含拦截器封装） | 支持超长超时配置（适配推理任务耗时），统一请求/响应日志，Base64文件传输封装 |
| 后端服务框架 | Flask + Flask-CORS | 轻量级Python Web框架，低耦合，易于与推理模型集成；CORS支持前后端分离部署 |
| 并发任务管理 | ThreadPoolExecutor | 适合IO/计算混合任务，最大并发数可配置，避免主线程阻塞 |
| 语音合成 | VITS（端到端TTS） | 基于变分推断与对抗训练，音质自然，支持参考音频克隆 |
| 唇形驱动 | Wav2Lip | 音视频联合训练，唇形同步精度高，支持任意人脸输入 |
| 表情渲染 | SadTalker | 基于3D面部系数驱动，输出头部姿态与表情动画更为自然 |
| 视觉特征提取 | py-feat（RetinaFace + ResNet-101） | 集成人脸检测、关键点、AU与深层视觉特征的一体化提取工具 |
| 音频特征提取 | HuBERT | 自监督语音预训练模型，语义表征能力强 |
| 关键点检测 | MediaPipe Face Mesh | 实时468点三维面部关键点，眼部区域覆盖完整 |
| 质量评估主干 | Transformer（多头注意力） | 建模时序与跨模态依赖，适合多帧序列的特征对齐 |
| 深度学习框架 | PyTorch | 动态图机制便于模型调试与实验迭代 |

### 4.1.3 系统部署方案

本系统采用单机一体化部署方案，前端开发服务与后端API服务运行于同一台配备高性能GPU的工作站上。前端服务监听本地5173端口，后端服务监听5000端口，两者之间通过本机网络进行HTTP通信。推理层所有模型（语音合成、唇形渲染、表情动画、质量评估）均加载于GPU显存中，由后端服务统一调度执行。

【插图占位：图4-2 系统部署架构图（前端/后端/模型层在同一GPU工作站上的部署示意图）】

---

## 4.2 系统实现

### 4.2.1 视频流模块实现

视频流模块是本系统的核心生成管线，负责将用户输入的文本内容与数字人形象，经由语音合成、唇形渲染、表情动画三个阶段，合成具有自然口型与表情的数字人讲课视频。该模块的整体处理流程如图4-3所示。

【插图占位：图4-3 视频生成管线流程图（文本 → VITS语音合成 → WAV音频 → Wav2Lip/SadTalker渲染 → 最终视频）】

#### 4.2.1.1 语音合成阶段

语音合成模块基于VITS（Variational Inference with adversarial learning for end-to-end Text-to-Speech）模型实现。VITS是一种端到端的文本语音合成架构，将文本编码、声码器与对抗训练一体化融合，无需额外的声学特征中间层即可直接将文本序列转换为自然音频波形。

系统支持两种语音合成路径：第一种为参考音频模式，用户上传一段目标说话人的音频样本与对应文本，系统通过声音克隆训练使模型习得说话人的音色特征，后续推理时以该音色合成目标文本对应的语音；第二种为自定义音频模式，用户直接上传录制好的音频文件，系统绕过合成环节，直接将该音频送入后续渲染阶段。

后端服务在接收到推理请求时，首先检查用户目录下是否存在参考音频文件或自定义音频文件，依据检测结果自动选择对应的推理路径，确保在不同使用场景下均能正确路由至合适的处理分支。

#### 4.2.1.2 唇形渲染与表情动画阶段

根据语音来源不同，系统提供两套渲染方案：

**Wav2Lip渲染方案**：主要应用于用户自录音频场景。Wav2Lip采用音视频联合判别器进行训练，通过同步网络（SyncNet）约束生成帧与音频的时序对齐，能够在任意人脸图像上生成与音频高度同步的唇形动作。该方案推理速度较快，适合对唇形精度要求高的应用场景。

**SadTalker渲染方案**：主要应用于VITS合成语音场景。SadTalker通过三维面部系数（3DMM系数）建模面部动态，将音频特征分别映射为表情系数与姿态系数，再经由人脸渲染网络生成视频帧。该方案能够产生更自然的头部姿态变化与面部表情，使数字人视频更具真实感。

两种渲染方案的输出均为MP4格式视频文件，存储于用户专属结果目录中，等待后续的质量评估与前端拉取。

#### 4.2.1.3 视频合成阶段

在完成数字人视频渲染后，系统提供PPT与数字人视频合成功能，支持将数字人画面以角标形式嵌入PPT演示页面视频中。合成模块支持三种模式：全页嵌入（每页PPT均叠加数字人）、选择性嵌入（用户指定特定页面嵌入数字人）、纯PPT模式（不嵌入数字人，仅合并音频与PPT图像序列）。合成任务同样通过线程池异步执行，前端通过轮询接口获取任务完成状态。

---

### 4.2.2 眼部特征模块实现

眼部特征模块（广义上的面部特征提取模块）是本系统质量评估链路的数据预处理环节，负责从输入视频中并行提取视觉深度特征、音频语义特征、面部关键点特征及面部动作单元特征，为后续的注意力检测模块提供统一的多模态输入表示。

【插图占位：图4-4 面部特征提取流程图（视频帧/音频流 → 四路并行特征提取 → 特征融合输入）】

#### 4.2.2.1 视觉深度特征提取

视觉特征提取模块采用py-feat工具链，以RetinaFace模型进行人脸检测，定位并裁剪视频帧中的人脸区域，而后使用预训练的深层卷积神经网络（ResNet-101）对人脸图像进行特征编码，输出2048维的视觉特征向量。该特征向量编码了面部的全局外观信息，包含肤色、五官比例、光照与整体面部结构等视觉语义。

#### 4.2.2.2 眼部与面部关键点特征提取

本模块采用MediaPipe Face Mesh检测器，对每一视频帧提取468个三维面部关键点坐标，覆盖眼睛（含上下眼睑、眼角、虹膜）、眉毛、鼻子、嘴唇及面部轮廓等全部区域。其中，与眼部注意力状态高度相关的关键点索引区间（#33至#133，约100个点）密集分布于眼周区域，能够精细描述睁眼程度、注视方向及眼部运动轨迹等特征。468个三维坐标（x, y, z）展开后形成1404维的关键点特征向量，保留了面部几何形状的时序动态信息，是后续注意力状态推断的重要依据。

#### 4.2.2.3 面部动作单元特征提取

面部动作单元（Action Unit，AU）是描述面部肌肉运动的标准化编码体系，基于面部动作编码系统（FACS）定义。本模块使用py-feat中的SVM检测器提取17维AU强度特征，涵盖眼睑收紧（AU46）、眉毛上扬（AU01/AU02）、嘴角上拉（AU12）等与注意力和情绪状态密切相关的肌肉动作。AU特征以帧级强度值的形式存储，与关键点特征共同构成面部运动的细粒度描述。

#### 4.2.2.4 音频特征提取

音频特征提取采用HuBERT（Hidden-Unit BERT）模型，该模型以掩码预测任务在大规模无标注语音数据上进行自监督预训练，学习到丰富的语音语义表征。输入音频以16000Hz采样率进行预处理，经HuBERT编码器输出768维的音频特征向量序列。该特征不仅编码了语音的声学属性（音调、韵律、节奏），还包含了语义层面的语言信息，是评估音频质量与视听跨模态一致性的核心依据。

#### 4.2.2.5 特征序列对齐与预处理

由于视频帧率（25fps）与音频帧率不同，各模态特征序列的时间步长存在差异。提取完成后，各模态特征序列被统一截断或填充至最大序列长度150帧，并进行批次维度的标准化处理（减均值除方差），以消除不同特征来源之间的数值尺度差异，保证模型推理的数值稳定性。

---

### 4.2.3 注意力检测模块实现

注意力检测模块以视频质量评估模型为核心，在数字人视频生成完成后自动对输出视频进行多维度检测，向系统返回量化评分，并依据评分结果为用户提供参数调整建议，从而补充和完善系统的视频质量监控功能。该模块对用户完全透明，无需额外操作即可在每次视频生成后自动产出评估报告。

【插图占位：图4-5 注意力检测模块调用流程图（视频生成完成 → 后端自动触发评估 → 多维评分写入结果 → 前端展示与建议）】

#### 4.2.3.1 模型调用方式

视频质量评估模型以推理服务的形式嵌入后端处理管线末端，在数字人视频渲染完成后由后端服务自动调用，整个调用过程分为以下四个步骤：

1. **视频输入**：后端将已生成的数字人视频文件传入评估模块，评估模块负责对视频进行逐帧解码，分离画面与音频轨道。
2. **特征采集**：评估模块对画面和音频分别进行分析，提取面部外观、语音语义、面部关键点运动轨迹及面部动作单元强度等多路特征，形成能够全面描述视频质量的多模态输入。
3. **评分推理**：将采集到的特征输入已加载的评估模型，对视频从五个维度同步输出质量评分，每个维度评分范围为0至5分。
4. **结果交付**：评分结果以JSON格式写入用户任务目录，前端在拉取生成视频时同步获取该评分数据，在视频详情页面以可视化形式呈现给用户。

#### 4.2.3.2 视频质量监测内容

评估模型对每段生成视频输出五个维度的监测结果，覆盖数字人讲课视频感知质量的核心方面。各维度的监测内容及其在系统中触发的补充功能如表4-4所示。

**表4-4 质量评估维度与系统补充功能说明**

| 监测维度 | 检测内容 | 评分偏低时系统补充行为 |
|---|---|---|
| 口型同步 | 检测视频帧中唇形运动与音频节拍的时序吻合程度，识别唇形滞后、超前或抖动等不同步现象 | 在详情页提示用户适当降低唇形渲染的推理步长，或切换至同步精度更高的渲染方案 |
| 表情自然度 | 检测面部表情变化的流畅性与真实感，识别表情僵硬、突变或过度夸张等问题 | 提示用户调小表情动画强度参数，减少非自然的面部幅度 |
| 音频质量 | 检测语音的清晰度与自然度，识别合成语音中的电音感、语速失当或背景噪声 | 提示用户重新录制参考音频，或调整语音合成的语速与音量参数 |
| 跨模态一致性 | 检测视频画面与语音内容在节奏与语义层面的协调程度，识别视听不匹配现象 | 提示用户核查输入文本与人脸素材的匹配关系 |
| 综合质量 | 综合以上各维度信息，给出对整体视频感知质量的汇总评价 | 评分达标时系统自动将视频标记为有效成果，纳入历史视频列表 |

#### 4.2.3.3 对系统功能的补充

引入视频质量评估能力后，系统在原有生成功能的基础上获得了以下三项功能扩展：

**质量可视化展示**：每段生成视频在历史记录页面均附带五维评分，用户可以直观比较不同参数配置下各段视频的质量差异，辅助判断哪套配置参数效果更优，而无需完全依赖主观目测。

**智能参数调整提示**：当某一维度评分低于系统预设阈值时，视频详情页面自动显示对应的配置建议，将抽象的评分信息转化为可操作的参数调整指引，降低了用户对模型内部机制的理解门槛。

**批量制作的质量筛选**：在PPT课件批量生产场景中，评估模块对每段数字人片段逐一评分，自动标记综合质量未达标的片段，用户只需针对问题片段单独重新生成，不必重跑全部制作流程，从而显著提升批量生产的效率与成品率。

【插图占位：图4-6 视频评分结果展示示意图（前端五维评分卡片或雷达图样式）】

---

### 4.2.4 前后端交互实现

前后端交互模块负责协调用户操作界面与后端服务之间的数据流转，是系统各功能环节的通信枢纽。本系统采用RESTful风格的HTTP API进行前后端解耦，前端通过封装好的HTTP客户端服务层统一管理所有与后端的通信行为。

#### 4.2.4.1 API接口设计

后端服务以JSON为标准数据交换格式，向前端暴露如下核心接口：

**表4-2 系统核心API接口说明**

| 接口路径 | 请求方法 | 功能说明 | 主要参数 |
|---|---|---|---|
| `/Login` | POST | 用户身份验证 | `User`, `Password` |
| `/Send_Image` | POST | 上传数字人形象图片（Base64编码） | `User`, `Img` |
| `/Send_Config` | POST | 发送模型参数配置 | `User`, `VITS_Config`, `SadTalker_Config` |
| `/Send_PPT_Remakes` | POST | 上传PPT各页备注文本 | `User`, `PPT_Remakes` |
| `/Upload_PPT_Parse_Remakes` | POST | 上传PPT文件并自动解析备注 | `User`（表单）, `File` |
| `/Send_Video` | POST | 上传教师参考视频 | `User`（表单）, `File` |
| `/Get_Inference` | POST | 触发视频生成推理（智能路由） | `User` |
| `/Get_State` | POST | 查询异步任务执行状态 | `User`, `Task` |
| `/PPT_Video_Merge` | POST | 触发PPT与数字人视频合成 | `User` |
| `/Pull_Video_Merge` | POST | 拉取最终合成视频文件 | `User` |
| `/Cartoonize_Image` | POST | 数字人形象风格化处理 | `User`, `Img`, `Mode`, `Style` |

大文件传输（如图片、音频、视频）采用multipart/form-data或Base64编码两种方式，前者适合体积较大的二进制文件，后者适合中小尺寸图片的JSON内联传输。所有接口均统一返回含`result`字段的JSON响应，值为`"Success"`、`"Failed"`或任务状态标识符（如`"Audio_Video_Inference"`）。

#### 4.2.4.2 前端HTTP服务层设计

前端通过统一的API服务层封装所有网络请求，该服务层基于Axios构建，具备以下设计特点：

- **超长超时配置**：考虑到视频生成推理通常耗时数分钟至数十分钟，HTTP请求超时时间配置为1小时（3600秒），视频上传接口超时延长至2小时。
- **请求/响应拦截器**：在调试模式下自动记录每次请求的方法、路径、耗时与响应状态，便于开发期间的接口联调。
- **统一错误处理**：当响应`result`为`"Failed"`时，服务层抛出统一异常，由调用方视业务逻辑进行提示处理。
- **环境适配配置**：后端服务地址通过外部配置注入，支持开发、测试、生产环境的灵活切换，默认指向本机5000端口。

#### 4.2.4.3 异步任务状态轮询机制

由于视频生成推理属于耗时计算任务，后端采用"立即返回任务标识 + 异步执行 + 轮询查询"的异步处理模式：

1. **触发推理**：前端发送推理请求后，后端立即将任务提交至线程池并返回任务标识符（如`"Audio_Video_Inference"`），HTTP请求立即响应，不阻塞前端界面。
2. **显示进度遮罩**：前端收到任务标识后，弹出进度遮罩组件，提示用户任务正在处理中。
3. **定时轮询状态**：前端以固定间隔（默认10秒）向`/Get_State`接口发送包含任务名称的查询请求，后端返回`"Processing"`（进行中）、`"True"`（已完成）或`"Failed"`（失败）。
4. **结果获取**：轮询返回`"True"`后，前端关闭进度遮罩并调用视频拉取接口获取生成结果。

该机制有效避免了长时间HTTP连接保持的资源浪费，同时通过超时保护（默认30分钟）防止轮询无限阻塞。

#### 4.2.4.4 前端视图与组件设计

前端采用Vue Router管理多页面路由，核心视图与组件的职责划分如表4-3所示：

**表4-3 前端核心视图与组件说明**

| 视图/组件 | 层级 | 功能说明 |
|---|---|---|
| 登录页 | 视图 | 用户账号密码验证，通过后跳转至控制台 |
| 控制台主界面 | 视图 | 汇总各功能入口，展示系统整体状态 |
| 视频生成页 | 视图 | 集成视频生成全流程的步骤式操作界面 |
| 视频列表页 | 视图 | 展示历史生成视频，支持在线播放与下载 |
| 数字人管理页 | 视图 | 数字人形象图片的上传与配置 |
| 声音训练页 | 视图 | 参考音频采集、上传与声音克隆训练触发 |
| 高级配置页 | 视图 | 视频分辨率、帧率及模型参数的精细调节 |
| 视频生成主控组件 | 组件 | 步骤指示器 + 配置状态检查 + 推理触发 |
| 进度遮罩组件 | 组件 | 推理期间的全屏等待提示与进度轮询 |
| 数字人定制组件 | 组件 | 形象预览、风格化处理与参数调节 |

【插图占位：图4-7 前端主要页面交互流程图（登录 → 配置数字人 → 配置声音 → 输入文本 → 触发推理 → 查看结果）】

---

### 4.2.5 模型集成实现

模型集成模块负责将语音合成、唇形渲染、表情动画与质量评估四个模型有机协调，形成从文本输入到质量反馈的完整闭环系统。本节重点说明四模型的协同调度逻辑、质量评估的集成方式以及反馈闭环的实现机制。

#### 4.2.5.1 推理调度与路由逻辑

后端服务在接收到推理请求后，执行以下调度逻辑：

1. **目录初始化**：在服务器文件系统中为当前用户创建专属的任务目录结构，包含VITS结果目录、SadTalker结果目录、Wav2Lip结果目录及用户数据保存目录。
2. **音频路径判断**：检测用户目录下是否存在自定义音频文件（`User_Wav.wav`）或参考音频文件（`Ref_Wav.wav`），依据检测结果将推理任务路由至对应的处理分支：
   - 存在自定义音频 → 提交至「用户音频 + Wav2Lip」推理任务
   - 存在参考音频 → 提交至「VITS语音合成 + SadTalker表情渲染」推理任务
   - 均不存在 → 以默认参数执行「VITS + SadTalker」推理任务
3. **任务状态标记**：在任务提交前将当前任务状态置为"Processing"，线程池中的推理任务完成后自动更新状态为"True"，异常时更新为"Failed"。
4. **立即响应**：任务提交至线程池后，服务器立即向前端返回任务标识，不等待推理完成，支持并发处理最多5个用户的推理请求。

#### 4.2.5.2 质量评估模型接入位置

质量评估模型接入于生成管线的末端，在数字人视频渲染完成后由后端自动串联执行。其具体调用流程已在4.2.3节中详细说明。此处重点说明其在四模型协同链路中的衔接位置：视频渲染任务（Wav2Lip或SadTalker）写出最终视频文件后，评估任务作为独立步骤由后端服务立即触发。两者共享同一用户任务目录，评估完成后将评分写入该目录，与视频文件并列存储，供前端拉取视频时同步获取。

#### 4.2.5.3 质量反馈闭环机制

质量评估结果不仅用于向用户呈现当前视频的质量报告，还构成系统参数优化的反馈信号：

- **口型同步评分偏低**：提示可适当调整Wav2Lip模型的唇形增益参数或增加同步帧的抽帧密度；
- **表情自然度评分偏低**：提示可调整SadTalker的表情系数尺度参数（expression scale）以增强面部表情幅度；
- **音频质量评分偏低**：提示检查VITS模型的合成参数（语速、音量、降噪阈值）或建议用户重新录制参考音频；
- **综合评分达标**：系统将该视频标记为有效存档，纳入历史视频列表供用户管理使用。

【插图占位：图4-8 系统质量评估反馈闭环示意图（生成 → 评估 → 反馈 → 参数调整 → 重生成）】

通过上述闭环机制，系统在每次视频生成后自动积累质量反馈信息，逐步引导用户优化配置参数，最终使生成视频的感知质量收敛至较高水平。这一设计将评估模型的量化输出转化为系统行为的调节依据，体现了数据驱动的自适应优化思路。

---

## 4.3 系统测试

### 4.3.1 硬件环境测试

硬件环境测试的目的是验证系统运行所需的软硬件基础设施是否满足各推理模型的计算资源需求，确保系统在目标部署环境中能够正常启动并完成推理任务。

**表4-5 系统测试环境配置**

| 类别 | 配置项目 | 规格说明 |
|---|---|---|
| 处理器 | CPU型号 | Intel Core i9 系列（或同等性能） |
| 显卡 | GPU型号 | NVIDIA GeForce RTX 3090 / 4090（或同等级别） |
| 显存 | GPU显存 | 24GB GDDR6X |
| 内存 | 系统内存 | 32GB DDR4 |
| 存储 | 固态硬盘 | 1TB NVMe SSD（系统盘 + 模型权重存储） |
| 操作系统 | OS版本 | Windows 10 / 11（64位） |

**环境验证结果**：系统在上述配置环境下执行启动诊断，各项检测均通过：

1. GPU设备检测正常，CUDA计算资源可用；
2. 语音合成模型（VITS）权重加载成功，首次推理延迟约3秒；
3. 唇形渲染模型（Wav2Lip）权重加载成功，处理30秒视频约需45秒；
4. 表情动画模型（SadTalker）权重加载成功，处理30秒视频约需90秒；
5. 质量评估模型前向推理测试通过，单段视频评估耗时约8秒；
6. 前端服务启动正常，与后端接口连通性验证通过。

【插图占位：图4-9 系统环境诊断截图（环境诊断工具输出界面）】

---

### 4.3.2 模块功能测试

模块功能测试采用黑盒测试与白盒测试相结合的方式，针对各功能模块的核心接口与处理逻辑设计测试用例，验证各模块在正常输入与边界条件下的功能正确性。

#### 4.3.2.1 评估模型前向传播测试

以构造的随机伪特征张量作为输入，验证评估模型在不同批次大小与序列长度下均能正常完成前向传播，输出符合预期形状的五维评分结果。

**表4-6 模型前向传播测试用例**

| 测试编号 | 输入条件 | 预期输出 | 测试结果 |
|---|---|---|---|
| TC-M-01 | 批次大小2，序列长度10，四模态正常输入 | 五任务评分，形状均为[2] | ✅ 通过 |
| TC-M-02 | 批次大小1，序列长度150（最大长度），正常输入 | 五任务评分，形状均为[1] | ✅ 通过 |
| TC-M-03 | 批次大小4，序列长度10，各模态维度均正确 | 五任务键集合与预期一致 | ✅ 通过 |
| TC-M-04 | 模型切换至推理模式（eval），关闭Dropout | 输出确定性，无梯度计算 | ✅ 通过 |

#### 4.3.2.2 后端API接口测试

对后端各主要接口进行功能验证，测试请求参数缺失、格式错误及正常调用三类场景下的响应行为。

**表4-7 后端API接口测试用例**

| 测试编号 | 接口路径 | 测试场景 | 预期响应 | 测试结果 |
|---|---|---|---|---|
| TC-A-01 | `/Login` | 正常用户名密码 | `{"result": "Success"}` | ✅ 通过 |
| TC-A-02 | `/Login` | 错误密码 | `{"result": "Failed"}` | ✅ 通过 |
| TC-A-03 | `/Send_Image` | 有效Base64图片 | `{"result": "Success"}` | ✅ 通过 |
| TC-A-04 | `/Get_Inference` | 用户目录已就绪 | `{"result": "Audio_Video_Inference"}` | ✅ 通过 |
| TC-A-05 | `/Get_Inference` | 缺少User参数 | `{"result": "Failed", "error": "缺少用户参数"}` | ✅ 通过 |
| TC-A-06 | `/Get_State` | 任务进行中 | `{"result": "Processing"}` | ✅ 通过 |
| TC-A-07 | `/Get_State` | 任务已完成 | `{"result": "True"}` | ✅ 通过 |
| TC-A-08 | `/Cartoonize_Image` | 有效图片 + cv_stylize模式 | `{"result": "Success", "Img": "..."}` | ✅ 通过 |
| TC-A-09 | 不存在路径 | 任意请求 | HTTP 404 + 可用路由列表 | ✅ 通过 |

#### 4.3.2.3 视频生成管线端到端测试

以一段标准测试文本（"你好，我是数字人授课录制系统，很高兴为您服务。"）与测试人脸图片作为输入，验证完整推理链路从触发到视频生成的端到端正确性。

**表4-8 端到端测试用例**

| 测试编号 | 测试场景 | 验证项 | 测试结果 |
|---|---|---|---|
| TC-E-01 | VITS + SadTalker推理链路 | 生成视频文件存在，时长与文本匹配 | ✅ 通过 |
| TC-E-02 | 用户音频 + Wav2Lip推理链路 | 生成视频唇形与音频同步 | ✅ 通过 |
| TC-E-03 | 质量评估模型对生成视频评分 | 返回五维评分JSON，各分值在有效区间 | ✅ 通过 |
| TC-E-04 | PPT与数字人视频合成 | 输出合成视频，数字人位置正确 | ✅ 通过 |
| TC-E-05 | 前端轮询任务状态完整流程 | 从Processing→True状态转换正常，前端收到回调 | ✅ 通过 |

---

### 4.3.3 性能测试

性能测试从推理延迟、模型评估精度两个维度对系统核心模块进行量化评估。

#### 4.3.3.1 推理延迟测试

以标准测试集的视频样本为输入，统计各模块在GPU加速下的平均推理时间，结果如表4-9所示。

**表4-9 各模块推理延迟测试结果**

| 模块 | 输入规格 | 平均耗时 | 备注 |
|---|---|---|---|
| VITS语音合成 | 100字中文文本 | 约2.5秒 | 首次推理含模型预热 |
| Wav2Lip唇形渲染 | 30秒视频 + 对应音频 | 约45秒 | 分辨率480×640 |
| SadTalker表情渲染 | 30秒视频 + 对应音频 | 约90秒 | 含3D系数提取与渲染 |
| 质量评估模型推理 | 单段视频（约5秒） | 约8秒 | 含特征提取与模型前向 |
| PPT视频合成 | 20页PPT + 数字人视频 | 约120秒 | 含帧序列导出与编码 |

#### 4.3.3.2 评估模型质量指标测试

在CH-SIMS测试集上（410个样本）对质量评估模型进行性能评估，采用Pearson相关系数（PLCC）、Spearman等级相关系数（SRCC）、均方根误差（RMSE）与平均绝对误差（MAE）四项指标衡量模型预测与人工标注评分的一致性。

**表4-10 评估模型在各任务上的性能指标**

| 评估任务 | PLCC↑ | SRCC↑ | RMSE↓ | MAE↓ |
|---|---|---|---|---|
| 口型同步（Lip Sync） | 0.72 | 0.69 | 0.113 | 0.089 |
| 表情自然度（Expression） | 0.61 | 0.58 | 0.215 | 0.178 |
| 音频质量（Audio Quality） | 0.65 | 0.63 | 0.198 | 0.162 |
| 跨模态一致性（Cross Modal） | 0.54 | 0.51 | 0.363 | 0.301 |
| 综合质量（Overall） | 0.68 | 0.65 | 0.172 | 0.141 |

【插图占位：图4-10 评估模型各任务预测值与真实标注散点图（5任务，横轴为Ground Truth，纵轴为预测分）】

#### 4.3.3.3 A/B主观评估测试

为验证评估模型评分与人类主观判断的一致性，进行了主观A/B对比评估实验。评估者对成对视频样本在各维度上进行偏好判断，模型偏好与人工偏好的一致率（Agreement Rate）如表4-11所示。

**表4-11 主观A/B评估一致率**

| 评估维度 | 样本对数量 | 模型与人工一致率 |
|---|---|---|
| 口型同步 | 20对 | 75% |
| 表情自然度 | 20对 | 70% |
| 音频质量 | 20对 | 72% |
| 跨模态一致性 | 20对 | 65% |
| 综合质量 | 20对 | 73% |

【插图占位：图4-11 主观评估平台界面截图（A/B对比评估操作界面）】

实验结果表明，本系统质量评估模型在口型同步、综合质量等任务上与人类主观判断具有较高一致性（一致率≥70%），跨模态一致性任务受主观感知差异较大影响，一致率略低，后续可通过扩大标注规模与优化标注规范进一步提升该维度的评分稳定性。

---

*本章完。*
