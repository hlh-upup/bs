# AI生成说话人脸视频评价模型配置文件

# 数据配置
data:
  dataset: "EmotionTalk"  # 数据集名称
  train_split: 0.8       # 训练集比例
  val_split: 0.1         # 验证集比例
  test_split: 0.1        # 测试集比例
  batch_size: 8         # 批次大小（优化: 增加批量以提升训练稳定性）
  num_workers: 0         # 数据加载线程数（Windows 下建议0以避免多进程问题）
  max_frames: 150        # 最大帧数
  frame_rate: 25         # 视频帧率
  audio_sample_rate: 16000  # 音频采样率
  face_size: 96          # 人脸图像大小

# 特征提取配置
feature_extraction:
  feature_types: ['visual', 'audio', 'keypoint', 'au', 'syncnet'] # 要使用的特征类型
  visual:
    model: "py-feat"     # 使用py-feat进行面部特征提取
    fallback_model: "resnet101"  # 备用视觉特征提取模型
    pretrained: true     # 是否使用预训练权重
    output_dim: 2048     # 输出特征维度
    face_model: "retinaface"     # py-feat人脸检测模型
    landmark_model: "mobilefacenet"  # py-feat关键点模型
    au_model: "svm"      # py-feat AU检测模型
    emotion_model: "resmasknet"  # py-feat情感识别模型
    facepose_model: "img2pose"   # py-feat头部姿态模型
  audio:
    model: "hubert"      # 音频特征提取模型
    pretrained: true     # 是否使用预训练权重
    output_dim: 768      # 输出特征维度
    model_path: "models/hubert-base"  # 本地模型路径
  keypoints:
    model: "mediapipe"   # 关键点检测模型（备用）
    num_points: 468      # 关键点数量
  au:
    model: "py-feat"     # 使用py-feat进行AU检测
    fallback_model: "openface"  # 备用AU检测模型
  syncnet:
    model_path: 'f:/bs/models/pre-trained/stable_syncnet.pt'
    batch_size: 64
    v_shift: 0
features:
  visual:
    model: "resnet101"
    pretrained: true
    feature_dim: 2048     # 改名从output_dim
    target_fps: 25        # 新增
    sequence_length: 150  # 新增
  audio:
    model: "hubert"
    pretrained: true
    feature_dim: 768      # 改名从output_dim
    sample_rate: 16000    # 新增
    sequence_length: 150  # 新增
  keypoint:
    model: "mediapipe"
    num_points: 468
    feature_dim: 1404     # 新增 (468*3)
    target_fps: 25        # 新增
    sequence_length: 150  # 新增
  au:
    model: "openface"
    feature_dim: 17       # 新增 (假设17个AU)
    target_fps: 25        # 新增
    sequence_length: 150  # 新增
  syncnet:
    model_path: 'f:/bs/models/pre-trained/stable_syncnet.pt'
    batch_size: 64
    v_shift: 0
  consistency:
    feature_dim: 1
# 模型配置
model:
  name: "AdvancedMultiTaskTalkingFaceEvaluator"  # 切换为高级多任务模型（含动态权重/一致性损失）
  # 特征维度配置（与实际提取的特征维度匹配）
  visual_dim: 2048       # 视觉特征维度 (修正: 与py-feat输出对齐)
  audio_dim: 768         # 音频特征维度
  keypoint_dim: 1404     # 关键点特征维度
  au_dim: 17             # AU特征维度
  encoder_dim: 256       # 编码器输出维度
  hidden_dim: 256       # 模型隐层维度（与 encoder_dim 对齐）
  dropout: 0.3           # Dropout比例
  transformer:
    num_layers: 3        # Transformer层数
    num_heads: 8         # 注意力头数
    dim_feedforward: 1024 # 前馈网络维度
    dropout: 0.1         # Transformer dropout
  task_heads:
    # 统一分数区间，便于训练/评估裁剪与映射
    score_min: -1.0
    score_max: 1.0
    out_activation: none   # 改为 none，避免与当前标签[-0.8,1]等范围不一致
    clamp_output: false    # 先关闭硬裁剪，便于模型学习真实标签量纲
    lip_sync:            # 口型同步评估任务
      hidden_dims: [128, 64]
      loss_weight: 1.0
      dropout: 0.2
    expression:          # 表情自然度评估任务
      hidden_dims: [128, 64]
      loss_weight: 1.0
      dropout: 0.2
    audio_quality:       # 音频质量评估任务
      hidden_dims: [128, 64]
      loss_weight: 1.0
      dropout: 0.2
    cross_modal:         # 跨模态一致性评估任务
      hidden_dims: [128, 64]
      loss_weight: 1.0
      dropout: 0.2
    overall:
      hidden_dims: [128, 64]
      dropout: 0.2
  task_weights:          # 任务权重配置
    lip_sync: 1000   # 该标签当前几乎无方差（全为4.763），降低权重避免主导训练
    expression: 1.0
    audio_quality: 1.0
    cross_modal: 1.0
    overall: 1.0

  # 高级训练策略（仅对 Advanced/Improved 生效，其他模型忽略）
  advanced:
    task_weighting: uncertainty   # learned | uncertainty | gradnorm
    consistency:
      mode: corr                  # std | corr | cov
      weight: 0.05                # 一致性损失权重（建议0.02~0.1之间网格尝试）
      tasks: [lip_sync, expression, audio_quality, cross_modal]

# 训练配置
train:
  epochs: 30            # 训练轮数
  output_dir: "../experiments"  # 输出目录
  save_dir: "../experiments"    # 模型保存目录（兼容 main.py）
  log_dir: "../experiments/logs" # 日志目录（兼容 main.py）
  save_interval: 10      # 保存间隔
  # 可选：对输入按批做标准化（对最后一维），能缓解 VISUAL/AU 尺度差异与 NaN 影响
  batch_norm_inputs: true
  optimizer:
    type: "adam"         # 优化器类型
    lr: 0.0001           # 学习率
    weight_decay: 0.0001 # 权重衰减
  scheduler:
    type: "cosine"       # 学习率调度器类型
    T_max: 100           # 余弦退火最大步数
    eta_min: 0           # 最小学习率
  early_stopping: 10     # 早停轮数
  gradient_accumulation: 4  # 梯度累积步数
  mixed_precision: true  # 是否使用混合精度训练

# 评估配置
eval:
  bootstrap:
    n_resamples: 200
  metrics: ["pearson", "spearman", "rmse", "mae"]  # 评估指标
  visualization: true    # 是否可视化结果

# 推理配置
inference:
  threshold:
    lip_sync: 3.0        # 口型同步评分阈值
    expression: 3.0      # 表情自然度评分阈值
    audio_quality: 3.0   # 音频质量评分阈值
    cross_modal: 3.0     # 跨模态一致性评分阈值
  output_format: "json"  # 输出格式