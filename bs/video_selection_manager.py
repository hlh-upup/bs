#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è§†é¢‘é€‰å–å’Œå®éªŒç®¡ç†ç³»ç»Ÿ
ç”¨äºä»æ•°æ®é›†ä¸­é€‰å–ä»£è¡¨æ€§è§†é¢‘è¿›è¡Œä¸»è§‚è¯„ä»·å®éªŒ
"""

import os
import sys
import json
import pickle
import random
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils import load_config
from models import MultiTaskTalkingFaceEvaluator, ImprovedMultiTaskTalkingFaceEvaluator
from data import create_dataloaders_from_pkl
from evaluation import Evaluator

class VideoSelector:
    """è§†é¢‘é€‰å–å™¨ - åŸºäºæ¨¡å‹è¯„åˆ†é€‰å–ä»£è¡¨æ€§è§†é¢‘"""
    
    def __init__(self, config: dict):
        self.config = config
        self.output_dir = Path(config.get('output_dir', 'subjective_experiment'))
        self.videos_dir = self.output_dir / 'videos'
        self.selection_log = self.output_dir / 'selection_log.json'
        
    def load_model_predictions(self, model_path: str, test_loader, device) -> Dict:
        """åŠ è½½æ¨¡å‹é¢„æµ‹ç»“æœ"""
        
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹é¢„æµ‹ç»“æœ: {model_path}")
        
        # åŠ è½½æ¨¡å‹
        checkpoint = torch.load(model_path, map_location=device)
        model_state = checkpoint['model_state_dict']
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model_config = self.config['model']
        if 'improved' in model_path.lower():
            model = ImprovedMultiTaskTalkingFaceEvaluator(model_config)
        else:
            model = MultiTaskTalkingFaceEvaluator(model_config)
        
        model.load_state_dict(model_state)
        model.to(device)
        model.eval()
        
        # è·å–é¢„æµ‹ç»“æœ
        all_predictions = []
        all_targets = []
        video_indices = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                # æ¨¡å‹é¢„æµ‹
                predictions, losses = model(
                    visual_features=batch['visual_features'].to(device),
                    audio_features=batch['audio_features'].to(device),
                    keypoint_features=batch.get('keypoints', torch.zeros(batch['visual_features'].size(0), 0).to(device)),
                    au_features=batch.get('au_features', torch.zeros(batch['visual_features'].size(0), 0).to(device)),
                    targets={k: v for k, v in batch.items() if k.endswith('_score')}
                )
                
                # æ”¶é›†ç»“æœ
                all_predictions.append(predictions)
                all_targets.append({k: v for k, v in batch.items() if k.endswith('_score')})
                
                # å‡è®¾æ¯ä¸ªbatchå¯¹åº”ä¸åŒçš„è§†é¢‘
                if 'video_indices' in batch:
                    video_indices.extend(batch['video_indices'].tolist())
                else:
                    video_indices.extend(range(batch_idx * batch['visual_features'].size(0), 
                                            (batch_idx + 1) * batch['visual_features'].size(0)))
        
        # æ•´ç†ç»“æœ
        results = {
            'predictions': all_predictions,
            'targets': all_targets,
            'video_indices': video_indices,
            'model_path': model_path
        }
        
        return results
    
    def select_videos_by_quality_stratification(self, results: Dict, n_videos: int = 20) -> Dict:
        """åŸºäºè´¨é‡åˆ†å±‚é€‰å–è§†é¢‘"""
        
        print("ğŸ¯ åŸºäºè´¨é‡åˆ†å±‚é€‰å–è§†é¢‘...")
        
        # æå–æ•´ä½“è¯„åˆ†
        overall_scores = []
        for pred_dict in results['predictions']:
            if 'overall' in pred_dict:
                overall_scores.extend(pred_dict['overall'].cpu().numpy().flatten())
            else:
                # å¦‚æœæ²¡æœ‰æ•´ä½“è¯„åˆ†ï¼Œè®¡ç®—å„ä»»åŠ¡å¹³å‡
                task_scores = []
                for task in ['lip_sync', 'expression', 'audio_quality', 'cross_modal']:
                    if task in pred_dict:
                        task_scores.append(pred_dict[task].cpu().numpy().flatten())
                if task_scores:
                    overall_scores.extend(np.mean(task_scores, axis=0))
        
        overall_scores = np.array(overall_scores)
        
        # æŒ‰è´¨é‡åˆ†å±‚
        percentiles = np.percentile(overall_scores, [25, 75])
        low_quality_mask = overall_scores <= percentiles[0]
        high_quality_mask = overall_scores >= percentiles[1]
        medium_quality_mask = ~low_quality_mask & ~high_quality_mask
        
        # åˆ†å±‚é€‰å–
        n_high = n_videos // 3
        n_medium = n_videos // 3
        n_low = n_videos - n_high - n_medium
        
        selected_indices = []
        
        # é«˜è´¨é‡è§†é¢‘
        high_indices = np.where(high_quality_mask)[0]
        selected_high = np.random.choice(high_indices, min(n_high, len(high_indices)), replace=False)
        selected_indices.extend(selected_high)
        
        # ä¸­ç­‰è´¨é‡è§†é¢‘
        medium_indices = np.where(medium_quality_mask)[0]
        selected_medium = np.random.choice(medium_indices, min(n_medium, len(medium_indices)), replace=False)
        selected_indices.extend(selected_medium)
        
        # ä½è´¨é‡è§†é¢‘
        low_indices = np.where(low_quality_mask)[0]
        selected_low = np.random.choice(low_indices, min(n_low, len(low_indices)), replace=False)
        selected_indices.extend(selected_low)
        
        selection_info = {
            'method': 'quality_stratification',
            'total_videos': len(overall_scores),
            'selected_count': len(selected_indices),
            'quality_distribution': {
                'high': len(selected_high),
                'medium': len(selected_medium),
                'low': len(selected_low)
            },
            'quality_thresholds': {
                'low_threshold': percentiles[0],
                'high_threshold': percentiles[1]
            },
            'selected_indices': selected_indices.tolist(),
            'scores': overall_scores.tolist()
        }
        
        return selection_info
    
    def select_videos_by_diversity_sampling(self, results: Dict, n_videos: int = 20) -> Dict:
        """åŸºäºå¤šæ ·æ€§é‡‡æ ·é€‰å–è§†é¢‘"""
        
        print("ğŸ­ åŸºäºå¤šæ ·æ€§é‡‡æ ·é€‰å–è§†é¢‘...")
        
        # æå–å¤šç»´åº¦ç‰¹å¾
        features_list = []
        for pred_dict in results['predictions']:
            features = []
            for task in ['lip_sync', 'expression', 'audio_quality', 'cross_modal']:
                if task in pred_dict:
                    features.append(pred_dict[task].cpu().numpy().flatten())
            
            if features:
                # åˆå¹¶æ‰€æœ‰ä»»åŠ¡ç‰¹å¾
                video_features = np.column_stack(features)
                features_list.append(video_features)
        
        if not features_list:
            print("âš ï¸ æ— æ³•æå–ç‰¹å¾ï¼Œä½¿ç”¨éšæœºé€‰å–")
            return self.select_videos_randomly(results, n_videos)
        
        all_features = np.vstack(features_list)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        features_normalized = scaler.fit_transform(all_features)
        
        # ä½¿ç”¨K-meansèšç±»
        n_clusters = min(n_videos, len(all_features))
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(features_normalized)
        
        # ä»æ¯ä¸ªç°‡ä¸­é€‰å–ä»£è¡¨æ€§æ ·æœ¬
        selected_indices = []
        for cluster_id in range(n_clusters):
            cluster_mask = cluster_labels == cluster_id
            cluster_indices = np.where(cluster_mask)[0]
            
            if len(cluster_indices) > 0:
                # é€‰æ‹©è·ç¦»ç°‡ä¸­å¿ƒæœ€è¿‘çš„æ ·æœ¬
                cluster_center = kmeans.cluster_centers_[cluster_id]
                distances = np.linalg.norm(features_normalized[cluster_mask] - cluster_center, axis=1)
                representative_idx = cluster_indices[np.argmin(distances)]
                selected_indices.append(representative_idx)
        
        # å¦‚æœé€‰å–æ•°é‡ä¸è¶³ï¼Œéšæœºè¡¥å……
        if len(selected_indices) < n_videos:
            remaining_indices = [i for i in range(len(all_features)) if i not in selected_indices]
            additional_needed = n_videos - len(selected_indices)
            additional_indices = np.random.choice(remaining_indices, min(additional_needed, len(remaining_indices)), replace=False)
            selected_indices.extend(additional_indices)
        
        selection_info = {
            'method': 'diversity_sampling',
            'total_videos': len(all_features),
            'selected_count': len(selected_indices),
            'n_clusters': n_clusters,
            'selected_indices': selected_indices,
            'cluster_distribution': {
                str(cluster_id): int(np.sum(cluster_labels == cluster_id))
                for cluster_id in range(n_clusters)
            }
        }
        
        return selection_info
    
    def select_videos_randomly(self, results: Dict, n_videos: int = 20) -> Dict:
        """éšæœºé€‰å–è§†é¢‘ï¼ˆåŸºçº¿æ–¹æ³•ï¼‰"""
        
        print("ğŸ² éšæœºé€‰å–è§†é¢‘...")
        
        total_videos = len(results['video_indices'])
        selected_indices = np.random.choice(total_videos, min(n_videos, total_videos), replace=False)
        
        selection_info = {
            'method': 'random_sampling',
            'total_videos': total_videos,
            'selected_count': len(selected_indices),
            'selected_indices': selected_indices.tolist()
        }
        
        return selection_info
    
    def create_video_pairs(self, original_selection: Dict, improved_selection: Dict) -> Dict:
        """åˆ›å»ºè§†é¢‘é…å¯¹ç”¨äºå¯¹æ¯”è¯„ä»·"""
        
        print("ğŸ”— åˆ›å»ºè§†é¢‘é…å¯¹...")
        
        # ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹é€‰å–ç›¸åŒçš„è§†é¢‘ç´¢å¼•
        common_indices = list(set(original_selection['selected_indices']) & 
                            set(improved_selection['selected_indices']))
        
        if not common_indices:
            print("âš ï¸ æ²¡æœ‰å…±åŒè§†é¢‘ç´¢å¼•ï¼Œä½¿ç”¨æ”¹è¿›æ¨¡å‹çš„é€‰å–ç»“æœ")
            common_indices = improved_selection['selected_indices'][:20]
        
        # åˆ›å»ºé…å¯¹
        video_pairs = []
        for idx in common_indices:
            pair = {
                'video_index': idx,
                'original_video_path': f'videos/original/video_{idx:04d}.mp4',
                'improved_video_path': f'videos/improved/video_{idx:04d}.mp4',
                'pair_id': len(video_pairs)
            }
            video_pairs.append(pair)
        
        # éšæœºåŒ–å‘ˆç°é¡ºåº
        random.shuffle(video_pairs)
        
        pairing_info = {
            'total_pairs': len(video_pairs),
            'pairing_method': 'matched_pairs',
            'video_pairs': video_pairs,
            'randomization_seed': 42
        }
        
        return pairing_info
    
    def generate_selection_report(self, original_results: Dict, improved_results: Dict,
                                original_selection: Dict, improved_selection: Dict,
                                pairing_info: Dict) -> str:
        """ç”Ÿæˆè§†é¢‘é€‰å–æŠ¥å‘Š"""
        
        report = f"""# è§†é¢‘é€‰å–æŠ¥å‘Š

## ğŸ“Š é€‰å–æ¦‚è§ˆ

### æ¨¡å‹æ€§èƒ½å¯¹æ¯”
- **åŸå§‹æ¨¡å‹è§†é¢‘æ•°é‡**: {original_selection['selected_count']}
- **ä¼˜åŒ–æ¨¡å‹è§†é¢‘æ•°é‡**: {improved_selection['selected_count']}
- **æœ€ç»ˆé…å¯¹æ•°é‡**: {pairing_info['total_pairs']}

### é€‰å–æ–¹æ³•
- **åŸå§‹æ¨¡å‹**: {original_selection['method']}
- **ä¼˜åŒ–æ¨¡å‹**: {improved_selection['method']}
- **é…å¯¹æ–¹æ³•**: {pairing_info['pairing_method']}

## ğŸ“ˆ è´¨é‡åˆ†å¸ƒåˆ†æ

### åŸå§‹æ¨¡å‹
"""
        
        if 'quality_distribution' in original_selection:
            report += f"""
- é«˜è´¨é‡è§†é¢‘: {original_selection['quality_distribution']['high']}
- ä¸­ç­‰è´¨é‡è§†é¢‘: {original_selection['quality_distribution']['medium']}
- ä½è´¨é‡è§†é¢‘: {original_selection['quality_distribution']['low']}
"""
        
        report += "\n### ä¼˜åŒ–æ¨¡å‹\n"
        
        if 'quality_distribution' in improved_selection:
            report += f"""
- é«˜è´¨é‡è§†é¢‘: {improved_selection['quality_distribution']['high']}
- ä¸­ç­‰è´¨é‡è§†é¢‘: {improved_selection['quality_distribution']['medium']}
- ä½è´¨é‡è§†é¢‘: {improved_selection['quality_distribution']['low']}
"""
        
        report += f"""
## ğŸ“‹ é€‰å–çš„è§†é¢‘ç´¢å¼•

### åŸå§‹æ¨¡å‹é€‰å–çš„è§†é¢‘
{', '.join(map(str, original_selection['selected_indices'][:10]))}{'...' if len(original_selection['selected_indices']) > 10 else ''}

### ä¼˜åŒ–æ¨¡å‹é€‰å–çš„è§†é¢‘
{', '.join(map(str, improved_selection['selected_indices'][:10]))}{'...' if len(improved_selection['selected_indices']) > 10 else ''}

### é…å¯¹ä¿¡æ¯
æ€»å…±åˆ›å»ºäº† {pairing_info['total_pairs']} ä¸ªè§†é¢‘å¯¹ï¼Œç”¨äºä¸»è§‚è¯„ä»·å®éªŒã€‚

## ğŸ¯ å®éªŒè®¾è®¡å»ºè®®

1. **è§†é¢‘å‘ˆç°é¡ºåº**: å·²éšæœºåŒ–ï¼Œå‡å°‘é¡ºåºæ•ˆåº”
2. **ç›²åŒ–è®¾è®¡**: ä½¿ç”¨A/Bæ ‡ç­¾æ›¿ä»£åŸå§‹/ä¼˜åŒ–æ ‡ç­¾
3. **å¹³è¡¡è®¾è®¡**: æ¯ä¸ªå‚ä¸è€…è¯„ä»·æ‰€æœ‰è§†é¢‘å¯¹
4. **è´¨é‡æ§åˆ¶**: åŒ…å«é«˜è´¨é‡ã€ä¸­ç­‰è´¨é‡å’Œä½è´¨é‡è§†é¢‘

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report
    
    def save_selection_results(self, results: Dict):
        """ä¿å­˜é€‰å–ç»“æœ"""
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_path = self.output_dir / 'selection_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / 'selection_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(results['report'])
        
        print(f"âœ… é€‰å–ç»“æœå·²ä¿å­˜: {results_path}")
        print(f"ğŸ“„ é€‰å–æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return results_path, report_path

class ExperimentManager:
    """å®éªŒç®¡ç†å™¨ - ç®¡ç†ä¸»è§‚è¯„ä»·å®éªŒçš„å®Œæ•´æµç¨‹"""
    
    def __init__(self, config: dict):
        self.config = config
        self.output_dir = Path(config.get('output_dir', 'subjective_experiment'))
        self.experiment_data = self.output_dir / 'experiment_data.json'
        
    def create_participant_info(self, participant_id: str) -> Dict:
        """åˆ›å»ºå‚ä¸è€…ä¿¡æ¯"""
        
        participant_info = {
            'participant_id': participant_id,
            'start_time': None,
            'end_time': None,
            'total_duration': None,
            'trial_results': [],
            'completed_trials': 0,
            'total_trials': 20,
            'quality_check': {
                'attention_passed': False,
                'time_consistent': False,
                'rating_consistent': False
            }
        }
        
        return participant_info
    
    def create_trial_structure(self, video_pairs: List[Dict]) -> List[Dict]:
        """åˆ›å»ºè¯•éªŒç»“æ„"""
        
        trials = []
        for pair_idx, pair in enumerate(video_pairs):
            trial = {
                'trial_id': pair_idx,
                'video_pair': pair,
                'presentation_order': random.choice(['AB', 'BA']),  # éšæœºå‘ˆç°é¡ºåº
                'condition_labels': {
                    'A': random.choice(['original', 'improved']),
                    'B': 'improved' if random.choice(['original', 'improved']) == 'original' else 'original'
                },
                'time_limit': 300,  # 5åˆ†é’Ÿæ—¶é—´é™åˆ¶
                'required_ratings': [
                    'lip_sync_A', 'expression_A', 'audio_quality_A', 'visual_clarity_A', 'overall_quality_A',
                    'lip_sync_B', 'expression_B', 'audio_quality_B', 'visual_clarity_B', 'overall_quality_B',
                    'preference'
                ]
            }
            trials.append(trial)
        
        return trials
    
    def setup_experiment_database(self) -> Dict:
        """è®¾ç½®å®éªŒæ•°æ®åº“"""
        
        experiment_db = {
            'experiment_id': f"subj_eval_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}",
            'created_time': pd.Timestamp.now().isoformat(),
            'status': 'setup',
            'participants': {},
            'trials': [],
            'config': self.config,
            'statistics': {
                'total_participants': 0,
                'completed_participants': 0,
                'average_duration': 0,
                'completion_rate': 0
            }
        }
        
        # ä¿å­˜æ•°æ®åº“
        with open(self.experiment_data, 'w', encoding='utf-8') as f:
            json.dump(experiment_db, f, ensure_ascii=False, indent=2, default=str)
        
        return experiment_db
    
    def generate_experiment_summary(self, experiment_db: Dict) -> str:
        """ç”Ÿæˆå®éªŒæ€»ç»“"""
        
        summary = f"""# ä¸»è§‚è¯„ä»·å®éªŒæ€»ç»“

## ğŸ“Š å®éªŒæ¦‚è§ˆ

- **å®éªŒID**: {experiment_db['experiment_id']}
- **åˆ›å»ºæ—¶é—´**: {experiment_db['created_time']}
- **å®éªŒçŠ¶æ€**: {experiment_db['status']}

## ğŸ‘¥ å‚ä¸è€…ç»Ÿè®¡

- **æ€»å‚ä¸è€…**: {experiment_db['statistics']['total_participants']}
- **å®Œæˆå®éªŒ**: {experiment_db['statistics']['completed_participants']}
- **å®Œæˆç‡**: {experiment_db['statistics']['completion_rate']:.1f}%
- **å¹³å‡æ—¶é•¿**: {experiment_db['statistics']['average_duration']:.1f}åˆ†é’Ÿ

## ğŸ“ˆ åˆæ­¥ç»“æœ

### å®Œæˆåº¦åˆ†æ
"""
        
        if experiment_db['statistics']['completed_participants'] > 0:
            completion_rate = (experiment_db['statistics']['completed_participants'] / 
                             experiment_db['statistics']['total_participants']) * 100
            summary += f"- å®éªŒå®Œæˆç‡: {completion_rate:.1f}%\n"
        
        summary += f"""
## ğŸ“‹ åç»­æ­¥éª¤

1. **æ•°æ®æ”¶é›†**: ç»§ç»­æ‹›å‹Ÿå‚ä¸è€…ç›´è‡³è¾¾åˆ°ç›®æ ‡æ•°é‡
2. **è´¨é‡æ§åˆ¶**: æ£€æŸ¥æ•°æ®è´¨é‡ï¼Œæ’é™¤æ— æ•ˆæ•°æ®
3. **ç»Ÿè®¡åˆ†æ**: è¿›è¡Œè¯¦ç»†çš„ç»Ÿè®¡åˆ†æ
4. **ç»“æœå¯è§†åŒ–**: ç”Ÿæˆå›¾è¡¨å’ŒæŠ¥å‘Š

---
*æ€»ç»“ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return summary

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ å¼€å§‹è§†é¢‘é€‰å–å’Œå®éªŒç®¡ç†...")
    
    # åŠ è½½é…ç½®
    config = load_config("config/optimized_config.yaml")
    config['output_dir'] = 'subjective_experiment'
    
    # åˆ›å»ºè§†é¢‘é€‰å–å™¨
    selector = VideoSelector(config)
    
    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    try:
        _, _, test_loader = create_dataloaders_from_pkl("datasets/ac.pkl", config)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½æ•°æ®é›†: {e}")
        return
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    original_model_path = "experiments_original/checkpoints/best_model.pth"
    improved_model_path = "experiments_improved/checkpoints/best_model.pth"
    
    if not os.path.exists(original_model_path):
        print(f"âš ï¸ åŸå§‹æ¨¡å‹ä¸å­˜åœ¨: {original_model_path}")
        return
    
    if not os.path.exists(improved_model_path):
        print(f"âš ï¸ ä¼˜åŒ–æ¨¡å‹ä¸å­˜åœ¨: {improved_model_path}")
        return
    
    try:
        # åŠ è½½æ¨¡å‹é¢„æµ‹ç»“æœ
        original_results = selector.load_model_predictions(original_model_path, test_loader, device)
        improved_results = selector.load_model_predictions(improved_model_path, test_loader, device)
        
        # é€‰å–è§†é¢‘
        original_selection = selector.select_videos_by_quality_stratification(original_results, n_videos=20)
        improved_selection = selector.select_videos_by_diversity_sampling(improved_results, n_videos=20)
        
        # åˆ›å»ºè§†é¢‘é…å¯¹
        pairing_info = selector.create_video_pairs(original_selection, improved_selection)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = selector.generate_selection_report(
            original_results, improved_results, 
            original_selection, improved_selection, 
            pairing_info
        )
        
        # ä¿å­˜ç»“æœ
        results = {
            'timestamp': pd.Timestamp.now().isoformat(),
            'original_selection': original_selection,
            'improved_selection': improved_selection,
            'pairing_info': pairing_info,
            'report': report
        }
        
        selector.save_selection_results(results)
        
        # è®¾ç½®å®éªŒç®¡ç†
        print("ğŸ§ª è®¾ç½®å®éªŒç®¡ç†ç³»ç»Ÿ...")
        manager = ExperimentManager(config)
        experiment_db = manager.setup_experiment_database()
        
        # åˆ›å»ºè¯•éªŒç»“æ„
        trials = manager.create_trial_structure(pairing_info['video_pairs'])
        experiment_db['trials'] = trials
        
        # ä¿å­˜æ›´æ–°çš„æ•°æ®åº“
        with open(manager.experiment_data, 'w', encoding='utf-8') as f:
            json.dump(experiment_db, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… å®éªŒæ•°æ®åº“å·²åˆ›å»º: {manager.experiment_data}")
        
        # ç”Ÿæˆå®éªŒæ€»ç»“
        summary = manager.generate_experiment_summary(experiment_db)
        summary_path = selector.output_dir / 'experiment_summary.md'
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary)
        
        print(f"ğŸ“„ å®éªŒæ€»ç»“å·²ä¿å­˜: {summary_path}")
        
        print(f"\nğŸ‰ è§†é¢‘é€‰å–å’Œå®éªŒç®¡ç†è®¾ç½®å®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {selector.output_dir}")
        print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print(f"1. å‡†å¤‡å¯¹åº”çš„è§†é¢‘æ–‡ä»¶")
        print(f"2. éƒ¨ç½²è¯„ä»·ç•Œé¢åˆ°WebæœåŠ¡å™¨")
        print(f"3. å¼€å§‹æ‹›å‹Ÿå‚ä¸è€…")
        print(f"4. ç›‘æ§å®éªŒè¿›å±•å’Œæ•°æ®è´¨é‡")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()