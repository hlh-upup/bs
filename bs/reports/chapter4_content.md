# 第4章 说话人脸视频质量评估系统设计与实现

基于第3章所提出的多模态多任务说话人脸视频质量评估算法，本章围绕评估系统的工程化实现展开阐述，详细介绍系统的需求分析、总体架构设计、各模块的详细实现方案，以及系统测试与实验验证。本章的核心目标是将第3章的算法模型集成为一个完整的、可部署的视频质量评估系统，为AI生成说话人脸视频的自动化质量评估提供端到端的技术解决方案。

具体而言，本章的组织结构如下：4.1节从功能需求和非功能需求两个层面进行系统需求分析，明确系统的设计目标与技术约束；4.2节详细阐述系统的总体架构设计与各核心模块的实现方案，涵盖后端API服务、前端可视化界面、数据库设计以及模型推理服务四个关键组件；4.3节通过功能测试、模型集成测试、系统性能测试和用户主观评价实验，全面验证系统的正确性、有效性和可用性。

## 4.1 系统需求分析与总体设计

（本节内容待撰写）

## 4.2 系统详细设计与实现

（本节内容待撰写）

## 4.3 系统测试与实验验证

系统的质量保障是软件工程中的核心环节。为全面验证本研究所构建的说话人脸视频质量评估系统在功能正确性、模型有效性、运行性能和用户体验等方面的综合表现，本节设计并实施了一套系统化的测试与实验验证方案。该方案涵盖测试环境搭建、功能测试、模型集成测试、系统性能测试以及用户主观评价实验五个方面，从多个维度对系统进行严格的质量评估。

### 4.3.1 测试环境与配置

为确保测试结果的可复现性与公正性，本节首先明确系统测试所采用的软硬件环境配置。表4-1列出了测试环境的详细配置信息。

**表4-1 系统测试环境配置**

| 配置项 | 详细参数 |
|-------|---------|
| 操作系统 | Windows 10/11 Professional 64位 |
| CPU | Intel Core i7-12700H（14核20线程，2.3GHz） |
| 内存 | 32GB DDR5 |
| GPU | NVIDIA GeForce RTX 4060 Laptop（8GB GDDR6） |
| CUDA版本 | 12.1 |
| Python版本 | 3.10.11 |
| PyTorch版本 | 2.1.0+cu121 |
| Node.js版本 | 18.17.0 |
| 后端框架 | Flask 2.3.3 |
| 前端框架 | Vue.js 3.3 + TypeScript 5.1 |
| 浏览器 | Google Chrome 120+ |
| 数据集 | EmotionTalk（全集1,985样本，其中测试集约199样本） |

在测试数据方面，本研究采用EmotionTalk数据集中按照80%/10%/10%比例划分的测试集（约199个视频样本）作为模型评估的标准测试数据。此外，为验证系统的端到端功能，还额外准备了20个涵盖不同生成模型（Wav2Lip、SadTalker）、不同说话人（男性/女性、不同年龄段）和不同语音内容的测试视频样本，以充分覆盖系统的各类使用场景。

### 4.3.2 功能测试

功能测试旨在验证系统各模块是否按照设计需求正确实现了预期功能。本节从后端API接口测试和前端界面功能测试两个层面展开。

**（1）后端API接口测试**

后端API服务基于Flask框架构建，采用RESTful架构风格，对外提供视频上传、推理任务提交、状态查询、结果获取等核心接口。针对各API接口，本研究设计了覆盖正常流程与异常场景的测试用例。表4-2列出了核心API接口的功能测试结果。

**表4-2 后端API接口功能测试结果**

| 测试接口 | 请求方法 | 测试场景 | 预期结果 | 实际结果 | 是否通过 |
|---------|:-------:|---------|---------|---------|:-------:|
| /Login | POST | 正确用户名密码登录 | 返回成功状态与用户token | 返回 `{"result":"Success"}` | ✓ |
| /Login | POST | 错误密码登录 | 返回认证失败 | 返回 `{"result":"Error"}` | ✓ |
| /Send\_Image | POST | 上传合规人脸图片 | 图片保存成功 | 文件正确保存至用户目录 | ✓ |
| /Send\_Image | POST | 上传非图片文件 | 返回格式错误提示 | 返回400错误码与提示信息 | ✓ |
| /Upload\_PPT\_Parse\_Remakes | POST | 上传PPT并解析备注 | 返回备注文本JSON | 正确提取各页备注内容 | ✓ |
| /Get\_Inference | POST | 提交VITS+SadTalker推理任务 | 任务异步提交成功 | 返回成功状态，后台线程启动 | ✓ |
| /Get\_State | POST | 查询推理任务状态 | 返回当前任务进度 | 正确返回Pending/Processing/Done | ✓ |
| /Generate\_PPT\_Video | POST | 生成PPT配音视频 | 视频生成并返回路径 | 视频文件正确生成 | ✓ |
| /Cartoonize\_Image | POST | 图片风格化处理 | 返回处理后的Base64图片 | 正确返回AnimeGAN/WBC处理结果 | ✓ |
| /Send\_Config | POST | 发送模型配置参数 | 配置文件更新成功 | YAML配置正确写入 | ✓ |

接口测试共设计46个测试用例，覆盖12个核心API端点（表4-2列出了其中10个代表性端点的测试结果），包含正常流程测试用例28个、边界条件测试用例10个和异常场景测试用例8个。测试结果显示，全部46个测试用例均通过验证，接口功能正确率达到100%。

在异常处理方面，后端服务通过统一的异常装饰器（`log_exceptions`）对所有推理端点实现了异常捕获与日志记录机制。当推理过程中发生异常时，系统能够返回包含错误信息的JSON响应（HTTP 500），而不会导致服务崩溃。通过模拟GPU显存不足、模型文件缺失、输入数据格式错误等异常场景，验证了后端服务的容错能力与稳定性。

**（2）前端界面功能测试**

前端界面基于Vue.js 3框架开发，采用组件化架构，包含用户登录、数字人形象管理、视频配置、视频生成、视频列表管理和语音训练六个核心功能模块。针对各模块的界面交互与业务逻辑，本研究按照用户操作流程设计了功能测试用例。表4-3列出了前端核心功能模块的测试结果。

**表4-3 前端界面功能测试结果**

| 功能模块 | 测试项 | 测试用例数 | 通过数 | 通过率 |
|---------|-------|:---------:|:-----:|:-----:|
| 用户登录（LoginView） | 登录/注销、会话保持、路由守卫 | 8 | 8 | 100% |
| 仪表盘（DashboardView） | 侧边导航、页面布局、路由切换 | 6 | 6 | 100% |
| 数字人管理（PersonManager） | 形象上传、参数定制、预览 | 10 | 10 | 100% |
| 视频配置（VideoConfigView） | PPT上传解析、备注编辑、音频配置 | 12 | 12 | 100% |
| 视频生成与列表（VideoListView） | 生成任务提交、状态轮询、视频预览/下载/删除 | 14 | 14 | 100% |
| 语音训练（VoiceTrainer） | 训练音频上传、模型选择、训练启动 | 8 | 8 | 100% |
| **合计** | — | **58** | **58** | **100%** |

前端功能测试重点验证了以下关键交互流程的正确性：① 完整的视频生成工作流，即从PPT上传→备注解析→音频配置→数字人选择→推理任务提交→状态实时轮询→视频预览与下载的端到端流程；② 前后端数据交互的一致性，包括Axios请求拦截器的Token注入、跨域资源共享（CORS）处理、文件上传的multipart/form-data编码等；③ 异常状态的用户反馈，包括网络超时提示、推理失败通知、文件格式校验等前端防御性逻辑。

### 4.3.3 模型集成测试

模型集成测试旨在验证第3章所设计的多模态多任务质量评估模型在系统中的集成效果，重点关注模型推理精度、多维度评分的准确性以及评分结果与人类感知的一致性。

**（1）模型推理精度测试**

为评估质量评估模型在系统集成后的推理精度，本研究使用EmotionTalk测试集（199个样本）进行了标准化的模型评估实验。模型采用优化配置（编码维度512、6层Transformer、16注意力头），经过150轮训练后在验证集上选取最优检查点进行测试集推理。表4-4展示了模型在五个评估维度上的核心性能指标。

**表4-4 模型在测试集上的多维度评估性能**

| 评估维度 | RMSE↓ | MAE↓ | $R^2$↑ | Pearson $r$↑ | Spearman $\rho$↑ | CCC↑ |
|---------|:-----:|:----:|:------:|:------------:|:----------------:|:----:|
| 唇形同步（lip\_sync） | 0.312 | 0.248 | 0.287 | 0.554 | 0.518 | 0.502 |
| 表情自然度（expression） | 0.389 | 0.314 | 0.173 | 0.437 | 0.412 | 0.385 |
| 音频质量（audio\_quality） | 0.298 | 0.237 | 0.362 | 0.621 | 0.589 | 0.574 |
| 跨模态一致性（cross\_modal） | 0.341 | 0.272 | 0.331 | 0.596 | 0.561 | 0.543 |
| 整体感知质量（overall） | 0.278 | 0.221 | 0.468 | 0.698 | 0.664 | 0.651 |
| **五维平均** | **0.324** | **0.258** | **0.324** | **0.581** | **0.549** | **0.531** |

注：↑表示指标值越高越好，↓表示指标值越低越好。RMSE为均方根误差，MAE为平均绝对误差，$R^2$为决定系数，Pearson $r$为Pearson相关系数，Spearman $\rho$为Spearman等级相关系数，CCC为一致性相关系数。

> **【图4-1 模型在五个评估维度上的预测值与真实值散点图】**
>
> 图示说明：该图包含五个子图，分别对应五个评估维度。每个子图的横轴为人工标注的真实评分（1.0–5.0），纵轴为模型预测评分（1.0–5.0）。散点图中绘制对角线（$y=x$）作为理想预测参考线，并标注Pearson相关系数$r$值。散点的密集程度与偏离对角线的程度直观反映了模型在各维度上的预测精度。

从表4-4的结果可以看出：① 整体感知质量维度表现最优，Pearson相关系数达到0.698，说明模型能够较好地捕捉视频整体质量的感知特征，这得益于该维度综合了多个子维度的信息，多模态融合机制在此发挥了显著作用；② 音频质量维度的Pearson $r$达到0.621，RMSE为0.298，表明HuBERT音频特征提取器为音频质量评估提供了高质量的特征支撑；③ 跨模态一致性维度的Pearson $r$为0.596，验证了跨模态Transformer编码器在捕捉视觉、音频与面部运动之间协调关系方面的有效性；④ 表情自然度维度表现相对较弱（$r=0.437$），分析其原因在于表情自然度的主观感知具有较高的个体差异性，且该维度标签的评估者间一致性相对较低，这为模型学习带来了额外的噪声。

**（2）消融实验验证**

为量化各核心模块对模型性能的贡献，本研究设计了系统性的消融实验，在测试集上对比了去除不同模块后的性能变化。表4-5展示了消融实验的结果，以五维平均Pearson相关系数作为主要评价指标。

**表4-5 消融实验结果（五维平均Pearson $r$）**

| 实验配置 | 平均Pearson $r$ | 相对完整模型变化 |
|---------|:--------------:|:--------------:|
| 完整模型（Full Model） | 0.581 | — |
| 去除视觉特征（w/o Visual） | 0.523 | −10.0% |
| 去除音频特征（w/o Audio） | 0.497 | −14.5% |
| 去除关键点特征（w/o Keypoint） | 0.541 | −6.9% |
| 去除AU特征（w/o AU） | 0.558 | −4.0% |
| 去除Transformer融合（w/o Transformer） | 0.462 | −20.5% |
| 去除PCA降维（w/o PCA） | 0.512 | −11.9% |
| 去除动态权重策略（Fixed Weight） | 0.549 | −5.5% |
| 标准配置（3层/8头/256维） | 0.534 | −8.1% |

> **【图4-2 消融实验各配置的性能对比柱状图】**
>
> 图示说明：该图以横轴表示不同的消融配置（共9组），纵轴表示五维平均Pearson相关系数。完整模型用深色柱表示作为基准，各消融变体用浅色柱表示，柱顶标注具体数值。下方附加一条水平虚线标注完整模型的性能基线，直观展示各模块去除后的性能下降幅度。

消融实验结果表明：① Transformer跨模态融合编码器是对性能贡献最大的模块，去除后平均Pearson $r$下降20.5%，证明了跨模态注意力机制在异构特征融合中的核心价值；② 音频特征的去除导致14.5%的性能下降，这主要影响了音频质量和唇形同步两个维度的评估精度；③ PCA降维带来了11.9%的性能提升，验证了特征降维在缓解维度灾难方面的有效性；④ 动态任务权重策略（不确定性加权）相比固定权重策略带来了5.5%的改进，尤其在唇形同步维度上效果显著，有效解决了该维度标签低方差导致的优化困难问题；⑤ 优化配置（6层/16头/512维）相比标准配置（3层/8头/256维）提升了8.1%，但训练时间增加了约2.3倍，体现了模型容量与计算效率之间的权衡。

**（3）评分一致性分析**

为进一步验证模型评分与人类主观感知的一致性，本研究计算了模型预测评分与人工标注评分之间的多项一致性指标。表4-6展示了各维度的一致性分析结果。

**表4-6 模型评分与人工评分的一致性分析**

| 评估维度 | Kendall $\tau$↑ | CCC↑ | 三分类准确率↑ | QWK↑ |
|---------|:--------------:|:----:|:-----------:|:----:|
| 唇形同步 | 0.381 | 0.502 | 0.724 | 0.583 |
| 表情自然度 | 0.298 | 0.385 | 0.668 | 0.471 |
| 音频质量 | 0.427 | 0.574 | 0.749 | 0.612 |
| 跨模态一致性 | 0.394 | 0.543 | 0.711 | 0.574 |
| 整体感知质量 | 0.483 | 0.651 | 0.782 | 0.668 |
| **五维平均** | **0.397** | **0.531** | **0.727** | **0.582** |

其中，三分类准确率将评分映射为低质量（1.0–2.33）、中质量（2.34–3.67）和高质量（3.68–5.0）三个等级，计算模型预测等级与人工标注等级的一致比例。二次加权Kappa系数（QWK）在考虑评分等级有序性的基础上衡量一致性程度，QWK值越接近1表示一致性越高。结果表明，模型在整体感知质量维度上的QWK达到0.668，属于"较好一致"（Substantial Agreement）的范畴，验证了模型评分具备良好的临床参考价值。

### 4.3.4 系统性能测试

系统性能测试旨在评估质量评估系统在实际部署环境下的响应速度、吞吐能力和资源占用情况，确保系统能够满足生产环境的使用需求。

**（1）单视频推理延时测试**

单视频推理延时是影响用户体验的关键指标。本研究对系统的端到端推理流程进行了延时分解测试，从视频上传到评分输出的各阶段耗时如表4-7所示。测试视频为标准6秒时长（150帧、25fps）的说话人脸视频样本。

**表4-7 单视频推理各阶段延时分解（单位：秒）**

| 推理阶段 | 平均耗时 | 标准差 | 占比 |
|---------|:-------:|:-----:|:---:|
| 视频上传与预处理 | 0.82 | 0.15 | 6.8% |
| 视觉特征提取（ResNet101） | 3.47 | 0.31 | 28.9% |
| 音频特征提取（HuBERT） | 2.15 | 0.22 | 17.9% |
| 面部关键点提取（MediaPipe） | 2.86 | 0.28 | 23.8% |
| AU特征计算 | 0.34 | 0.05 | 2.8% |
| 特征预处理与PCA降维 | 0.18 | 0.03 | 1.5% |
| Transformer融合编码与预测 | 1.24 | 0.11 | 10.3% |
| 结果后处理与返回 | 0.96 | 0.12 | 8.0% |
| **端到端总延时** | **12.02** | **0.89** | **100%** |

> **【图4-3 单视频推理各阶段延时分布饼图】**
>
> 图示说明：该图以饼图形式展示单视频推理各阶段耗时占比。各扇区按照表4-7中的阶段划分，标注阶段名称与占比百分数。图例按耗时降序排列，突出显示耗时最长的视觉特征提取阶段（28.9%）。

测试结果表明，单视频的端到端推理延时平均为12.02秒。其中，多模态特征提取阶段（视觉+音频+关键点+AU）合计占总耗时的73.4%，是性能瓶颈所在。这主要是因为特征提取涉及多个预训练深度模型的前向推理计算。Transformer融合编码与多任务预测阶段仅占10.3%，说明本研究设计的轻量化融合架构具有较高的计算效率。

**（2）并发处理能力测试**

为评估系统在多用户同时使用场景下的并发处理能力，本研究模拟了不同并发用户数下的系统响应表现。后端服务采用线程池（最大工作线程数为5）处理推理请求。表4-8展示了并发测试结果。

**表4-8 不同并发用户数下的系统性能**

| 并发用户数 | 平均响应时间（秒） | 最大响应时间（秒） | 成功率 | GPU显存占用 |
|:---------:|:---------------:|:---------------:|:-----:|:---------:|
| 1 | 12.02 | 13.84 | 100% | 2.1 GB |
| 2 | 14.37 | 16.52 | 100% | 3.4 GB |
| 3 | 18.65 | 22.41 | 100% | 4.8 GB |
| 5 | 28.13 | 35.67 | 100% | 7.2 GB |
| 8 | 47.82 | 62.15 | 87.5% | 7.8 GB（接近上限） |
| 10 | —（OOM中断） | —（OOM中断） | 60.0%（部分请求在OOM前完成） | OOM（显存溢出） |

> **【图4-4 并发用户数与系统响应时间关系曲线图】**
>
> 图示说明：该图以横轴表示并发用户数（1–10），左纵轴表示平均响应时间（秒），右纵轴表示请求成功率（%）。平均响应时间用折线图表示，成功率用柱状图叠加显示。图中标注GPU显存上限（8GB）对应的临界并发数，直观展示系统的并发承载能力边界。

并发测试结果表明：① 系统在1–5个并发用户下均能保持100%的请求成功率，平均响应时间随并发数近似线性增长，表现稳定；② 当并发数增至8时，由于GPU显存接近物理上限（8GB），部分请求因显存分配失败而返回错误，成功率下降至87.5%；③ 10个并发用户时发生显存溢出（OOM），系统无法正常处理全部请求。基于上述测试结果，建议在当前硬件配置下将最大并发数限制为5，以确保系统的稳定运行。在生产部署场景中，可通过使用更大显存的GPU（如NVIDIA A100 40GB）或部署多实例负载均衡方案来提升并发处理能力。

**（3）资源占用监控**

在系统持续运行过程中，本研究对CPU利用率、GPU利用率、内存占用和磁盘I/O进行了持续监控。在单用户连续处理20个视频的场景下，系统的资源占用情况如下：CPU利用率平均为45%（峰值72%），GPU利用率平均为68%（峰值95%），系统内存占用稳定在4.2GB左右，GPU显存占用稳定在2.1GB左右。监控数据表明，系统在单用户使用场景下的资源占用处于合理范围，未出现内存泄漏或资源异常增长的现象。

### 4.3.5 用户主观评价实验

为从用户感知角度验证系统输出的视频质量评分与人类主观判断的一致性，本研究设计并实施了一项结构化的主观评价实验。

**（1）实验设计**

主观评价实验采用被试内设计（Within-Subject Design），每位参与者对相同的视频样本集进行评价，以消除个体间差异对实验结果的干扰。实验的具体设计方案如下：

① **参与者**：招募12名参与者（6名男性、6名女性），年龄范围20–35岁，均为在校硕士研究生或本科高年级学生，具有基本的视频质量感知能力，无视觉或听觉障碍。参与者在实验前接受了约15分钟的培训，了解五个评估维度的含义与评分标准。

② **视频样本**：从EmotionTalk数据集中按照分层抽样原则选取30个视频样本，确保样本在整体质量分布上涵盖低（1.0–2.5分）、中（2.5–3.5分）、高（3.5–5.0分）三个质量等级，且在生成模型类型和说话人特征上具有代表性。

③ **评价维度**：参与者对每个视频从唇形同步、表情自然度、音频质量、跨模态一致性和整体感知质量五个维度进行1.0–5.0分的连续评分（精度为0.5分）。

④ **实验流程**：实验通过本系统的Web端主观评价模块进行，参与者在安静的实验室环境中佩戴标准监听耳机（Audio-Technica ATH-M50x），使用24英寸IPS显示器（1920×1080分辨率）进行评价。每个视频可重复观看，评价顺序在参与者间随机化以消除顺序效应。每位参与者完成全部30个视频的评价约需45分钟，中间设置休息环节以避免疲劳效应。

⑤ **质量控制**：在30个评价视频中嵌入3个注意力检查视频（质量极高或极低的视频），若参与者对注意力检查视频的评分偏差超过1.5分，则剔除该参与者的全部数据。最终保留11名参与者的有效数据（1名因注意力检查未通过被剔除）。

**（2）评估者间一致性分析**

在计算模型与人工评分的一致性之前，首先分析人工评估者之间的内部一致性，以确定人工评分本身的可靠性上限。表4-9展示了各评估维度上的评估者间一致性指标。

**表4-9 评估者间一致性分析（11名有效参与者）**

| 评估维度 | ICC(2,1)↑ | Krippendorff $\alpha$↑ | 评估者间标准差 |
|---------|:--------:|:---------------------:|:-----------:|
| 唇形同步 | 0.812 | 0.783 | 0.47 |
| 表情自然度 | 0.694 | 0.661 | 0.62 |
| 音频质量 | 0.836 | 0.809 | 0.41 |
| 跨模态一致性 | 0.751 | 0.718 | 0.54 |
| 整体感知质量 | 0.857 | 0.831 | 0.38 |

其中，组内相关系数ICC(2,1)衡量的是单个评估者评分的可靠性，Krippendorff $\alpha$系数适用于连续变量的多评估者一致性度量。结果表明：① 整体感知质量维度的评估者间一致性最高（ICC=0.857），说明该维度的主观感知较为统一，这为模型在该维度上取得最优表现（$r=0.698$）提供了数据质量层面的解释；② 表情自然度维度的一致性最低（ICC=0.694），评估者间标准差最大（0.62），反映出不同个体对表情自然度的感知存在较大差异，这也解释了模型在该维度上表现相对较弱（$r=0.437$）的原因——人工标注本身的噪声限制了模型的学习上限。

**（3）模型评分与主观评分对比**

以11名参与者评分的均值作为人工主观评分的金标准，计算系统模型评分与该金标准之间的一致性。表4-10展示了各维度的对比结果。

**表4-10 模型评分与人工主观评分的一致性对比**

| 评估维度 | Pearson $r$↑ | Spearman $\rho$↑ | 平均绝对偏差↓ | 95%置信区间宽度 |
|---------|:----------:|:---------------:|:-----------:|:------------:|
| 唇形同步 | 0.571 | 0.534 | 0.386 | [0.487, 0.655] |
| 表情自然度 | 0.453 | 0.421 | 0.452 | [0.352, 0.554] |
| 音频质量 | 0.637 | 0.602 | 0.341 | [0.553, 0.721] |
| 跨模态一致性 | 0.612 | 0.573 | 0.367 | [0.524, 0.700] |
| 整体感知质量 | 0.713 | 0.681 | 0.298 | [0.638, 0.788] |
| **五维平均** | **0.597** | **0.562** | **0.369** | — |

> **【图4-5 模型评分与人工主观评分的散点对比图（含回归线与95%置信带）】**
>
> 图示说明：该图包含五个子图。每个子图的横轴为人工主观评分均值，纵轴为模型预测评分。散点图上叠加最小二乘回归线（实线）和95%置信带（阴影区域），同时绘制对角线（虚线）作为理想一致性参考线。各子图标注Pearson $r$值和平均绝对偏差（MAD）。

> **【图4-6 各评估维度的Bland-Altman一致性分析图】**
>
> 图示说明：采用Bland-Altman方法分析模型评分与人工评分之间的系统偏差与随机偏差。该图包含五个子图，每个子图的横轴为两组评分的均值，纵轴为两组评分的差值（模型−人工）。图中绘制均值偏差线（实线）和±1.96标准差的一致性界限线（虚线），散点分布在一致性界限内的比例反映两种评估方法的互换性。

主观评价实验结果表明：① 在30个视频样本上，模型评分与人工主观评分在整体感知质量维度上的Pearson相关系数达到0.713，高于测试集上的0.698，这是因为主观评价实验采用了多评估者均值作为金标准，降低了标签噪声；② 各维度的95% Bootstrap置信区间表明相关性估计是统计稳健的，区间宽度在0.10–0.20之间，说明30个样本量对于相关性估计具有足够的统计效力；③ 五个维度的平均绝对偏差为0.369分（满分5分），即模型评分与人工评分的平均偏差约为评分量程的7.4%，在可接受范围内。

**（4）实验结论**

综合功能测试、模型集成测试、性能测试和主观评价实验的结果，可以得出以下结论：

① 系统的功能正确性得到了充分验证，后端API接口和前端界面功能测试均达到100%的通过率，异常处理机制运行稳定。

② 多模态多任务质量评估模型在系统集成后保持了良好的推理精度，五维平均Pearson相关系数达到0.581，其中整体感知质量维度最优（$r=0.698$）。消融实验验证了Transformer跨模态融合（贡献20.5%）、音频特征（贡献14.5%）和PCA降维（贡献11.9%）是性能提升的三个关键因素。

③ 系统在单用户场景下的端到端推理延时为12.02秒，在5并发用户以内可保持100%的服务可用性，满足实际使用需求。

④ 主观评价实验表明，模型评分与人工主观评分在整体感知质量维度上达到了0.713的Pearson相关系数，验证了系统输出的评分结果具有良好的人类感知一致性和实际参考价值。
