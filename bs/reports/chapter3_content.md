# 第3章 多模态多任务说话人脸视频质量评估算法设计与实现

随着音频驱动的说话人脸视频生成技术（如Wav2Lip、SadTalker、GeneFace++等）在虚拟数字人、在线教育和视频配音等应用场景中的快速普及，如何对AI生成说话人脸视频的感知质量进行客观、全面且高效的自动化评估，已成为制约该领域进一步发展的关键瓶颈。与传统视频质量评估主要关注编码失真和信号保真度不同，AI生成说话人脸视频的质量评估需要聚焦于多个与人类感知高度相关的核心维度，包括口型与语音的同步精度、面部表情的自然流畅度、合成语音的音质清晰度，以及视觉、音频与面部运动之间的跨模态协调一致性。然而，现有的评价方法往往局限于单一维度的独立评估（如仅关注唇形同步的SyncNet），或高度依赖耗时且主观性强的人工评价流程，难以满足大规模、多维度联合评估的实际需求。为了解决上述问题，本章提出了一种基于Transformer的多模态多任务学习框架，旨在实现对AI生成说话人脸视频质量的细粒度、多维度联合评估。

具体而言，本章围绕多模态多任务说话人脸视频质量评估算法的设计与实现展开系统性阐述，主要包含以下四个方面的内容。

第一，在问题分析层面（3.1节），本章从三个关键技术挑战出发，深入剖析了AI生成说话人脸视频质量评估任务的核心难点：（1）多模态信息融合难题——视觉特征（基于ResNet101提取，维度为2048）、音频特征（基于HuBERT模型提取，维度为768）、面部关键点特征（基于MediaPipe Face Mesh提取468个三维关键点坐标，维度为1404）和面部动作单元特征（基于OpenFace提取，维度为17）四类异构模态特征在维度尺度和语义空间上存在显著差异，直接拼接或简单融合难以实现有效的跨模态语义对齐；（2）多任务评估的均衡性难题——唇形同步（lip\_sync）、表情自然度（expression）、音频质量（audio\_quality）、跨模态一致性（cross\_modal）和整体感知质量（overall）五个评估子任务之间存在严重的标签分布不均衡现象，其中lip\_sync任务的标签方差接近于零而其余任务的标签方差显著较大，这一特性导致简单固定权重的多任务学习策略难以同时有效优化各子任务；（3）高维特征空间下的训练效率与模型泛化难题——原始四类模态特征的总维度高达4237维，高维输入不仅增大了模型参数规模，还容易引发维度灾难问题，导致训练过程中的梯度不稳定与过拟合风险，如何在保持模型表达能力的同时有效缓解特征冗余并提升训练收敛效率，是本研究需要解决的关键算法设计问题。

第二，在算法设计层面（3.2节），本章系统性地提出了一套完整的多模态多任务评估算法方案。3.2.1小节给出了算法的总体设计方案与处理流程，描述了从原始视频输入到五维质量评分输出的完整计算管线，涵盖多模态特征提取、特征预处理与降维、跨模态融合编码以及多任务预测四个核心阶段。3.2.2小节详细阐述了多模态特征提取模块的设计与实现，该模块基于FeatureExtractor类和GPUOptimizedExtractor类，分别利用预训练的ResNet101、HuBERT、MediaPipe Face Mesh和OpenFace模型提取视觉、音频、关键点和动作单元四类特征，并辅以SyncNetFeatureExtractor计算唇形同步分数，同时通过主成分分析（PCA）将原始特征总维度从4237维压缩至367维，降维率达84.4\%，大幅降低了后续模型训练的计算开销。3.2.3小节重点介绍了本文提出的跨模态Transformer融合编码器，该编码器采用四路并行的特征嵌入层将异构模态特征映射至统一的语义空间，并通过可学习位置编码与多层多头自注意力机制实现跨模态信息的深层交互融合；同时，本文设计了双路径融合策略——统计拼接路径（Path A）与交互式Transformer路径（Path B）相互补充，以提升融合特征的鲁棒性与表达能力。3.2.4小节介绍了五个独立的任务特定预测头结构以及三种动态任务权重策略（固定权重策略、不确定性加权策略和GradNorm策略），用于解决多任务学习中的梯度冲突与收敛不均衡问题。3.2.5小节围绕多模态多任务模型的训练稳定性与收敛效率问题，提出了包含梯度检查点技术、动态损失缩放、梯度累积、自适应余弦退火学习率调度和多任务独立学习率调度在内的一系列训练优化策略，以提升模型在高维多任务场景下的训练效率与泛化性能。

第三，在实验设置层面（3.3节），本章详细描述了实验所采用的数据集、评估指标体系、实验环境配置、对比实验方案以及超参数设置。本研究基于EmotionTalk数据集（包含1,985个AI生成说话人脸视频样本）开展实验，采用80\%/10\%/10\%的比例划分训练集、验证集和测试集，并构建了包含均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）、中位数绝对误差（MEDAE）、决定系数（\( R^2 \)）、Pearson相关系数、Spearman等级相关系数、Kendall's \( \tau \) 系数、一致性相关系数（CCC）、三分类准确率和二次加权Kappa系数（QWK）在内的全面评估指标体系。

第四，在实验结果及分析层面（3.4节），本章从整体性能评估、消融实验、预测误差分析、主观评价一致性验证和不同配置对比五个方面全面展示并深入分析了实验结果。实验表明，本文所提出的多模态多任务评估模型在整体感知质量维度上达到了0.698的Pearson相关系数，在音频质量维度上达到了0.621，在跨模态一致性维度上达到了0.596，验证了所提方法在多维度视频质量评估任务上的有效性。

综上所述，本章通过系统性的问题分析、算法设计、实验验证与结果分析，构建了一个完整的基于跨模态Transformer的多任务说话人脸视频质量评估框架。该框架通过多模态特征融合与动态任务权重策略，有效解决了异构特征对齐和多任务均衡优化两大核心挑战，为后续第4章的系统集成、API服务部署与前端可视化界面开发奠定了坚实的算法基础与技术支撑。

## 3.1 问题分析

AI生成说话人脸视频质量评估是一项涉及多模态信息理解与多维度感知判断的复杂任务。与传统视频质量评估（Video Quality Assessment, VQA）侧重于编码失真、信号噪声比和结构相似性等底层视觉保真度指标不同，说话人脸视频的质量评估需要综合考量视觉呈现、音频表达、面部动态以及跨模态协调等多个与人类感知密切相关的高层语义维度。这一任务的特殊性使得传统的单模态、单任务评估方法难以胜任，要求研究者从多模态信息表征、多任务联合学习以及高维特征空间优化等多个层面进行系统性的算法设计。通过对本研究所采用的EmotionTalk数据集（包含1,985个AI生成说话人脸视频样本）以及现有评估方法的深入分析，本节从以下三个核心技术挑战出发，系统性地剖析当前AI生成说话人脸视频质量评估任务面临的关键难点。

### 3.1.1 多模态异构特征的语义对齐与融合难题

AI生成说话人脸视频的质量评估需要从视觉、音频、面部几何结构和面部动作等多个互补的信息通道中提取具有判别力的特征表示。在本研究中，针对每个视频样本（统一为150帧、25fps的标准序列），采用四类预训练模型分别提取不同模态的特征：

（1）**视觉特征**：采用在ImageNet数据集上预训练的ResNet101深度卷积网络作为视觉编码器，对每帧图像提取2048维的高层视觉语义表示。该特征能够有效捕捉面部纹理细节、渲染质量和视觉伪影等信息，对于评估合成人脸的视觉真实感具有重要作用。

（2）**音频特征**：采用自监督预训练的HuBERT（Hidden-Unit BERT）模型提取768维的音频嵌入表示。HuBERT模型通过大规模无标注语音数据的掩码预测任务学习到了丰富的语音语义信息，能够有效表征语音的清晰度、自然度以及韵律特征，为音频质量评估提供了高质量的特征支撑。

（3）**面部关键点特征**：采用MediaPipe Face Mesh模型提取每帧图像中468个面部三维关键点坐标，形成1404维（468×3）的面部几何结构特征。该特征精细地刻画了面部各区域（眼部、口部、轮廓等）的空间位置关系与运动轨迹，对于评估面部表情的自然流畅度和口型变化的合理性具有不可替代的作用。

（4）**面部动作单元特征**：采用OpenFace工具提取17维面部动作单元（Action Unit, AU）强度值。动作单元基于面部动作编码系统（Facial Action Coding System, FACS），以标准化的方式描述面部肌肉运动模式，能够量化表征微表情变化和情绪表达的细微差异。

上述四类特征的原始总维度高达4237维（2048+768+1404+17），且它们在维度尺度、数值范围和语义空间上存在显著的异构性差异。具体而言，视觉特征位于高维卷积语义空间中，数值分布呈非负特性；音频特征处于自监督表征学习的嵌入空间中，数值范围较为对称；面部关键点特征本质上是低层级的几何坐标信息，数值范围跨度大且与图像分辨率强相关；动作单元特征则是经过编码的动作强度标量，维度远低于其他模态。

对本研究数据集中各模态特征的统计分析进一步揭示了严重的数值尺度失衡现象。四类特征的数值范围差异超过三个数量级：关键点特征的数值范围约为[-30142, 30142]，视觉特征约为[0, 14210]，音频特征约为[-6741, 6749]，而动作单元特征约为[-941, 941]。

这种跨模态的尺度失衡意味着，若直接将四类原始特征进行简单拼接或逐元素融合，维度较高且数值范围较大的模态（如关键点和视觉特征）将在梯度更新中占据主导地位，而低维度模态（如动作单元特征）的信息贡献则容易被淹没，从而导致融合后的特征表示无法充分利用各模态的互补信息。此外，不同模态特征所处的语义空间本质上不可直接比较——卷积网络提取的视觉语义与自监督模型学习的音频嵌入之间缺乏天然的对应关系，简单的线性映射难以实现有效的跨模态语义对齐。因此，如何设计合理的特征预处理与跨模态融合机制，将异构模态特征映射至统一的语义表示空间并实现深层次的信息交互，是本研究需要解决的首要技术挑战。

### 3.1.2 多任务评估的标签不均衡与优化冲突难题

本研究将说话人脸视频质量评估建模为一个多任务回归问题，同时预测以下五个评估维度的质量分数（评分区间为1.0至5.0分）：

（1）**唇形同步质量**（lip\_sync）：衡量生成视频中口型运动与驱动语音之间的时序同步精度，是说话人脸视频最基本的质量要求。

（2）**表情自然度**（expression）：评估面部表情变化的真实感与流畅性，包括情绪表达的合理性和表情转换的平滑度。

（3）**音频质量**（audio\_quality）：评价合成或处理后语音的清晰度、自然度和音质保真度。

（4）**跨模态一致性**（cross\_modal）：综合评估视觉、音频与面部运动三个通道之间的协调一致程度。

（5）**整体感知质量**（overall）：从主观感知的角度对视频整体质量给出综合评分。

然而，对EmotionTalk数据集中五个维度标签分布的深入统计分析揭示了一个严重的标签不均衡问题。在全部1,985个视频样本中，唇形同步维度的标签值几乎完全相同（均值为4.763，标准差趋近于零），呈现出极端的低方差特性；与之形成鲜明对比的是，表情自然度（标准差约0.590）、音频质量（标准差约0.519）、跨模态一致性（标准差约0.566）和整体感知质量等其余维度的标签则呈现出正常的分布特征和合理的方差水平。这一现象表明，数据集中的说话人脸视频在唇形同步质量上高度一致（普遍达到较高水平），但在表情表达、音频品质和跨模态协调等维度上存在显著的质量差异。

上述标签分布的严重不均衡给多任务学习框架的设计带来了本质性的挑战。在标准的多任务学习范式中，总损失函数通常表示为各子任务损失的加权和：

$$L_{total} = \sum_{k=1}^{K} w_k \cdot L_k$$

其中 $K=5$ 为任务数量，$w_k$ 为第 $k$ 个任务的权重，$L_k$ 为对应的均方误差损失。当采用固定等权重策略（$w_k=1, \forall k$）时，由于唇形同步任务的标签方差趋近于零，其损失值量级远小于其他任务，导致该任务在总损失中的梯度贡献几乎可以忽略，模型将无法学习到有意义的唇形同步评估能力。反之，若为唇形同步任务设置过大的权重以补偿其低方差特性，则该任务的梯度信号可能反过来干扰其他任务的正常优化过程，引发负迁移（negative transfer）现象。

此外，数据集中标签的不完整性进一步加剧了多任务优化的复杂性。统计分析表明，除唇形同步维度的标签完整率为100%外，表情自然度的有效标签比例仅为72.8%，音频质量为77.8%，跨模态一致性和整体感知质量均为72.6%，其余样本的标签以无效值（-1.0）标记。这意味着不同任务在每个训练批次中的有效样本数量存在差异，进一步增大了多任务梯度均衡的难度。因此，如何设计自适应的动态任务权重策略，使模型能够在标签分布严重不均衡且部分缺失的条件下同时有效优化五个评估子任务，是本研究面临的第二个关键技术挑战。

### 3.1.3 高维特征空间下的训练效率与模型泛化难题

如前文所述，四类模态特征的原始总维度高达4237维，每个视频样本对应的特征序列形状为150×4237（150个时间步，每步4237维特征向量）。如此高维的输入特征空间给模型训练带来了多方面的挑战。

首先，**维度灾难**（curse of dimensionality）问题。在高维特征空间中，数据点之间的距离趋于均匀化，传统的距离度量和相似性计算逐渐失效。对于本研究所采用的中等规模数据集（1,985个样本，其中训练集仅约1,588个），这一问题尤为突出：高维输入意味着模型需要估计的参数数量与输入维度成正比增长，而有限的训练样本难以为如此多的参数提供充分的约束，极易导致模型在训练集上过拟合而在测试集上泛化性能下降。

其次，**特征冗余与噪声干扰**问题。在4237维的原始特征中，不同模态特征之间以及同一模态特征的不同维度之间可能存在大量的冗余信息。例如，面部关键点的1404维坐标特征中，许多关键点在帧间的运动具有高度的空间相关性，其独立信息量远低于名义维度所暗示的水平。此外，由于特征提取过程中视觉特征存在约0.08%的缺失值（共计2,445个NaN值需要通过中位数插补进行填充），插补引入的噪声也可能影响模型的学习效果。这些冗余和噪声维度不仅增加了不必要的计算开销，还可能干扰模型对真正具有判别力的特征模式的学习。

最后，**训练收敛效率**问题。高维输入特征经过嵌入层映射后，产生的梯度信号需要穿越更深、更宽的网络结构才能回传至各模态的嵌入层，梯度消失或梯度爆炸的风险随维度增加而加剧。同时，高维特征空间中损失函数的优化曲面更加复杂，存在更多的鞍点和局部极小值，使得基于随机梯度下降的优化算法更难以高效地收敛到全局最优解。当结合前述的多任务优化框架时，不同任务的梯度在高维空间中发生冲突的概率进一步增大，导致训练过程中出现振荡或收敛缓慢的现象。

因此，如何在保持各模态特征表达能力的前提下，通过合理的特征降维与预处理策略有效缓解维度灾难，并配合针对性的训练优化技术提升模型在高维多任务场景下的收敛效率和泛化性能，构成了本研究需要解决的第三个核心技术挑战。

### 3.1.4 本章技术方案概述

基于上述三个核心技术挑战的分析，本章提出构建一个基于跨模态Transformer的多模态多任务学习框架，以实现对AI生成说话人脸视频质量的全面、准确评估。该框架的核心设计思路如下：

针对多模态异构特征的语义对齐与融合难题，本研究首先通过特征级的标准化预处理（StandardScaler）消除不同模态间的数值尺度差异，然后利用主成分分析（PCA）将原始4237维特征压缩至367维（降维率达91.3%），在大幅降低特征冗余的同时保留95%以上的原始方差信息。在此基础上，设计四路并行的模态特定嵌入网络将降维后的异构特征映射至统一维度的语义空间，并通过多层多头自注意力机制实现跨模态信息的深层交互融合。

针对多任务评估的标签不均衡与优化冲突难题，本研究引入了自适应的动态任务权重策略，包括基于同方差不确定性的自动权重调节机制和基于梯度范数均衡的GradNorm策略，使模型能够根据各任务的训练进度和难度自动调整损失权重，从而在标签分布不均衡的条件下实现多任务的协同优化。同时，采用掩码机制处理无效标签，确保缺失标签不参与损失计算。

针对高维特征空间下的训练效率与模型泛化难题，本研究提出了一套综合性的训练优化方案，涵盖梯度检查点、混合精度训练、梯度累积、自适应学习率调度等多项技术，以提升模型在高维多任务场景下的训练稳定性和收敛效率，同时通过标签平滑、Dropout正则化和早停策略有效防止过拟合。

上述技术方案的详细设计与实现将在3.2节中展开阐述。

## 3.2 算法设计

### 3.2.1 算法设计方案

基于3.1节对三个核心技术挑战的深入分析，本节给出多模态多任务说话人脸视频质量评估算法的总体设计方案。该算法以端到端的计算管线形式组织，从原始视频输入出发，经过多模态特征提取、特征预处理与降维、跨模态Transformer融合编码以及多任务质量预测四个核心阶段，最终输出唇形同步、表情自然度、音频质量、跨模态一致性和整体感知质量五个维度的质量评分。图3-1展示了算法的总体架构与数据流向。

**（1）多模态特征提取阶段**

算法的第一阶段负责从原始视频中提取多模态感知特征。给定一个输入视频文件，系统首先将其统一采样为150帧（帧率25fps）的标准序列，并通过FFmpeg工具分离出对应的音频流（采样率16000Hz）。随后，四个独立的预训练特征提取器并行工作，分别从视频帧序列和音频信号中提取四类异构模态特征。

在视觉与音频模态方面，视觉特征提取器基于在ImageNet数据集上预训练的ResNet101卷积网络，对每帧224×224的人脸图像提取2048维的高层视觉语义表示，形成维度为150×2048的视觉特征矩阵；音频特征提取器基于自监督预训练的HuBERT模型，对16000Hz采样率的音频信号提取768维的语音嵌入表示，形成维度为150×768的音频特征矩阵。

在面部几何与动作模态方面，面部关键点提取器基于MediaPipe Face Mesh模型，检测每帧图像中468个面部三维关键点坐标，形成维度为150×1404的几何结构特征矩阵；面部动作单元提取器基于OpenFace工具，计算每帧17个动作单元的激活强度值，形成维度为150×17的动作单元特征矩阵。此外，SyncNet同步检测模块计算音视频之间的同步置信度分数与时序偏移量，作为辅助参考信息。

经过该阶段处理后，每个视频样本被表示为四组时序特征矩阵，原始特征总维度为4237维（2048+768+1404+17）。

**（2）特征预处理与降维阶段**

算法的第二阶段针对3.1.1节和3.1.3节分析的多模态尺度失衡与高维特征冗余问题，对原始特征进行系统性的预处理与降维。该阶段包含三个关键步骤：

第一步为缺失值处理。由于特征提取过程中部分视频帧可能存在遮挡、模糊等问题，导致视觉特征中约0.08%的元素值缺失（共计2,445个NaN值）。本研究采用基于中位数的插补策略（median imputation）填充缺失值，以避免极端离群值对后续计算的干扰。

第二步为特征标准化。如3.1.1节所述，四类模态特征的数值范围存在三个数量级以上的差异。为消除这种尺度失衡，本研究对每类模态特征独立应用Z-score标准化（StandardScaler），将各维度特征的均值调整为0、标准差调整为1，确保不同模态特征在统一的数值尺度上参与后续的融合与学习过程。

第三步为主成分分析降维。为有效缓解高维特征空间带来的维度灾难和特征冗余问题，本研究采用主成分分析（Principal Component Analysis, PCA）对标准化后的各模态特征进行降维压缩。具体而言，面部关键点特征从1404维降至50维，保留了约85%的原始方差信息；视觉特征从2048维降至100维；音频特征从768维降至200维；动作单元特征由于原始维度较低（17维），保持不变。降维后的四类特征总维度从4237维压缩至367维（100+200+50+17），降维率达到91.3%，大幅降低了后续模型的参数规模与计算复杂度，同时通过去除冗余维度有效提升了特征的信噪比。

**（3）跨模态Transformer融合编码阶段**

算法的第三阶段是整个框架的核心组件，负责将经过预处理和降维的四类异构模态特征映射至统一的语义空间，并通过深层注意力机制实现跨模态信息的充分交互融合。该阶段的设计思路如下：

首先，四路并行的模态特定嵌入网络分别将各模态的低维特征映射至统一的高维语义空间。每路嵌入网络采用两层全连接结构，中间引入ReLU激活函数和Dropout正则化层，将维度各异的模态特征（100维、200维、50维、17维）统一映射为512维的语义嵌入向量。四路嵌入后的特征矩阵形状均为150×512，实现了异构模态在维度上的对齐。

随后，四路嵌入特征通过逐元素平均操作融合为单一的150×512维特征序列，并叠加可学习的位置编码向量以注入时序位置信息。

融合后的特征序列被送入多层Transformer编码器进行深层语义编码。Transformer编码器采用6层编码层，每层包含16个注意力头的多头自注意力模块和维度为2048的前馈网络，通过全局自注意力机制使每个时间步的特征表示能够同时关注序列中所有其他时间步的信息，从而有效捕捉跨模态、跨时间步的长程依赖关系。

最后，编码器输出的150×512维特征序列经全局平均池化操作沿时间维度聚合为单一的512维特征向量，作为该视频样本的全局多模态融合表示，供后续的多任务预测头使用。

**（4）多任务质量预测阶段**

算法的第四阶段基于融合后的全局特征向量，通过五个独立的任务特定预测头分别输出五个质量维度的评分。每个预测头采用三层全连接网络结构（512→256→128→1），中间层引入ReLU激活函数和Dropout正则化，最后通过Sigmoid激活函数将输出映射至[0, 1]区间，再经线性缩放映射至[1.0, 5.0]的评分区间。五个预测头共享相同的网络结构但参数完全独立，使得每个质量维度的预测能够学习到针对该维度的特定映射模式，同时通过共享的Transformer编码器特征实现多任务间的隐式信息传递。

在训练阶段，总损失函数定义为各子任务加权均方误差损失之和。为应对3.1.2节分析的标签不均衡与缺失问题，本研究引入了动态任务权重策略和标签掩码机制：对于标注为无效值（-1.0）的标签，通过布尔掩码排除其参与损失计算；对于不同任务间的损失量级差异，通过基于不确定性的自适应权重或基于梯度范数均衡的GradNorm策略动态调节各任务的权重系数，确保模型能够在标签分布严重不均衡的条件下同时有效优化五个评估子任务。

在推理阶段，给定一个待评估的视频文件，系统依次执行上述四个阶段的前向计算，最终输出五个维度的质量评分，整个推理过程无需人工干预，实现了全自动化的多维度视频质量评估。

**（5）总体设计特点**

综合来看，本算法的总体设计方案具有以下三个显著特点：第一，模块化设计，各阶段功能职责明确、接口定义清晰，便于独立开发调试与灵活组合；第二，两套配置体系，标准配置（编码维度256、3层Transformer、8注意力头）适用于快速原型验证，优化配置（编码维度512、6层Transformer、16注意力头）适用于追求最优性能的正式评估，两套配置通过统一的YAML配置文件切换，无需修改任何代码；第三，端到端可训练，从特征嵌入层到多任务预测头的所有参数均可通过反向传播联合优化，使得模型能够自适应地学习最有利于多维度质量评估的特征表示与预测映射。上述设计方案的各核心模块将在后续3.2.2至3.2.5小节中逐一展开详细阐述。

### 3.2.2 多模态特征提取模块

多模态特征提取是整个评估算法的基础环节，其核心目标是从原始说话人脸视频中提取出能够充分表征视觉质量、语音特性、面部动态以及跨模态协调性的多维特征表示。本节详细阐述多模态特征提取模块的设计与实现，包括四类模态特征的提取方法、唇形同步分数的计算方式，以及面向后续模型训练的特征预处理与降维流程。图3-2展示了多模态特征提取模块的整体架构。

#### 3.2.2.1 视觉特征提取

视觉特征负责捕捉生成视频中人脸区域的纹理细节、渲染质量与视觉伪影等信息，是评估合成人脸视觉真实感的关键依据。本研究采用在ImageNet大规模图像分类数据集上预训练的ResNet101深度残差卷积网络作为视觉特征编码器。

给定一个输入视频 $V$，系统首先将其统一采样为 $T=150$ 帧（目标帧率 $f_{\text{target}}=25$ fps）的标准序列。采样过程中，根据原始视频帧率 $f_{\text{orig}}$ 计算采样间隔 $\Delta = \max(1, \lfloor f_{\text{orig}} / f_{\text{target}} \rceil)$，以均匀间隔抽取视频帧。若采样后帧数超过150帧，则通过均匀下采样截取至目标长度；若不足150帧，则以零向量填充至标准长度，确保所有样本具有统一的时序维度。

对于第 $t$ 帧图像 $I_t$（$t=1,2,\ldots,T$），系统首先通过RetinaFace人脸检测器定位面部区域，随后将裁剪后的人脸图像缩放至 $224\times224$ 像素的标准输入分辨率，并按照ImageNet预训练模型的要求进行归一化处理。归一化过程为：

$$\hat{I}_t^{(c)} = \frac{I_t^{(c)} - \mu_c}{\sigma_c}, \quad c \in \{R, G, B\}$$

其中 $\mu = [0.485, 0.456, 0.406]$ 和 $\sigma = [0.229, 0.224, 0.225]$ 分别为ImageNet数据集各通道的均值和标准差。

经过归一化的帧图像被送入ResNet101网络。ResNet101由一个初始卷积层、四个残差块组（分别包含3、4、23、3个残差单元）和一个全局平均池化层构成。本研究移除网络最后的全连接分类层，以全局平均池化层的输出作为视觉特征向量。对于每帧输入，ResNet101的特征提取过程可表示为：

$$\mathbf{v}_t = \text{GAP}(\text{ResBlock}_4(\text{ResBlock}_3(\text{ResBlock}_2(\text{ResBlock}_1(\text{Conv}_1(\hat{I}_t)))))) \in \mathbb{R}^{2048}$$

其中 $\text{GAP}(\cdot)$ 表示全局平均池化操作，$\text{ResBlock}_i$ 表示第 $i$ 个残差块组。最终，每个视频样本的视觉特征矩阵为 $\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_T]^\top \in \mathbb{R}^{150 \times 2048}$。

为提升批量处理效率，在GPU加速模式下，系统采用分组推理策略：将150帧按每组8帧进行批处理，利用GPU的并行计算能力一次性提取多帧特征，并通过自动混合精度（Automatic Mixed Precision, AMP）技术在前向传播中使用半精度浮点数（FP16），将单帧推理的显存开销降低约50%。

#### 3.2.2.2 音频特征提取

音频特征负责表征生成视频中语音信号的清晰度、自然度和韵律特征，为音频质量维度的评估提供核心特征支撑。本研究采用Meta AI提出的HuBERT（Hidden-Unit BERT）自监督预训练语音模型作为音频特征编码器。HuBERT模型通过在大规模无标注语音数据上进行掩码预测任务的预训练，学习到了丰富的语音语义表示，在语音识别、情感识别和说话人验证等下游任务上展现出优异的迁移性能。

音频特征的提取流程如下：首先，通过FFmpeg工具从输入视频中分离出音频流，并将其转换为采样率 $f_s = 16000$ Hz、单声道的PCM格式波形信号。该采样率与HuBERT预训练时的设置一致，可避免重采样引入的信号失真。音频加载过程利用librosa音频处理库实现，将原始音频信号归一化为 $[-1, 1]$ 范围的浮点波形序列 $\mathbf{a} = [a_1, a_2, \ldots, a_N]$，其中 $N = f_s \times D$ 为总采样点数，$D$ 为音频时长（单位：秒）。

随后，波形信号经过Wav2Vec2特征提取器（Wav2Vec2FeatureExtractor）进行预处理，包括帧级分段和归一化，生成模型所需的输入张量。处理后的音频输入被送入HuBERT-Base模型（facebook/hubert-base-ls960），该模型包含7层卷积特征编码器和12层Transformer编码器。本研究提取模型最后一层Transformer编码器的隐藏状态作为音频特征表示：

$$\mathbf{H}_{\text{audio}} = \text{HuBERT}(\mathbf{a}) = \text{Transformer}_{12}(\text{CNN}(\mathbf{a})) \in \mathbb{R}^{T_a \times 768}$$

其中 $T_a$ 为HuBERT输出的时间步数，768为模型的隐藏维度。由于HuBERT的卷积特征编码器对输入波形进行了约320倍的降采样（每20ms一帧），$T_a$ 与视频帧数 $T=150$ 通常不一致。为实现音视频特征在时间维度上的对齐，本研究采用线性插值方法将音频特征序列统一调整为150个时间步：

$$\mathbf{A} = \text{Interpolate}(\mathbf{H}_{\text{audio}}, T) \in \mathbb{R}^{150 \times 768}$$

其中 $\text{Interpolate}(\cdot, T)$ 表示沿时间轴将特征序列线性插值至长度 $T$ 的操作。

#### 3.2.2.3 面部关键点特征提取

面部关键点特征通过精细的面部几何结构信息刻画口型变化、表情运动和面部轮廓形变等动态特征，对于评估唇形同步精度和表情自然度具有不可替代的作用。本研究采用Google提出的MediaPipe Face Mesh模型提取面部三维关键点。

MediaPipe Face Mesh是一个轻量级的实时面部关键点检测模型，能够在单张图像中检测468个面部三维关键点，覆盖面部的眉毛、眼睛、鼻子、嘴唇、下颌轮廓等所有关键区域。模型的检测置信度阈值设置为0.5，即仅保留置信度高于50%的检测结果。

对于视频序列的第 $t$ 帧图像 $I_t$，MediaPipe Face Mesh输出468个关键点的三维坐标：

$$\mathbf{P}_t = \{(x_i^t, y_i^t, z_i^t)\}_{i=1}^{468}$$

其中 $(x_i^t, y_i^t)$ 为第 $i$ 个关键点在图像平面上的归一化坐标（值域为 $[0, 1]$），$z_i^t$ 为相对深度估计值。将每帧的468个三维坐标展平为一维向量，得到1404维（$468 \times 3$）的关键点特征：

$$\mathbf{k}_t = [x_1^t, y_1^t, z_1^t, x_2^t, y_2^t, z_2^t, \ldots, x_{468}^t, y_{468}^t, z_{468}^t]^\top \in \mathbb{R}^{1404}$$

对于未检测到人脸的帧（如严重遮挡或画面模糊），系统以零向量填充以保持序列完整性。最终，每个视频样本的关键点特征矩阵为 $\mathbf{K} = [\mathbf{k}_1, \mathbf{k}_2, \ldots, \mathbf{k}_T]^\top \in \mathbb{R}^{150 \times 1404}$。

面部关键点特征的独特价值在于其提供了精确的面部几何拓扑信息。不同于视觉特征侧重于纹理和外观语义，关键点特征直接编码了面部各部位的空间位置关系和帧间运动轨迹。例如，口唇区域关键点的时序运动模式可以直接反映口型变化的节奏与幅度，为唇形同步质量评估提供了显式的几何线索；眼部和眉毛区域关键点的协调运动则体现了表情变化的自然性，对表情自然度评估具有重要的判别作用。

#### 3.2.2.4 面部动作单元特征提取

面部动作单元（Action Unit, AU）是面部动作编码系统（Facial Action Coding System, FACS）中定义的基本面部肌肉运动单位，以标准化的方式量化描述面部表情的细微变化。本研究提取17个核心动作单元的激活强度值作为面部动作特征，用于捕捉微表情变化和情绪表达的细粒度差异。

与直接调用OpenFace工具不同，本研究基于MediaPipe Face Mesh检测到的468个面部关键点，通过几何距离计算方法近似估计各动作单元的激活强度。这一设计避免了对额外外部工具的依赖，同时保证了特征提取流程的统一性与可控性。

具体而言，对于第 $t$ 帧的关键点集合 $\mathbf{P}_t = \{(x_i^t, y_i^t, z_i^t)\}_{i=1}^{468}$，各动作单元的强度值通过特定关键点对之间的欧氏距离计算获得。以下给出部分核心动作单元的计算方式。

对于眉部区域的动作单元，AU1（内侧眉毛上提）通过左右内侧眉毛关键点与鼻根关键点之间距离的均值表征：

$$\text{AU1}_t = \frac{1}{2}\left(d(\mathbf{p}_{\text{brow\_inner\_L}}^t, \mathbf{p}_{\text{nose}}^t) + d(\mathbf{p}_{\text{brow\_inner\_R}}^t, \mathbf{p}_{\text{nose}}^t)\right)$$

其中 $d(\cdot, \cdot)$ 为欧氏距离函数。AU2（外侧眉毛上提）的计算方式类似，但采用外侧眉毛关键点。AU4（眉毛下压）则通过左右眉毛中部关键点之间的距离表征。

对于眼部区域的动作单元，AU5（上眼睑提升）通过左右眼睛的上下眼睑关键点距离的均值表征：

$$\text{AU5}_t = \frac{1}{2}\left(d(\mathbf{p}_{\text{upper\_lid\_L}}^t, \mathbf{p}_{\text{lower\_lid\_L}}^t) + d(\mathbf{p}_{\text{upper\_lid\_R}}^t, \mathbf{p}_{\text{lower\_lid\_R}}^t)\right)$$

AU7（眼睑收紧）定义为AU5的倒数，即 $\text{AU7}_t = 1/(\text{AU5}_t + \epsilon)$，其中 $\epsilon = 10^{-6}$ 为防止除零的微小常数。

对于口唇区域的动作单元，AU12（唇角拉伸）通过左右嘴角关键点与参考点之间距离的均值表征，AU20（嘴唇横向拉伸）通过左右嘴角之间的距离表征，AU25（双唇分开）通过上下唇关键点之间的距离表征：

$$\text{AU25}_t = d(\mathbf{p}_{\text{upper\_lip}}^t, \mathbf{p}_{\text{lower\_lip}}^t)$$

AU26（下颌张开）定义为AU25的线性放大，即 $\text{AU26}_t = 1.2 \times \text{AU25}_t$，反映下颌运动的幅度通常大于唇部分开程度。

表3-1汇总了本研究提取的17个动作单元的定义与计算方法。

**表3-1 17个面部动作单元的定义与计算方法**

| 动作单元 | 名称 | 计算方法 |
|---------|------|---------|
| AU1 | 内侧眉毛上提 | 内侧眉毛与鼻根距离的均值 |
| AU2 | 外侧眉毛上提 | 外侧眉毛与鼻根距离的均值 |
| AU4 | 眉毛下压 | 左右眉毛中部的间距 |
| AU5 | 上眼睑提升 | 上下眼睑距离的均值 |
| AU6 | 面颊上提 | 眼睛至嘴角距离的均值 |
| AU7 | 眼睑收紧 | $1/(\text{AU5} + \epsilon)$ |
| AU9 | 鼻部皱缩 | 鼻翼关键点间距 |
| AU10 | 上唇上提 | 上唇与鼻尖距离 |
| AU12 | 唇角拉伸 | 嘴角与参考点距离的均值 |
| AU14 | 酒窝形成 | $1/(\text{AU12} + \epsilon)$ |
| AU15 | 唇角下拉 | $0.8 \times \text{AU14}$ |
| AU17 | 下颏上提 | 下唇与鼻尖距离 |
| AU20 | 嘴唇横向拉伸 | 左右嘴角间距 |
| AU23 | 嘴唇收紧 | $1/(d(\text{upper\_lip}, \text{lower\_lip}) + \epsilon)$ |
| AU25 | 双唇分开 | 上下唇间距 |
| AU26 | 下颌张开 | $1.2 \times \text{AU25}$ |
| AU28 | 嘴唇内卷 | $1/(\text{AU20} + \epsilon)$ |

提取完成后，每帧的17维AU强度向量经过最大值归一化处理，将各维度特征值映射至 $[0, 1]$ 区间：

$$\hat{\mathbf{u}}_t = \frac{\mathbf{u}_t}{\max(\mathbf{u}_t) + \epsilon}$$

其中 $\mathbf{u}_t = [\text{AU1}_t, \text{AU2}_t, \ldots, \text{AU28}_t]^\top \in \mathbb{R}^{17}$ 为归一化前的原始AU特征向量。最终，每个视频样本的动作单元特征矩阵为 $\mathbf{U} = [\hat{\mathbf{u}}_1, \hat{\mathbf{u}}_2, \ldots, \hat{\mathbf{u}}_T]^\top \in \mathbb{R}^{150 \times 17}$。

#### 3.2.2.5 唇形同步分数计算

除上述四类帧级时序特征外，本研究还引入了基于SyncNet模型的唇形同步分数作为辅助评估信息。SyncNet是一个经典的音视频同步检测网络，通过对比学习方法训练，能够判断视频中口型运动与语音信号之间的时序同步程度。

唇形同步分数的计算过程如下：首先，将预训练的SyncNet模型加载至计算设备，并将输入视频与对应音频送入模型进行同步分析。模型输出包括同步置信度 $c \in \mathbb{R}$ 和时序偏移量 $o \in \mathbb{Z}$（单位：帧）。置信度值越高表示音视频同步性越好，偏移量反映口型运动与语音信号之间的时序偏移程度。

为将置信度值映射至统一的 $[0, 5]$ 评分区间，本研究设计了基于Sigmoid函数的非线性映射：

$$s_{\text{sync}} = \frac{5}{1 + \exp(-10 \cdot (\hat{c} - 0.5))}$$

其中 $\hat{c}$ 为归一化后的置信度值。该映射函数以0.5为中心点，通过陡峭的Sigmoid曲线（温度系数为10）将置信度值压缩至 $[0, 5]$ 区间，使得同步质量较高的视频获得接近5分的评分，而同步质量较差的视频评分趋近于0分。最终输出为二维向量 $[s_{\text{sync}}, o]$，分别表示同步分数与帧偏移量。

#### 3.2.2.6 特征预处理与降维

经过上述多模态特征提取后，每个视频样本被表示为四组时序特征矩阵：视觉特征 $\mathbf{V} \in \mathbb{R}^{150 \times 2048}$、音频特征 $\mathbf{A} \in \mathbb{R}^{150 \times 768}$、关键点特征 $\mathbf{K} \in \mathbb{R}^{150 \times 1404}$ 和动作单元特征 $\mathbf{U} \in \mathbb{R}^{150 \times 17}$。如3.1节所分析，这些原始特征在数值尺度和维度上存在显著差异，且高维特征空间容易引发维度灾难与过拟合问题。因此，在送入后续的跨模态Transformer融合编码器之前，需要对原始特征进行系统性的预处理与降维。

**（1）缺失值处理**

在大规模特征提取过程中，部分视频帧可能因严重遮挡、画面模糊或模型检测失败等原因导致特征值缺失（表现为NaN值）。统计分析表明，本研究数据集中视觉特征存在约2,445个NaN值（占总特征元素的0.08%）。为处理这些缺失值，本研究采用基于中位数的特征插补策略。

具体而言，对于特征矩阵中的每一个维度 $j$（$j=1,2,\ldots,d$），计算该维度上所有非缺失值的中位数 $m_j$，并用 $m_j$ 替换该维度上的所有NaN值：

$$\tilde{f}_{t,j} = \begin{cases} f_{t,j}, & \text{if } f_{t,j} \text{ is valid} \\ m_j = \text{Median}(\{f_{\tau,j} \mid f_{\tau,j} \text{ is valid}\}), & \text{if } f_{t,j} = \text{NaN} \end{cases}$$

选择中位数而非均值作为插补值，是因为中位数对离群值具有更强的鲁棒性，可以避免极端值对填充结果的偏倚影响。

**（2）特征标准化**

如3.1.1节所述，四类模态特征的数值范围跨越三个数量级以上。为消除这种尺度差异，本研究对每类模态特征独立应用Z-score标准化（StandardScaler）。对于模态 $m$ 的特征矩阵 $\mathbf{F}^{(m)} \in \mathbb{R}^{N \times d_m}$（$N$ 为样本数与时间步的乘积，$d_m$ 为该模态的特征维度），标准化过程为：

$$\hat{f}_{i,j}^{(m)} = \frac{f_{i,j}^{(m)} - \mu_j^{(m)}}{\sigma_j^{(m)} + \epsilon}$$

其中 $\mu_j^{(m)}$ 和 $\sigma_j^{(m)}$ 分别为模态 $m$ 第 $j$ 维特征在训练集上的均值和标准差，$\epsilon$ 为防止除零的微小常数。标准化后，各模态各维度特征的均值为0、标准差为1，有效消除了跨模态的数值尺度差异，确保不同模态特征在后续的融合与学习过程中具有均等的贡献机会。

需要注意的是，标准化参数 $(\mu_j^{(m)}, \sigma_j^{(m)})$ 仅在训练集上估计，并在验证集和测试集上直接应用，以避免数据泄露（data leakage）对模型评估结果的污染。

**（3）主成分分析降维**

为有效缓解高维特征空间带来的维度灾难问题，本研究采用主成分分析（PCA）对标准化后的各模态特征进行降维压缩。PCA通过正交线性变换将原始特征投影至方差最大的子空间上，在降低特征维度的同时最大化保留原始数据的信息量。

对于标准化后的模态特征矩阵 $\hat{\mathbf{F}}^{(m)} \in \mathbb{R}^{N \times d_m}$，PCA首先计算其协方差矩阵 $\mathbf{C}^{(m)} = \frac{1}{N-1} \hat{\mathbf{F}}^{(m)\top} \hat{\mathbf{F}}^{(m)} \in \mathbb{R}^{d_m \times d_m}$，然后对协方差矩阵进行特征值分解：

$$\mathbf{C}^{(m)} = \mathbf{W}^{(m)} \boldsymbol{\Lambda}^{(m)} \mathbf{W}^{(m)\top}$$

其中 $\boldsymbol{\Lambda}^{(m)} = \text{diag}(\lambda_1^{(m)}, \lambda_2^{(m)}, \ldots, \lambda_{d_m}^{(m)})$ 为特征值对角矩阵（$\lambda_1^{(m)} \geq \lambda_2^{(m)} \geq \cdots \geq \lambda_{d_m}^{(m)} \geq 0$），$\mathbf{W}^{(m)}$ 为对应的特征向量矩阵。选取前 $d_m'$ 个最大特征值对应的特征向量构成投影矩阵 $\mathbf{W}_{d_m'}^{(m)} \in \mathbb{R}^{d_m \times d_m'}$，降维后的特征为：

$$\tilde{\mathbf{F}}^{(m)} = \hat{\mathbf{F}}^{(m)} \mathbf{W}_{d_m'}^{(m)} \in \mathbb{R}^{N \times d_m'}$$

降维后所保留的方差比例（累计解释方差比）为：

$$\rho^{(m)} = \frac{\sum_{i=1}^{d_m'} \lambda_i^{(m)}}{\sum_{i=1}^{d_m} \lambda_i^{(m)}}$$

表3-2汇总了各模态特征的PCA降维配置与降维结果。

**表3-2 各模态特征的PCA降维配置**

| 模态特征 | 原始维度 $d_m$ | 目标维度 $d_m'$ | 降维率 | 累计解释方差比 $\rho^{(m)}$ |
|---------|:-------------:|:--------------:|:-----:|:-------------------------:|
| 视觉特征 | 2048 | 100 | 95.1% | ≥ 95% |
| 音频特征 | 768 | 200 | 74.0% | ≥ 95% |
| 关键点特征 | 1404 | 50 | 96.4% | ≥ 85% |
| 动作单元特征 | 17 | 17（不降维） | 0% | 100% |
| **合计** | **4237** | **367** | **91.3%** | — |

从表3-2可以看出，PCA降维在大幅压缩特征维度的同时保留了绝大部分原始方差信息。其中，关键点特征的降维率最高（96.4%），这与3.1.3节分析的面部关键点在帧间运动具有高度空间相关性的特性一致——相邻关键点的运动轨迹高度相关，大量的独立信息可被少数主成分有效捕获。动作单元特征由于原始维度仅为17维，无需进一步降维。

降维后的四类特征总维度从4237维压缩至367维，降维率达91.3%，这不仅大幅降低了后续跨模态Transformer融合编码器的参数规模和计算复杂度，还通过去除冗余维度和噪声维度有效提升了特征的信噪比，为模型在有限训练样本条件下的泛化性能提供了有利保障。

与PCA类似，降维所需的投影矩阵 $\mathbf{W}_{d_m'}^{(m)}$ 仅在训练集上通过特征值分解计算获得，验证集和测试集直接使用训练集上求得的投影矩阵进行变换，以严格避免数据泄露。

#### 3.2.2.7 GPU加速与批量处理优化

为满足大规模视频数据集的高效特征提取需求，本研究设计了基于GPU并行计算的加速特征提取方案。该方案通过GPUOptimizedExtractor类实现，在保持特征提取质量不变的前提下，显著提升了处理效率。

GPU加速方案包含以下关键优化策略：

第一，批量推理。将多个视频的帧图像组织为批量张量（默认批量大小为8），送入ResNet101和HuBERT模型进行并行前向推理，充分利用GPU的大规模并行计算能力。具体而言，视觉特征提取阶段将每个视频的150帧按每组8帧进行分批处理，通过PyTorch的DataLoader实现高效的数据加载与批量组织，并采用固定内存（pin_memory）加速CPU-GPU数据传输。

第二，自动混合精度训练。在前向推理阶段启用PyTorch的AMP（torch.cuda.amp.autocast）模块，对模型计算自动选择FP16或FP32精度。矩阵乘法和卷积运算采用FP16以降低显存占用和提升吞吐量，而累加和归一化运算保持FP32以维持数值稳定性。

第三，多线程音频加载。利用线程池（ThreadPoolExecutor，最大线程数为4）并行加载多个视频的音频数据，将I/O等待时间与GPU计算时间重叠，减少数据加载瓶颈对整体处理流程的影响。

第四，模型预热。在正式推理前，通过发送随机输入进行一次空推理（model warmup），触发CUDA核函数的即时编译与优化，确保后续推理以最优性能运行。

通过上述优化策略的协同作用，GPU加速模式下的特征提取速度相比纯CPU模式提升了约3至5倍，使得对包含近2,000个视频样本的完整数据集进行全量特征提取的耗时降低至可接受的范围内。

#### 3.2.2.8 本节小结

本节详细阐述了多模态特征提取模块的设计与实现。该模块以FeatureExtractor为统一调度接口，集成了视觉特征提取器（ResNet101，2048维）、音频特征提取器（HuBERT，768维）、面部关键点提取器（MediaPipe Face Mesh，1404维）、面部动作单元提取器（基于关键点几何计算，17维）和唇形同步检测器（SyncNet，2维）五个独立的子模块。通过缺失值处理（中位数插补）、特征标准化（Z-score）和主成分分析降维（PCA）三步预处理流程，原始四类模态特征的总维度从4237维压缩至367维（降维率91.3%），在大幅降低计算复杂度的同时保留了95%以上的关键信息。GPU加速与批量处理优化进一步保障了大规模数据集上特征提取的效率。经过本模块处理后的多模态特征将作为后续3.2.3节跨模态Transformer融合编码器的输入，实现四类异构模态信息的深层交互融合。
