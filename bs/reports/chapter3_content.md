# 第3章 多模态多任务说话人脸视频质量评估算法设计与实现

随着音频驱动的说话人脸视频生成技术（如Wav2Lip、SadTalker、GeneFace++等）在虚拟数字人、在线教育和视频配音等应用场景中的快速普及，如何对AI生成说话人脸视频的感知质量进行客观、全面且高效的自动化评估，已成为制约该领域进一步发展的关键瓶颈。与传统视频质量评估主要关注编码失真和信号保真度不同，AI生成说话人脸视频的质量评估需要聚焦于多个与人类感知高度相关的核心维度，包括口型与语音的同步精度、面部表情的自然流畅度、合成语音的音质清晰度，以及视觉、音频与面部运动之间的跨模态协调一致性。然而，现有的评价方法往往局限于单一维度的独立评估（如仅关注唇形同步的SyncNet），或高度依赖耗时且主观性强的人工评价流程，难以满足大规模、多维度联合评估的实际需求。为了解决上述问题，本章提出了一种基于Transformer的多模态多任务学习框架，旨在实现对AI生成说话人脸视频质量的细粒度、多维度联合评估。

具体而言，本章围绕多模态多任务说话人脸视频质量评估算法的设计与实现展开系统性阐述，主要包含以下四个方面的内容。

第一，在问题分析层面（3.1节），本章从三个关键技术挑战出发，深入剖析了AI生成说话人脸视频质量评估任务的核心难点：（1）多模态信息融合难题——视觉特征（基于ResNet101提取，维度为2048）、音频特征（基于HuBERT模型提取，维度为768）、面部关键点特征（基于MediaPipe Face Mesh提取468个三维关键点坐标，维度为1404）和面部动作单元特征（基于面部关键点几何距离计算，维度为17）四类异构模态特征在维度尺度和语义空间上存在显著差异，直接拼接或简单融合难以实现有效的跨模态语义对齐；（2）多任务评估的均衡性难题——唇形同步（lip\_sync）、表情自然度（expression）、音频质量（audio\_quality）、跨模态一致性（cross\_modal）和整体感知质量（overall）五个评估子任务之间存在严重的标签分布不均衡现象，其中lip\_sync任务的标签方差接近于零而其余任务的标签方差显著较大，这一特性导致简单固定权重的多任务学习策略难以同时有效优化各子任务；（3）高维特征空间下的训练效率与模型泛化难题——原始四类模态特征的总维度高达4237维，高维输入不仅增大了模型参数规模，还容易引发维度灾难问题，导致训练过程中的梯度不稳定与过拟合风险，如何在保持模型表达能力的同时有效缓解特征冗余并提升训练收敛效率，是本研究需要解决的关键算法设计问题。

第二，在算法设计层面（3.2节），本章系统性地提出了一套完整的多模态多任务评估算法方案。3.2.1小节给出了算法的总体设计方案与处理流程，描述了从原始视频输入到五维质量评分输出的完整计算管线，涵盖多模态特征提取、特征预处理与降维、跨模态融合编码以及多任务预测四个核心阶段。3.2.2小节详细阐述了多模态特征提取模块的设计与实现，该模块分别利用预训练的ResNet101、HuBERT、MediaPipe Face Mesh模型提取视觉、音频、关键点和动作单元四类特征，并辅以SyncNet模型计算唇形同步分数，同时通过主成分分析（PCA）将原始特征总维度从4237维压缩至367维，降维率达91.3\%，大幅降低了后续模型训练的计算开销。3.2.3小节重点介绍了跨模态融合编码与多任务预测模块的联合设计，该模块采用四路并行的特征嵌入层将异构模态特征映射至统一的语义空间，通过可学习位置编码与多层多头自注意力Transformer编码器实现跨模态信息的深层交互融合，并设计了五个独立的任务特定预测头与三种动态任务权重策略（固定权重策略、不确定性加权策略和GradNorm策略），有效解决了多任务学习中的梯度冲突与收敛不均衡问题。

第三，在实验设置层面（3.3节），本章详细描述了实验所采用的数据集、评估指标体系、实验环境配置、对比实验方案以及超参数设置。本研究基于EmotionTalk数据集（包含1,985个AI生成说话人脸视频样本）开展实验，采用80\%/10\%/10\%的比例划分训练集、验证集和测试集，并构建了包含均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）、中位数绝对误差（MEDAE）、决定系数（\( R^2 \)）、Pearson相关系数、Spearman等级相关系数、Kendall's \( \tau \) 系数、一致性相关系数（CCC）、三分类准确率和二次加权Kappa系数（QWK）在内的全面评估指标体系。

第四，在实验结果及分析层面（3.4节），本章从整体性能评估、消融实验、预测误差分析、主观评价一致性验证和不同配置对比五个方面全面展示并深入分析了实验结果。实验表明，本文所提出的多模态多任务评估模型在整体感知质量维度上达到了0.698的Pearson相关系数，在音频质量维度上达到了0.621，在跨模态一致性维度上达到了0.596，验证了所提方法在多维度视频质量评估任务上的有效性。

综上所述，本章通过系统性的问题分析、算法设计、实验验证与结果分析，构建了一个完整的基于跨模态Transformer的多任务说话人脸视频质量评估框架。该框架通过多模态特征融合与动态任务权重策略，有效解决了异构特征对齐和多任务均衡优化两大核心挑战，为后续第4章的系统集成、API服务部署与前端可视化界面开发奠定了坚实的算法基础与技术支撑。

## 3.1 问题分析

AI生成说话人脸视频质量评估是一项涉及多模态信息理解与多维度感知判断的复杂任务。与传统视频质量评估（Video Quality Assessment, VQA）侧重于编码失真、信号噪声比和结构相似性等底层视觉保真度指标不同，说话人脸视频的质量评估需要综合考量视觉呈现、音频表达、面部动态以及跨模态协调等多个与人类感知密切相关的高层语义维度。这一任务的特殊性使得传统的单模态、单任务评估方法难以胜任，要求研究者从多模态信息表征、多任务联合学习以及高维特征空间优化等多个层面进行系统性的算法设计。通过对本研究所采用的EmotionTalk数据集（包含1,985个AI生成说话人脸视频样本）以及现有评估方法的深入分析，本节从以下三个核心技术挑战出发，系统性地剖析当前AI生成说话人脸视频质量评估任务面临的关键难点。

### 3.1.1 多模态异构特征的语义对齐与融合难题

AI生成说话人脸视频的质量评估需要从视觉、音频、面部几何结构和面部动作等多个互补的信息通道中提取具有判别力的特征表示。在本研究中，针对每个视频样本（统一为150帧、25fps的标准序列），采用四类预训练模型分别提取不同模态的特征：

（1）**视觉特征**：采用在ImageNet数据集上预训练的ResNet101深度卷积网络作为视觉编码器，对每帧图像提取2048维的高层视觉语义表示。该特征能够有效捕捉面部纹理细节、渲染质量和视觉伪影等信息，对于评估合成人脸的视觉真实感具有重要作用。

（2）**音频特征**：采用自监督预训练的HuBERT（Hidden-Unit BERT）模型提取768维的音频嵌入表示。HuBERT模型通过大规模无标注语音数据的掩码预测任务学习到了丰富的语音语义信息，能够有效表征语音的清晰度、自然度以及韵律特征，为音频质量评估提供了高质量的特征支撑。

（3）**面部关键点特征**：采用MediaPipe Face Mesh模型提取每帧图像中468个面部三维关键点坐标，形成1404维（468×3）的面部几何结构特征。该特征精细地刻画了面部各区域（眼部、口部、轮廓等）的空间位置关系与运动轨迹，对于评估面部表情的自然流畅度和口型变化的合理性具有不可替代的作用。

（4）**面部动作单元特征**：基于MediaPipe Face Mesh检测到的面部关键点，通过关键点间的几何距离计算方法提取17维面部动作单元（Action Unit, AU）强度值。动作单元基于面部动作编码系统（Facial Action Coding System, FACS），以标准化的方式描述面部肌肉运动模式，能够量化表征微表情变化和情绪表达的细微差异。

上述四类特征的原始总维度高达4237维（2048+768+1404+17），且它们在维度尺度、数值范围和语义空间上存在显著的异构性差异。具体而言，视觉特征位于高维卷积语义空间中，数值分布呈非负特性；音频特征处于自监督表征学习的嵌入空间中，数值范围较为对称；面部关键点特征本质上是低层级的几何坐标信息，数值范围跨度大且与图像分辨率强相关；动作单元特征则是经过编码的动作强度标量，维度远低于其他模态。

对本研究数据集中各模态特征的统计分析进一步揭示了严重的数值尺度失衡现象。四类特征的数值范围差异超过三个数量级：关键点特征的数值范围约为[-30142, 30142]，视觉特征约为[0, 14210]，音频特征约为[-6741, 6749]，而动作单元特征约为[-941, 941]。

这种跨模态的尺度失衡意味着，若直接将四类原始特征进行简单拼接或逐元素融合，维度较高且数值范围较大的模态（如关键点和视觉特征）将在梯度更新中占据主导地位，而低维度模态（如动作单元特征）的信息贡献则容易被淹没，从而导致融合后的特征表示无法充分利用各模态的互补信息。此外，不同模态特征所处的语义空间本质上不可直接比较——卷积网络提取的视觉语义与自监督模型学习的音频嵌入之间缺乏天然的对应关系，简单的线性映射难以实现有效的跨模态语义对齐。因此，如何设计合理的特征预处理与跨模态融合机制，将异构模态特征映射至统一的语义表示空间并实现深层次的信息交互，是本研究需要解决的首要技术挑战。

### 3.1.2 多任务评估的标签不均衡与优化冲突难题

本研究将说话人脸视频质量评估建模为一个多任务回归问题，同时预测以下五个评估维度的质量分数（评分区间为1.0至5.0分）：

（1）**唇形同步质量**（lip\_sync）：衡量生成视频中口型运动与驱动语音之间的时序同步精度，是说话人脸视频最基本的质量要求。

（2）**表情自然度**（expression）：评估面部表情变化的真实感与流畅性，包括情绪表达的合理性和表情转换的平滑度。

（3）**音频质量**（audio\_quality）：评价合成或处理后语音的清晰度、自然度和音质保真度。

（4）**跨模态一致性**（cross\_modal）：综合评估视觉、音频与面部运动三个通道之间的协调一致程度。

（5）**整体感知质量**（overall）：从主观感知的角度对视频整体质量给出综合评分。

然而，对EmotionTalk数据集中五个维度标签分布的深入统计分析揭示了一个严重的标签不均衡问题。在全部1,985个视频样本中，唇形同步维度的标签值几乎完全相同（均值为4.763，标准差趋近于零），呈现出极端的低方差特性；与之形成鲜明对比的是，表情自然度（标准差约0.590）、音频质量（标准差约0.519）、跨模态一致性（标准差约0.566）和整体感知质量等其余维度的标签则呈现出正常的分布特征和合理的方差水平。这一现象表明，数据集中的说话人脸视频在唇形同步质量上高度一致（普遍达到较高水平），但在表情表达、音频品质和跨模态协调等维度上存在显著的质量差异。

上述标签分布的严重不均衡给多任务学习框架的设计带来了本质性的挑战。在标准的多任务学习范式中，总损失函数通常表示为各子任务损失的加权和：

$$L_{total} = \sum_{k=1}^{K} w_k \cdot L_k$$

其中 $K=5$ 为任务数量，$w_k$ 为第 $k$ 个任务的权重，$L_k$ 为对应的均方误差损失。当采用固定等权重策略（$w_k=1, \forall k$）时，由于唇形同步任务的标签方差趋近于零，其损失值量级远小于其他任务，导致该任务在总损失中的梯度贡献几乎可以忽略，模型将无法学习到有意义的唇形同步评估能力。反之，若为唇形同步任务设置过大的权重以补偿其低方差特性，则该任务的梯度信号可能反过来干扰其他任务的正常优化过程，引发负迁移（negative transfer）现象。

此外，数据集中标签的不完整性进一步加剧了多任务优化的复杂性。统计分析表明，除唇形同步维度的标签完整率为100%外，表情自然度的有效标签比例仅为72.8%，音频质量为77.8%，跨模态一致性和整体感知质量均为72.6%，其余样本的标签以无效值（-1.0）标记。这意味着不同任务在每个训练批次中的有效样本数量存在差异，进一步增大了多任务梯度均衡的难度。因此，如何设计自适应的动态任务权重策略，使模型能够在标签分布严重不均衡且部分缺失的条件下同时有效优化五个评估子任务，是本研究面临的第二个关键技术挑战。

### 3.1.3 高维特征空间下的训练效率与模型泛化难题

如前文所述，四类模态特征的原始总维度高达4237维，每个视频样本对应的特征序列形状为150×4237（150个时间步，每步4237维特征向量）。如此高维的输入特征空间给模型训练带来了多方面的挑战。

首先，**维度灾难**（curse of dimensionality）问题。在高维特征空间中，数据点之间的距离趋于均匀化，传统的距离度量和相似性计算逐渐失效。对于本研究所采用的中等规模数据集（1,985个样本，其中训练集仅约1,588个），这一问题尤为突出：高维输入意味着模型需要估计的参数数量与输入维度成正比增长，而有限的训练样本难以为如此多的参数提供充分的约束，极易导致模型在训练集上过拟合而在测试集上泛化性能下降。

其次，**特征冗余与噪声干扰**问题。在4237维的原始特征中，不同模态特征之间以及同一模态特征的不同维度之间可能存在大量的冗余信息。例如，面部关键点的1404维坐标特征中，许多关键点在帧间的运动具有高度的空间相关性，其独立信息量远低于名义维度所暗示的水平。此外，由于特征提取过程中视觉特征存在约0.08%的缺失值（共计2,445个NaN值需要通过中位数插补进行填充），插补引入的噪声也可能影响模型的学习效果。这些冗余和噪声维度不仅增加了不必要的计算开销，还可能干扰模型对真正具有判别力的特征模式的学习。

最后，**训练收敛效率**问题。高维输入特征经过嵌入层映射后，产生的梯度信号需要穿越更深、更宽的网络结构才能回传至各模态的嵌入层，梯度消失或梯度爆炸的风险随维度增加而加剧。同时，高维特征空间中损失函数的优化曲面更加复杂，存在更多的鞍点和局部极小值，使得基于随机梯度下降的优化算法更难以高效地收敛到全局最优解。当结合前述的多任务优化框架时，不同任务的梯度在高维空间中发生冲突的概率进一步增大，导致训练过程中出现振荡或收敛缓慢的现象。

因此，如何在保持各模态特征表达能力的前提下，通过合理的特征降维与预处理策略有效缓解维度灾难，并配合针对性的训练优化技术提升模型在高维多任务场景下的收敛效率和泛化性能，构成了本研究需要解决的第三个核心技术挑战。

### 3.1.4 本章技术方案概述

基于上述三个核心技术挑战的分析，本章提出构建一个基于跨模态Transformer的多模态多任务学习框架，以实现对AI生成说话人脸视频质量的全面、准确评估。该框架的核心设计思路如下：

针对多模态异构特征的语义对齐与融合难题，本研究首先通过特征级的标准化预处理（StandardScaler）消除不同模态间的数值尺度差异，然后利用主成分分析（PCA）将原始4237维特征压缩至367维（降维率达91.3%），在大幅降低特征冗余的同时保留95%以上的原始方差信息。在此基础上，设计四路并行的模态特定嵌入网络将降维后的异构特征映射至统一维度的语义空间，并通过多层多头自注意力机制实现跨模态信息的深层交互融合。

针对多任务评估的标签不均衡与优化冲突难题，本研究引入了自适应的动态任务权重策略，包括基于同方差不确定性的自动权重调节机制和基于梯度范数均衡的GradNorm策略，使模型能够根据各任务的训练进度和难度自动调整损失权重，从而在标签分布不均衡的条件下实现多任务的协同优化。同时，采用掩码机制处理无效标签，确保缺失标签不参与损失计算。

针对高维特征空间下的训练效率与模型泛化难题，本研究提出了一套综合性的训练优化方案，涵盖梯度检查点、混合精度训练、梯度累积、自适应学习率调度等多项技术，以提升模型在高维多任务场景下的训练稳定性和收敛效率，同时通过标签平滑、Dropout正则化和早停策略有效防止过拟合。

上述技术方案的详细设计与实现将在3.2节中展开阐述。

## 3.2 算法设计

### 3.2.1 算法设计方案

基于3.1节对三个核心技术挑战的深入分析，本节给出多模态多任务说话人脸视频质量评估算法的总体设计方案。该算法以端到端的计算管线形式组织，从原始视频输入出发，经过多模态特征提取、特征预处理与降维、跨模态Transformer融合编码以及多任务质量预测四个核心阶段，最终输出唇形同步、表情自然度、音频质量、跨模态一致性和整体感知质量五个维度的质量评分。图3-1展示了算法的总体架构与数据流向。

> **【图3-1 多模态多任务说话人脸视频质量评估算法总体架构图】**
>
> 图示说明：该图应从左至右展示四个核心阶段的数据流。左侧为输入端（原始视频），依次经过：（1）多模态特征提取阶段——四路并行的特征提取器（ResNet101→2048维、HuBERT→768维、MediaPipe→1404维、AU计算→17维），总维度4237；（2）特征预处理与降维阶段——NaN插补→Z-score标准化→PCA降维，总维度压缩至367维；（3）跨模态Transformer融合编码阶段——四路嵌入网络→均值融合→可学习位置编码→6层16头Transformer编码器→全局平均池化→512维融合向量；（4）多任务质量预测阶段——五个独立预测头分别输出lip\_sync、expression、audio\_quality、cross\_modal、overall五个维度的\[1.0, 5.0\]评分。右侧标注动态任务权重与标签掩码机制。

**（1）多模态特征提取阶段**

算法的第一阶段负责从原始视频中提取多模态感知特征。给定一个输入视频文件，系统首先将其统一采样为150帧（帧率25fps）的标准序列，并通过FFmpeg工具分离出对应的音频流（采样率16000Hz）。随后，四个独立的预训练特征提取器并行工作，分别从视频帧序列和音频信号中提取四类异构模态特征。

在视觉与音频模态方面，视觉特征提取器基于在ImageNet数据集上预训练的ResNet101卷积网络，对每帧224×224的人脸图像提取2048维的高层视觉语义表示，形成维度为150×2048的视觉特征矩阵；音频特征提取器基于自监督预训练的HuBERT模型，对16000Hz采样率的音频信号提取768维的语音嵌入表示，形成维度为150×768的音频特征矩阵。

在面部几何与动作模态方面，面部关键点提取器基于MediaPipe Face Mesh模型，检测每帧图像中468个面部三维关键点坐标，形成维度为150×1404的几何结构特征矩阵；面部动作单元提取器基于MediaPipe关键点的几何距离计算，估计每帧17个动作单元的激活强度值，形成维度为150×17的动作单元特征矩阵。此外，SyncNet同步检测模块计算音视频之间的同步置信度分数与时序偏移量，作为辅助参考信息。

经过该阶段处理后，每个视频样本被表示为四组时序特征矩阵，原始特征总维度为4237维（2048+768+1404+17）。

**（2）特征预处理与降维阶段**

算法的第二阶段针对3.1.1节和3.1.3节分析的多模态尺度失衡与高维特征冗余问题，对原始特征进行系统性的预处理与降维。该阶段包含三个关键步骤：

第一步为缺失值处理。由于特征提取过程中部分视频帧可能存在遮挡、模糊等问题，导致视觉特征中约0.08%的元素值缺失（共计2,445个NaN值）。本研究采用基于中位数的插补策略（median imputation）填充缺失值，以避免极端离群值对后续计算的干扰。

第二步为特征标准化。如3.1.1节所述，四类模态特征的数值范围存在三个数量级以上的差异。为消除这种尺度失衡，本研究对每类模态特征独立应用Z-score标准化（StandardScaler），将各维度特征的均值调整为0、标准差调整为1，确保不同模态特征在统一的数值尺度上参与后续的融合与学习过程。

第三步为主成分分析降维。为有效缓解高维特征空间带来的维度灾难和特征冗余问题，本研究采用主成分分析（Principal Component Analysis, PCA）对标准化后的各模态特征进行降维压缩。具体而言，面部关键点特征从1404维降至50维，保留了约85%的原始方差信息；视觉特征从2048维降至100维；音频特征从768维降至200维；动作单元特征由于原始维度较低（17维），保持不变。降维后的四类特征总维度从4237维压缩至367维（100+200+50+17），降维率达到91.3%，大幅降低了后续模型的参数规模与计算复杂度，同时通过去除冗余维度有效提升了特征的信噪比。

**（3）跨模态Transformer融合编码阶段**

算法的第三阶段是整个框架的核心组件，负责将经过预处理和降维的四类异构模态特征映射至统一的语义空间，并通过深层注意力机制实现跨模态信息的充分交互融合。该阶段的设计思路如下：

首先，四路并行的模态特定嵌入网络分别将各模态的低维特征映射至统一的高维语义空间。每路嵌入网络采用两层全连接结构，中间引入ReLU激活函数和Dropout正则化层，将维度各异的模态特征（100维、200维、50维、17维）统一映射为512维的语义嵌入向量。四路嵌入后的特征矩阵形状均为150×512，实现了异构模态在维度上的对齐。

随后，四路嵌入特征通过逐元素平均操作融合为单一的150×512维特征序列，并叠加可学习的位置编码向量以注入时序位置信息。

融合后的特征序列被送入多层Transformer编码器进行深层语义编码。Transformer编码器采用6层编码层，每层包含16个注意力头的多头自注意力模块和维度为2048的前馈网络，通过全局自注意力机制使每个时间步的特征表示能够同时关注序列中所有其他时间步的信息，从而有效捕捉跨模态、跨时间步的长程依赖关系。

最后，编码器输出的150×512维特征序列经全局平均池化操作沿时间维度聚合为单一的512维特征向量，作为该视频样本的全局多模态融合表示，供后续的多任务预测头使用。

**（4）多任务质量预测阶段**

算法的第四阶段基于融合后的全局特征向量，通过五个独立的任务特定预测头分别输出五个质量维度的评分。每个预测头采用三层全连接网络结构（512→256→128→1），中间层引入ReLU激活函数和Dropout正则化，最后通过Sigmoid激活函数将输出映射至[0, 1]区间，再经线性缩放映射至[1.0, 5.0]的评分区间。五个预测头共享相同的网络结构但参数完全独立，使得每个质量维度的预测能够学习到针对该维度的特定映射模式，同时通过共享的Transformer编码器特征实现多任务间的隐式信息传递。

在训练阶段，总损失函数定义为各子任务加权均方误差损失之和。为应对3.1.2节分析的标签不均衡与缺失问题，本研究引入了动态任务权重策略和标签掩码机制：对于标注为无效值（-1.0）的标签，通过布尔掩码排除其参与损失计算；对于不同任务间的损失量级差异，通过基于不确定性的自适应权重或基于梯度范数均衡的GradNorm策略动态调节各任务的权重系数，确保模型能够在标签分布严重不均衡的条件下同时有效优化五个评估子任务。

在推理阶段，给定一个待评估的视频文件，系统依次执行上述四个阶段的前向计算，最终输出五个维度的质量评分，整个推理过程无需人工干预，实现了全自动化的多维度视频质量评估。

**（5）总体设计特点**

综合来看，本算法的总体设计方案具有以下三个显著特点：第一，模块化设计，各阶段功能职责明确、接口定义清晰，便于独立开发调试与灵活组合；第二，两套配置体系，标准配置（编码维度256、3层Transformer、8注意力头）适用于快速原型验证，优化配置（编码维度512、6层Transformer、16注意力头）适用于追求最优性能的正式评估，两套配置通过统一的YAML配置文件切换，无需修改任何代码；第三，端到端可训练，从特征嵌入层到多任务预测头的所有参数均可通过反向传播联合优化，使得模型能够自适应地学习最有利于多维度质量评估的特征表示与预测映射。上述设计方案的各核心模块将在后续3.2.2和3.2.3小节中逐一展开详细阐述。

### 3.2.2 多模态特征提取模块

多模态特征提取是整个评估算法的基础环节，其核心目标是从原始说话人脸视频中提取出能够充分表征视觉质量、语音特性、面部动态以及跨模态协调性的多维特征表示。本节详细阐述多模态特征提取模块的设计与实现，包括四类模态特征的提取方法、唇形同步分数的计算方式，以及面向后续模型训练的特征预处理与降维流程。图3-2展示了多模态特征提取模块的整体架构。

> **【图3-2 多模态特征提取模块架构图】**
>
> 图示说明：该图应以输入视频为起点，展示五路并行的特征提取流程：（1）视觉路径——视频帧→RetinaFace人脸检测→224×224裁剪→ImageNet归一化→ResNet101（去FC层）→GAP→2048维/帧；（2）音频路径——FFmpeg分离音频→16kHz PCM→HuBERT-Base→768维→线性插值对齐至150帧；（3）关键点路径——MediaPipe Face Mesh→468个3D关键点→展平为1404维/帧；（4）AU路径——基于关键点几何距离→17个AU强度值→最大值归一化；（5）SyncNet路径——音视频同步置信度与偏移量→Sigmoid映射→\[0,5\]分数。下方展示预处理流程：NaN中位数插补→Z-score标准化→PCA降维（表3-2配置），最终输出367维标准化特征。

**（1）视觉与音频特征提取**

**① 视觉特征提取**

视觉特征负责捕捉生成视频中人脸区域的纹理细节、渲染质量与视觉伪影等信息。本研究采用在ImageNet数据集上预训练的ResNet101深度残差卷积网络作为视觉特征编码器。

给定一个输入视频 $V$，系统首先将其统一采样为 $T=150$ 帧（目标帧率 $f_{\text{target}}=25$ fps）的标准序列。采样间隔为 $\Delta = \max(1, \lfloor f_{\text{orig}} / f_{\text{target}} \rceil)$，若采样后帧数超过150帧则均匀下采样，不足则以零向量填充。对于第 $t$ 帧图像 $I_t$（$t=1,2,\ldots,T$），系统通过RetinaFace人脸检测器定位面部区域，将裁剪后的人脸图像缩放至 $224\times224$ 像素并按ImageNet标准进行通道归一化：

$$\hat{I}_t^{(c)} = \frac{I_t^{(c)} - \mu_c}{\sigma_c}, \quad c \in \{R, G, B\}$$

其中 $\mu = [0.485, 0.456, 0.406]$ 和 $\sigma = [0.229, 0.224, 0.225]$ 分别为ImageNet数据集各通道的均值和标准差。

归一化后的帧图像被送入ResNet101网络，本研究移除最后的全连接分类层，以全局平均池化层的输出作为视觉特征向量：

$$\mathbf{v}_t = \text{GAP}(\text{ResBlock}_4(\text{ResBlock}_3(\text{ResBlock}_2(\text{ResBlock}_1(\text{Conv}_1(\hat{I}_t)))))) \in \mathbb{R}^{2048}$$

其中 $\text{GAP}(\cdot)$ 表示全局平均池化操作，$\text{ResBlock}_i$ 表示第 $i$ 个残差块组。最终，每个视频样本的视觉特征矩阵为 $\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_T]^\top \in \mathbb{R}^{150 \times 2048}$。

**② 音频特征提取**

音频特征负责表征语音信号的清晰度、自然度和韵律特征。本研究采用Meta AI提出的HuBERT（Hidden-Unit BERT）自监督预训练语音模型作为音频特征编码器。HuBERT通过在大规模无标注语音数据上进行掩码预测任务的预训练，学习到了丰富的语音语义表示。

音频特征的提取流程如下：首先，通过FFmpeg工具从输入视频中分离音频流，转换为采样率 $f_s = 16000$ Hz、单声道的PCM格式波形信号 $\mathbf{a} = [a_1, a_2, \ldots, a_N]$，其中 $N = f_s \times D$ 为总采样点数。波形信号经Wav2Vec2特征提取器预处理后，送入HuBERT-Base模型（包含7层卷积编码器和12层Transformer编码器），提取最后一层隐藏状态作为音频特征：

$$\mathbf{H}_{\text{audio}} = \text{HuBERT}(\mathbf{a}) = \text{Transformer}_{12}(\text{CNN}(\mathbf{a})) \in \mathbb{R}^{T_a \times 768}$$

其中 $T_a$ 为HuBERT输出的时间步数，768为隐藏维度。由于HuBERT卷积编码器对输入波形进行了约320倍降采样（每20ms一帧），$T_a$ 与视频帧数 $T=150$ 通常不一致。为实现音视频特征的时间对齐，本研究采用线性插值将音频特征统一调整为150个时间步：

$$\mathbf{A} = \text{Interpolate}(\mathbf{H}_{\text{audio}}, T) \in \mathbb{R}^{150 \times 768}$$

**（2）面部关键点与动作单元特征提取**

**① 面部关键点特征提取**

面部关键点特征通过精细的面部几何结构信息刻画口型变化、表情运动和面部轮廓形变等动态特征。本研究采用Google提出的MediaPipe Face Mesh模型，该模型能够在单张图像中实时检测468个面部三维关键点，覆盖眉毛、眼睛、鼻子、嘴唇、下颌轮廓等所有关键区域（检测置信度阈值设为0.5）。

对于第 $t$ 帧图像 $I_t$，MediaPipe Face Mesh输出468个关键点的三维坐标 $\mathbf{P}_t = \{(x_i^t, y_i^t, z_i^t)\}_{i=1}^{468}$，其中 $(x_i^t, y_i^t)$ 为归一化图像坐标（值域为 $[0, 1]$），$z_i^t$ 为相对深度估计值。将每帧的三维坐标展平为一维向量，得到1404维（$468 \times 3$）的关键点特征：

$$\mathbf{k}_t = [x_1^t, y_1^t, z_1^t, x_2^t, y_2^t, z_2^t, \ldots, x_{468}^t, y_{468}^t, z_{468}^t]^\top \in \mathbb{R}^{1404}$$

对于未检测到人脸的帧，以零向量填充以保持序列完整性。最终，关键点特征矩阵为 $\mathbf{K} = [\mathbf{k}_1, \mathbf{k}_2, \ldots, \mathbf{k}_T]^\top \in \mathbb{R}^{150 \times 1404}$。关键点特征的独特价值在于其直接编码了面部各部位的空间位置关系和帧间运动轨迹，例如口唇区域关键点的时序运动模式可直接反映口型变化的节奏与幅度，为唇形同步评估提供了显式的几何线索。

**② 面部动作单元特征提取**

面部动作单元（Action Unit, AU）是面部动作编码系统（FACS）中定义的基本面部肌肉运动单位，以标准化方式量化描述面部表情的细微变化。本研究基于MediaPipe Face Mesh检测到的468个关键点，通过几何距离计算方法近似估计17个核心动作单元的激活强度，避免了对额外外部工具的依赖。

对于第 $t$ 帧的关键点集合 $\mathbf{P}_t$，各动作单元的强度值通过特定关键点对之间的欧氏距离 $d(\cdot, \cdot)$ 计算获得。以眉部区域为例，AU1（内侧眉毛上提）通过左右内侧眉毛关键点与鼻根关键点之间距离的均值表征：

$$\text{AU1}_t = \frac{1}{2}\left(d(\mathbf{p}_{\text{brow\_inner\_L}}^t, \mathbf{p}_{\text{nose}}^t) + d(\mathbf{p}_{\text{brow\_inner\_R}}^t, \mathbf{p}_{\text{nose}}^t)\right)$$

对于眼部区域，AU5（上眼睑提升）通过上下眼睑关键点距离的均值表征：

$$\text{AU5}_t = \frac{1}{2}\left(d(\mathbf{p}_{\text{upper\_lid\_L}}^t, \mathbf{p}_{\text{lower\_lid\_L}}^t) + d(\mathbf{p}_{\text{upper\_lid\_R}}^t, \mathbf{p}_{\text{lower\_lid\_R}}^t)\right)$$

对于口唇区域，AU25（双唇分开）通过上下唇关键点之间的距离表征，AU26（下颌张开）定义为AU25的线性放大：

$$\text{AU25}_t = d(\mathbf{p}_{\text{upper\_lip}}^t, \mathbf{p}_{\text{lower\_lip}}^t), \quad \text{AU26}_t = 1.2 \times \text{AU25}_t$$

此外，部分动作单元通过逆运算定义，如AU7（眼睑收紧）$\text{AU7}_t = 1/(\text{AU5}_t + \epsilon)$，其中 $\epsilon = 10^{-6}$ 为防止除零的微小常数。表3-1汇总了本研究提取的17个动作单元的完整定义。

**表3-1 17个面部动作单元的定义与计算方法**

| 动作单元 | 名称 | 计算方法 |
|---------|------|---------|
| AU1 | 内侧眉毛上提 | 内侧眉毛与鼻根距离的均值 |
| AU2 | 外侧眉毛上提 | 外侧眉毛与鼻根距离的均值 |
| AU4 | 眉毛下压 | 左右眉毛中部的间距 |
| AU5 | 上眼睑提升 | 上下眼睑距离的均值 |
| AU6 | 面颊上提 | 眼睛至嘴角距离的均值 |
| AU7 | 眼睑收紧 | $1/(\text{AU5} + \epsilon)$ |
| AU9 | 鼻部皱缩 | 鼻翼关键点间距 |
| AU10 | 上唇上提 | 上唇与鼻尖距离 |
| AU12 | 唇角拉伸 | 嘴角与参考点距离的均值 |
| AU14 | 酒窝形成 | $1/(\text{AU12} + \epsilon)$ |
| AU15 | 唇角下拉 | $0.8 \times \text{AU14}$ |
| AU17 | 下颏上提 | 下唇与鼻尖距离 |
| AU20 | 嘴唇横向拉伸 | 左右嘴角间距 |
| AU23 | 嘴唇收紧 | $1/(d(\text{upper\_lip}, \text{lower\_lip}) + \epsilon)$ |
| AU25 | 双唇分开 | 上下唇间距 |
| AU26 | 下颌张开 | $1.2 \times \text{AU25}$ |
| AU28 | 嘴唇内卷 | $1/(\text{AU20} + \epsilon)$ |

提取完成后，每帧的17维AU强度向量经过最大值归一化处理，将各维度特征值映射至 $[0, 1]$ 区间：

$$\hat{\mathbf{u}}_t = \frac{\mathbf{u}_t}{\max(\mathbf{u}_t) + \epsilon}$$

其中 $\mathbf{u}_t = [\text{AU1}_t, \text{AU2}_t, \ldots, \text{AU28}_t]^\top \in \mathbb{R}^{17}$ 为归一化前的原始AU特征向量。最终，动作单元特征矩阵为 $\mathbf{U} = [\hat{\mathbf{u}}_1, \hat{\mathbf{u}}_2, \ldots, \hat{\mathbf{u}}_T]^\top \in \mathbb{R}^{150 \times 17}$。

**（3）唇形同步分数计算**

除上述四类帧级时序特征外，本研究还引入了基于SyncNet模型的唇形同步分数作为辅助评估信息。SyncNet是一个经典的音视频同步检测网络，通过对比学习方法训练，能够判断视频中口型运动与语音信号之间的时序同步程度。模型输出包括同步置信度 $c \in \mathbb{R}$ 和时序偏移量 $o \in \mathbb{Z}$（单位：帧），置信度值越高表示音视频同步性越好。

为将置信度值映射至统一的 $[0, 5]$ 评分区间，本研究设计了基于Sigmoid函数的非线性映射：

$$s_{\text{sync}} = \frac{5}{1 + \exp(-10 \cdot (\hat{c} - 0.5))}$$

其中 $\hat{c}$ 为归一化后的置信度值。该映射函数以0.5为中心点，通过陡峭的Sigmoid曲线（温度系数为10）将置信度值压缩至 $[0, 5]$ 区间。最终输出为二维向量 $[s_{\text{sync}}, o]$，分别表示同步分数与帧偏移量。

**（4）特征预处理与降维**

经过上述多模态特征提取后，每个视频样本被表示为四组时序特征矩阵：视觉特征 $\mathbf{V} \in \mathbb{R}^{150 \times 2048}$、音频特征 $\mathbf{A} \in \mathbb{R}^{150 \times 768}$、关键点特征 $\mathbf{K} \in \mathbb{R}^{150 \times 1404}$ 和动作单元特征 $\mathbf{U} \in \mathbb{R}^{150 \times 17}$。如3.1节所分析，这些原始特征在数值尺度和维度上存在显著差异，且高维特征空间容易引发维度灾难与过拟合问题。因此，在送入后续的跨模态Transformer融合编码器之前，需要对原始特征进行系统性的预处理与降维。

**① 缺失值处理与特征标准化**

在特征提取过程中，部分视频帧可能因遮挡或检测失败导致特征值缺失（NaN值），本研究数据集中视觉特征存在约2,445个NaN值（占0.08%）。本研究采用基于中位数的特征插补策略，对每一维度 $j$ 计算非缺失值的中位数 $m_j$ 进行填充：

$$\tilde{f}_{t,j} = \begin{cases} f_{t,j}, & \text{if } f_{t,j} \text{ is valid} \\ m_j = \text{Median}(\{f_{\tau,j} \mid f_{\tau,j} \text{ is valid}\}), & \text{if } f_{t,j} = \text{NaN} \end{cases}$$

选择中位数而非均值是因为其对离群值具有更强的鲁棒性。插补完成后，对每类模态特征独立应用Z-score标准化（StandardScaler），消除跨模态的数值尺度差异：

$$\hat{f}_{i,j}^{(m)} = \frac{f_{i,j}^{(m)} - \mu_j^{(m)}}{\sigma_j^{(m)} + \epsilon}$$

其中 $\mu_j^{(m)}$ 和 $\sigma_j^{(m)}$ 分别为模态 $m$ 第 $j$ 维特征在训练集上的均值和标准差。标准化后各维度特征均值为0、标准差为1。需要注意，标准化参数仅在训练集上估计，验证集和测试集直接应用，以避免数据泄露。

**② 主成分分析降维**

为缓解高维特征空间带来的维度灾难问题，本研究采用主成分分析（PCA）对标准化后的各模态特征进行降维。对于模态 $m$ 的标准化特征矩阵 $\hat{\mathbf{F}}^{(m)} \in \mathbb{R}^{N \times d_m}$，PCA首先计算协方差矩阵并进行特征值分解：

$$\mathbf{C}^{(m)} = \frac{1}{N-1} \hat{\mathbf{F}}^{(m)\top} \hat{\mathbf{F}}^{(m)} = \mathbf{W}^{(m)} \boldsymbol{\Lambda}^{(m)} \mathbf{W}^{(m)\top}$$

其中 $\boldsymbol{\Lambda}^{(m)} = \text{diag}(\lambda_1^{(m)}, \lambda_2^{(m)}, \ldots, \lambda_{d_m}^{(m)})$ 为特征值对角矩阵（$\lambda_1^{(m)} \geq \lambda_2^{(m)} \geq \cdots \geq 0$），$\mathbf{W}^{(m)}$ 为特征向量矩阵。选取前 $d_m'$ 个主成分构成投影矩阵 $\mathbf{W}_{d_m'}^{(m)} \in \mathbb{R}^{d_m \times d_m'}$，降维后的特征及其累计解释方差比分别为：

$$\tilde{\mathbf{F}}^{(m)} = \hat{\mathbf{F}}^{(m)} \mathbf{W}_{d_m'}^{(m)} \in \mathbb{R}^{N \times d_m'}, \quad \rho^{(m)} = \frac{\sum_{i=1}^{d_m'} \lambda_i^{(m)}}{\sum_{i=1}^{d_m} \lambda_i^{(m)}}$$

表3-2汇总了各模态特征的PCA降维配置。降维所需的投影矩阵仅在训练集上计算获得，验证集和测试集直接使用训练集上求得的投影矩阵进行变换。

**表3-2 各模态特征的PCA降维配置**

| 模态特征 | 原始维度 $d_m$ | 目标维度 $d_m'$ | 降维率 | 累计解释方差比 $\rho^{(m)}$ |
|---------|:-------------:|:--------------:|:-----:|:-------------------------:|
| 视觉特征 | 2048 | 100 | 95.1% | ≥ 95% |
| 音频特征 | 768 | 200 | 74.0% | ≥ 95% |
| 关键点特征 | 1404 | 50 | 96.4% | ≥ 85% |
| 动作单元特征 | 17 | 17（不降维） | 0% | 100% |
| **合计** | **4237** | **367** | **91.3%** | — |

从表3-2可以看出，PCA降维在大幅压缩特征维度的同时保留了绝大部分原始方差信息。关键点特征的降维率最高（96.4%），这与面部关键点在帧间运动具有高度空间相关性的特性一致。降维后四类特征总维度从4237维压缩至367维（降维率91.3%），大幅降低了后续模型的参数规模和计算复杂度，并通过去除冗余维度有效提升了特征的信噪比。

**（5）本节小结**

本节详细阐述了多模态特征提取模块的设计与实现。该模块集成了视觉特征提取器（ResNet101，2048维）、音频特征提取器（HuBERT，768维）、面部关键点提取器（MediaPipe Face Mesh，1404维）、面部动作单元提取器（基于关键点几何计算，17维）和唇形同步检测器（SyncNet，2维）五个子模块。通过缺失值处理（中位数插补）、特征标准化（Z-score）和主成分分析降维（PCA）三步预处理流程，原始四类模态特征的总维度从4237维压缩至367维（降维率91.3%），在大幅降低计算复杂度的同时保留了95%以上的关键信息。经过本模块处理后的多模态特征将作为后续3.2.3节跨模态融合编码与多任务预测模块的输入，实现四类异构模态信息的深层交互融合与多维度质量评分预测。

### 3.2.3 跨模态融合编码与多任务预测

经过3.2.2节的多模态特征提取与预处理后，系统获得了四类经过PCA降维的标准化时序特征矩阵。然而，如何有效融合这些在语义空间和维度分布上存在显著差异的异构特征，并在此基础上实现五个评估维度的联合预测，是整个评估算法的核心技术挑战。本节详细阐述跨模态Transformer融合编码器与多任务预测头的联合设计，包括模态特征嵌入与对齐、Transformer编码器的跨模态注意力融合机制、多任务预测头结构，以及面向多任务均衡优化的动态权重策略。图3-3展示了跨模态融合编码与多任务预测模块的整体架构。

> **【图3-3 跨模态融合编码与多任务预测模块架构图】**
>
> 图示说明：该图应展示从降维后特征输入到质量评分输出的完整数据流。左侧为四路降维后特征输入（视觉100维、音频200维、关键点50维、AU 17维），分别经过四路并行嵌入网络（各含两层全连接+ReLU+Dropout）映射至统一的512维语义空间。中部展示融合过程：四路嵌入特征经均值池化→叠加可学习位置编码→送入6层Transformer编码器（每层含16头多头自注意力+FFN+残差连接+层归一化），编码器输出经全局平均池化得到512维融合向量。右侧展示五个独立的任务预测头（512→256→128→1→Sigmoid→\[1.0,5.0\]），分别输出五个维度的质量评分。底部标注动态权重策略（固定权重/不确定性加权/GradNorm）和标签掩码机制。

**（1）模态特征嵌入与Transformer融合编码**

**① 模态特征嵌入层**

由于四类模态特征经PCA降维后的维度仍存在差异（视觉100维、音频200维、关键点50维、AU 17维），直接在特征维度上进行拼接或求和操作无法实现有效的语义对齐。为此，本研究为每类模态设计了独立的两层前馈嵌入网络，将不同维度的特征统一映射至相同的 $d_{\text{embed}}$ 维语义空间。

具体而言，模态 $m$ 的嵌入网络 $\phi_m(\cdot)$ 定义为：

$$\phi_m(\mathbf{x}) = \mathbf{W}_2^{(m)} \cdot \text{ReLU}(\mathbf{W}_1^{(m)} \mathbf{x} + \mathbf{b}_1^{(m)}) + \mathbf{b}_2^{(m)}$$

其中 $\mathbf{W}_1^{(m)} \in \mathbb{R}^{h_m \times d_m}$ 和 $\mathbf{W}_2^{(m)} \in \mathbb{R}^{d_{\text{embed}} \times h_m}$ 为可学习的权重矩阵，$h_m$ 为各模态嵌入网络的隐藏层维度。在嵌入过程中，各层之间引入Dropout正则化（丢弃率0.3）以防止过拟合。四路嵌入网络的具体配置如下：

- 视觉分支：$\text{Linear}(d_{\text{visual}}, 256) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.3) \rightarrow \text{Linear}(256, d_{\text{embed}})$
- 音频分支：$\text{Linear}(d_{\text{audio}}, 512) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.3) \rightarrow \text{Linear}(512, d_{\text{embed}})$
- 关键点分支：$\text{Linear}(d_{\text{keypoint}}, 512) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.3) \rightarrow \text{Linear}(512, d_{\text{embed}})$
- AU分支：$\text{Linear}(d_{\text{au}}, 128) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.3) \rightarrow \text{Linear}(128, d_{\text{embed}})$

其中，视觉与AU分支采用较小的隐藏维度（分别为256和128），是因为视觉特征经PCA降维后维度已大幅缩减，而AU特征原始维度仅为17维；音频与关键点分支采用较大的隐藏维度（512），以保留更丰富的语义信息。

经过嵌入映射后，四类模态特征在时间维度上对齐为统一的表示形式：

$$\mathbf{E}_m = \phi_m(\tilde{\mathbf{F}}^{(m)}) \in \mathbb{R}^{T \times d_{\text{embed}}}, \quad m \in \{\text{visual}, \text{audio}, \text{keypoint}, \text{au}\}$$

**② 跨模态融合与位置编码**

为实现四类模态嵌入特征的跨模态交互融合，本研究首先将四组嵌入特征沿新增的模态维度进行堆叠，然后通过均值池化生成统一的融合特征序列：

$$\mathbf{E}_{\text{fused}} = \frac{1}{M} \sum_{m=1}^{M} \mathbf{E}_m \in \mathbb{R}^{T \times d_{\text{embed}}}$$

其中 $M=4$ 为模态数量。这一均值融合策略在保持各模态等权贡献的同时，有效消除了因模态数量变化带来的尺度不稳定性。

为使Transformer编码器能够感知输入序列中各时间步的位置信息，本研究引入可学习位置编码。与基于正弦函数的固定位置编码不同，可学习位置编码具有更强的适应性，能够在训练过程中自动学习到与具体任务相关的最优位置表示：

$$\mathbf{E}_{\text{pos}} = \mathbf{E}_{\text{fused}} + \mathbf{P}, \quad \mathbf{P} \in \mathbb{R}^{T \times d_{\text{embed}}}$$

其中 $\mathbf{P}$ 为可学习的位置编码参数矩阵，$T=150$ 为最大序列长度。

**③ Transformer编码器**

加入位置编码后的融合特征序列被送入多层Transformer编码器进行深层特征交互与信息提取。每层Transformer编码器由多头自注意力（Multi-Head Self-Attention, MHSA）子层和前馈神经网络（Feed-Forward Network, FFN）子层构成，两个子层均采用残差连接与层归一化。

多头自注意力机制的计算过程为：

$$\text{MHSA}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_H) \mathbf{W}^O$$

$$\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{W}_i^Q (\mathbf{K}\mathbf{W}_i^K)^\top}{\sqrt{d_k}}\right) \mathbf{V}\mathbf{W}_i^V$$

其中 $H$ 为注意力头数，$d_k = d_{\text{embed}} / H$ 为每个注意力头的维度，$\mathbf{W}_i^Q, \mathbf{W}_i^K, \mathbf{W}_i^V \in \mathbb{R}^{d_{\text{embed}} \times d_k}$ 和 $\mathbf{W}^O \in \mathbb{R}^{d_{\text{embed}} \times d_{\text{embed}}}$ 为可学习的投影矩阵。多头自注意力机制使得模型能够同时关注不同时间步之间的多种类型的依赖关系，从而有效捕捉跨模态特征之间的复杂交互模式。

前馈神经网络子层由两个线性变换和一个ReLU激活函数组成：

$$\text{FFN}(\mathbf{x}) = \text{ReLU}(\mathbf{x}\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2$$

其中前馈维度 $d_{\text{ff}}$ 为 $d_{\text{embed}}$ 的4倍。

本研究提供了两套Transformer编码器配置，以适应不同的计算资源与精度需求，具体参数对比如表3-3所示。

**表3-3 Transformer编码器的两套配置参数**

| 配置参数 | 标准配置 | 优化配置 |
|---------|:-------:|:-------:|
| 嵌入维度 $d_{\text{embed}}$ | 256 | 512 |
| 编码层数 $L$ | 3 | 6 |
| 注意力头数 $H$ | 8 | 16 |
| 每头维度 $d_k$ | 32 | 32 |
| 前馈维度 $d_{\text{ff}}$ | 1024 | 2048 |
| Dropout率 | 0.1 | 0.1 |

优化配置通过增加编码层数和注意力头数，显著增强了模型对跨模态时序依赖关系的建模能力，尤其适用于需要更精细特征交互的评估任务。

经过 $L$ 层Transformer编码后，输出序列通过全局平均池化沿时间维度进行压缩，得到固定维度的融合特征向量：

$$\mathbf{z} = \frac{1}{T} \sum_{t=1}^{T} \mathbf{h}_t^{(L)} \in \mathbb{R}^{d_{\text{embed}}}$$

其中 $\mathbf{h}_t^{(L)}$ 为第 $L$ 层编码器在时间步 $t$ 的输出。该融合特征向量 $\mathbf{z}$ 编码了四类模态特征在整个视频时序上的全局交互信息，将作为后续多任务预测头的共享输入表示。

**（2）多任务预测头与动态权重策略**

**① 任务特定预测头**

基于Transformer融合编码器输出的 $d_{\text{embed}}$ 维共享特征向量 $\mathbf{z}$，本研究为五个评估维度分别设计了独立的任务特定预测头。每个预测头采用相同的三层前馈网络结构，但拥有独立的可学习参数，以适应不同评估维度的特有评判标准与评分分布特征。

第 $k$ 个任务（$k \in \{\text{lip\_sync}, \text{expression}, \text{audio\_quality}, \text{cross\_modal}, \text{overall}\}$）的预测头定义为：

$$\hat{y}_k = s_{\min} + (s_{\max} - s_{\min}) \cdot \sigma\left(\mathbf{W}_3^{(k)} \cdot \text{ReLU}\left(\mathbf{W}_2^{(k)} \cdot \text{ReLU}\left(\mathbf{W}_1^{(k)} \mathbf{z} + \mathbf{b}_1^{(k)}\right) + \mathbf{b}_2^{(k)}\right) + \mathbf{b}_3^{(k)}\right)$$

其中 $\sigma(\cdot)$ 为Sigmoid激活函数，$s_{\min}=1.0$，$s_{\max}=5.0$ 分别为评分区间的下界和上界。具体的网络层级结构为：

$$\text{Linear}(d_{\text{embed}}, 256) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.3) \rightarrow \text{Linear}(256, 128) \rightarrow \text{ReLU} \rightarrow \text{Linear}(128, 1) \rightarrow \text{Sigmoid}$$

Sigmoid激活函数将网络输出约束在 $[0, 1]$ 范围内，再通过仿射变换映射至 $[1.0, 5.0]$ 评分区间，确保预测值在合理的评分范围内。各预测头之间共享Transformer编码器的参数但拥有独立的预测层参数，这种设计既利用了共享特征表示中的跨任务互补信息，又通过独立参数适应了各任务的特异性需求。

**② 多任务损失函数与标签掩码**

本研究采用加权均方误差（Weighted MSE）作为多任务联合训练的损失函数。由于数据集中各任务的标签存在约27-28%的缺失（部分视频未标注所有五个维度的分数），直接对所有样本计算损失会引入大量无效梯度信号。为此，本研究设计了基于有效标签掩码的损失计算机制。

对于第 $k$ 个任务，定义有效标签掩码 $\mathbf{m}_k \in \{0, 1\}^N$，其中 $m_{k,i} = 1$ 表示第 $i$ 个样本具有任务 $k$ 的有效标注。单任务损失为：

$$\mathcal{L}_k = \frac{1}{\sum_{i=1}^{N} m_{k,i}} \sum_{i=1}^{N} m_{k,i} \cdot (\hat{y}_{k,i} - y_{k,i})^2$$

多任务联合损失定义为各任务加权损失之和：

$$\mathcal{L}_{\text{total}} = \sum_{k=1}^{K} w_k \cdot \mathcal{L}_k$$

其中 $K=5$ 为任务总数，$w_k$ 为任务 $k$ 的权重。

**③ 动态任务权重策略**

如3.1.2节所分析，五个评估任务之间存在严重的标签分布不均衡现象。特别是lip\_sync任务的标签方差极低（均值4.763，标准差趋近于0），若采用等权策略，该任务的梯度信号将被其他高方差任务严重淹没，导致训练过程中该任务难以有效学习。为解决这一问题，本研究设计并对比了以下三种动态任务权重策略。

**策略一：基于先验知识的固定权重。** 根据各任务标签的统计特性，手动设定差异化的任务权重。由于lip\_sync任务的标签方差极低（几乎为常数），其损失值本身已很小，故赋予其较低的权重以避免对该近似恒定维度的过度拟合；对于标签分布更具区分性的其他任务，根据其在整体评估中的重要性和优化难度设定较高权重。本研究采用的固定权重配置为：

$$w_{\text{lip\_sync}} = 0.8, \quad w_{\text{expression}} = 1.2, \quad w_{\text{audio}} = 1.0, \quad w_{\text{cross\_modal}} = 1.5, \quad w_{\text{overall}} = 1.3$$

其中跨模态一致性维度被赋予最高权重（1.5），因为该维度是多模态融合质量的核心指标。

**策略二：基于不确定性的自适应加权。** 该策略引入可学习的同方差不确定性参数 $\sigma_k$（$k=1,\ldots,K$），通过贝叶斯框架自动调节各任务的相对权重。多任务损失函数改写为：

$$\mathcal{L}_{\text{total}} = \sum_{k=1}^{K} \frac{1}{2\sigma_k^2} \mathcal{L}_k + \ln \sigma_k$$

在训练过程中，高损失（即高不确定性）的任务其 $\sigma_k$ 会增大，从而降低该任务的有效权重，避免困难任务对整体优化方向的过度干扰；反之，低损失任务的权重会自动提升，使模型将更多优化资源分配给已具备较好基础的任务。

**策略三：基于梯度范数的动态平衡（GradNorm）。** 该策略通过监控各任务损失对共享参数的梯度范数 $\|\nabla_\theta \mathcal{L}_k\|_2$ 来动态调整权重。当某一任务的梯度范数相对于平均水平偏低时（表明该任务训练不足），自动增大其权重；反之则降低权重。GradNorm策略的权重更新规则为：

$$w_k(t+1) = w_k(t) \cdot \left(\frac{\|\nabla_\theta w_k \mathcal{L}_k\|_2}{\overline{\|\nabla_\theta w_k \mathcal{L}_k\|_2}} \cdot \frac{\tilde{L}_k(t)}{\overline{\tilde{L}_k(t)}}\right)^\alpha$$

其中 $\tilde{L}_k(t) = \mathcal{L}_k(t) / \mathcal{L}_k(0)$ 为任务 $k$ 的归一化损失比，$\alpha$ 为控制平衡力度的超参数。

上述三种策略从不同角度解决了多任务权重均衡问题，表3-4对三种策略的核心特征进行了对比总结。

**表3-4 三种动态任务权重策略对比**

| 对比维度 | 固定权重策略 | 不确定性加权策略 | GradNorm策略 |
|---------|:----------:|:-------------:|:-----------:|
| 权重调节方式 | 手动设定，训练中不变 | 通过可学习参数 $\sigma_k$ 自动调节 | 基于梯度范数动态更新 |
| 理论基础 | 先验知识与经验 | 贝叶斯同方差不确定性 | 梯度范数均衡 |
| 额外可学习参数 | 无 | $K$ 个不确定性参数 | $K$ 个权重参数 |
| 计算开销 | 低 | 低 | 中（需计算梯度范数） |
| 适应性 | 无自适应能力 | 自动适应损失变化 | 自动适应梯度冲突 |
| 适用场景 | 任务特性已知且稳定 | 任务难度动态变化 | 任务间梯度冲突严重 |

**（3）本节小结**

本节详细阐述了跨模态融合编码与多任务预测模块的设计与实现。在融合编码方面，本模块通过四路并行的模态嵌入网络将异构特征统一映射至 $d_{\text{embed}}$ 维语义空间，经过均值融合与可学习位置编码后，由多层多头自注意力Transformer编码器实现跨模态信息的深层交互，并通过全局平均池化输出 $d_{\text{embed}}$ 维融合特征向量。在多任务预测方面，五个独立的三层前馈预测头基于共享融合特征分别输出 $[1.0, 5.0]$ 区间的质量评分，配合标签掩码机制处理部分缺失标注问题。在权重优化方面，本研究设计了固定权重、不确定性加权和GradNorm三种动态权重策略，从先验知识、贝叶斯不确定性和梯度范数三个互补视角解决了多任务学习中的梯度冲突与收敛不均衡问题。经过本模块处理后，系统能够从多模态融合特征中联合预测五个维度的视频质量评分，为后续3.3节的实验验证提供了完整的模型架构基础。

## 3.3 实验设置

为全面验证本章所提出的多模态多任务说话人脸视频质量评估算法的有效性与实用性，本节对实验所涉及的数据集、评估指标体系、实验环境与训练配置、对比实验与消融实验方案以及主观评价实验设计进行详细说明。严谨且完整的实验设置是保证实验结果可复现性和结论可靠性的重要前提，本节从以下五个方面展开系统性阐述。

### 3.3.1 数据集与数据划分

**（1）数据集概况**

本研究采用EmotionTalk数据集作为实验的核心数据来源。EmotionTalk数据集是一个面向AI生成说话人脸视频质量评估任务的多维度标注数据集，包含1,985个由不同音频驱动方法（包括Wav2Lip、SadTalker等主流生成模型）生成的说话人脸视频样本。每个视频样本的时长经统一处理为6秒（150帧，帧率25fps），音频采样率统一为16,000Hz。数据集中的视频样本涵盖了不同性别（男性/女性）、不同年龄段（青年/中年）以及不同情感表达风格的说话人形象，确保了样本在说话人特征维度上的多样性与代表性。

在标注方面，EmotionTalk数据集提供了五个评估维度的人工主观评分标注，分别为唇形同步质量（lip\_sync）、表情自然度（expression）、音频质量（audio\_quality）、跨模态一致性（cross\_modal）和整体感知质量（overall），评分范围均为1.0（最差）至5.0（最优）。表3-5展示了各维度标签的详细统计信息。

**表3-5 EmotionTalk数据集各维度标签统计**

| 评估维度 | 有效标签数 | 无效标签数（-1.0） | 有效比例 | 均值 | 标准差 | 最小值 | 最大值 |
|---------|:---------:|:----------------:|:------:|:----:|:-----:|:-----:|:-----:|
| 唇形同步（lip\_sync） | 1,985 | 0 | 100.0% | 4.763 | ≈0 | 4.763 | 4.763 |
| 表情自然度（expression） | 1,445 | 540 | 72.8% | 3.21 | 0.58 | 1.0 | 5.0 |
| 音频质量（audio\_quality） | 1,545 | 440 | 77.8% | 3.45 | 0.52 | 1.0 | 5.0 |
| 跨模态一致性（cross\_modal） | 1,442 | 543 | 72.6% | 3.18 | 0.61 | 1.0 | 5.0 |
| 整体感知质量（overall） | 1,442 | 543 | 72.6% | 3.32 | 0.55 | 1.0 | 5.0 |

从表3-5可以观察到两个显著的数据特征：第一，唇形同步维度的标签方差接近于零（所有样本的评分均为4.763），这意味着该维度在模型训练中不具有有效的区分度，需要通过专门的权重策略进行处理（详见3.2.3节动态权重策略设计）；第二，表情自然度、跨模态一致性和整体感知质量三个维度存在约27\%–28\%的标签缺失（标记为-1.0），本研究通过标签掩码机制在训练时排除这些无效样本，确保损失计算的准确性。

**（2）数据划分策略**

为保证实验评估的客观性与统计可靠性，本研究采用固定比例的随机分层划分策略，将EmotionTalk数据集按80\%/10\%/10\%的比例划分为训练集、验证集和测试集三个互不重叠的子集。具体而言，训练集包含1,588个样本，用于模型参数的迭代优化；验证集包含199个样本，用于训练过程中的超参数调优和早停判定；测试集包含198个样本，仅在模型训练完成后用于最终的性能评估，以避免信息泄露。划分过程中设定固定随机种子（seed=42），确保实验结果的可复现性。表3-6展示了各子集的样本分布情况。

**表3-6 数据集划分统计**

| 数据子集 | 样本数量 | 占比 | 用途 |
|---------|:-------:|:---:|------|
| 训练集（Train） | 1,588 | 80% | 模型参数训练与优化 |
| 验证集（Val） | 199 | 10% | 超参数调优与早停判定 |
| 测试集（Test） | 198 | 10% | 最终性能评估 |
| **总计** | **1,985** | **100%** | — |

### 3.3.2 评估指标体系

为全面、多角度地衡量模型在五个评估维度上的预测性能，本研究构建了一套涵盖误差度量指标、相关性度量指标和一致性度量指标三大类别共11项评估指标的综合指标体系。表3-7对本研究所采用的全部评估指标进行了系统性的总结。

**（1）误差度量指标**

误差度量指标直接反映模型预测值与真实标注值之间的绝对偏差程度，是评估回归模型精度的基础指标。本研究采用以下四项误差指标：

① **均方误差**（Mean Squared Error, MSE）：计算预测值与真实值之差的平方均值，对较大的预测偏差具有更强的惩罚效果，定义为：

$$\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

② **均方根误差**（Root Mean Squared Error, RMSE）：为MSE的平方根，其数值量纲与原始评分一致，便于直观理解预测偏差的实际大小：

$$\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2}$$

③ **平均绝对误差**（Mean Absolute Error, MAE）：计算预测偏差的绝对值均值，相比MSE对离群值更加稳健：

$$\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|$$

④ **中位数绝对误差**（Median Absolute Error, MEDAE）：取预测偏差绝对值的中位数，进一步降低了离群值对整体评估结果的影响：

$$\text{MEDAE} = \text{median}(|y_1 - \hat{y}_1|, |y_2 - \hat{y}_2|, \ldots, |y_N - \hat{y}_N|)$$

**（2）相关性度量指标**

相关性度量指标衡量模型预测值与真实标注值之间的排序一致性和线性关联强度，是评价模型是否能够有效捕捉评分变化趋势的核心指标。本研究采用以下四项相关性指标：

① **决定系数**（Coefficient of Determination, $R^2$）：衡量模型预测对真实标注方差的解释能力，取值范围为 $(-\infty, 1]$，$R^2=1$ 表示完美预测，$R^2=0$ 表示模型预测能力与均值预测无异：

$$R^2 = 1 - \frac{\sum_{i=1}^{N}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{N}(y_i - \bar{y})^2}$$

② **Pearson相关系数**（Pearson Correlation Coefficient, $r$）：度量预测值与真实值之间的线性相关强度，取值范围为 $[-1, 1]$，是本研究的核心评估指标：

$$r = \frac{\sum_{i=1}^{N}(y_i - \bar{y})(\hat{y}_i - \bar{\hat{y}})}{\sqrt{\sum_{i=1}^{N}(y_i - \bar{y})^2 \cdot \sum_{i=1}^{N}(\hat{y}_i - \bar{\hat{y}})^2}}$$

③ **Spearman等级相关系数**（Spearman Rank Correlation Coefficient, $\rho$）：基于变量秩次计算的非参数相关性指标，不依赖于线性假设，能够捕捉预测值与真实值之间的单调关系：

$$\rho = 1 - \frac{6\sum_{i=1}^{N} d_i^2}{N(N^2-1)}$$

其中 $d_i = \text{rank}(y_i) - \text{rank}(\hat{y}_i)$ 为第 $i$ 个样本的真实值秩次与预测值秩次之差。

④ **Kendall秩相关系数**（Kendall's Tau, $\tau$）：基于一致对（concordant pairs）与非一致对（discordant pairs）计算的非参数相关指标，对样本对的排序一致性具有更严格的衡量：

$$\tau = \frac{C - D}{\frac{1}{2}N(N-1)}$$

其中 $C$ 为一致对数量（即两个样本的真实值大小关系与预测值大小关系一致的样本对数），$D$ 为非一致对数量。

**（3）一致性度量指标**

一致性度量指标不仅衡量预测值与真实值之间的相关趋势，还考察两者在绝对数值尺度上的一致程度，是评价模型输出是否可直接用于替代人工评分的关键指标。本研究采用以下三项一致性指标：

① **一致性相关系数**（Concordance Correlation Coefficient, CCC）：同时考量预测值与真实值之间的Pearson相关性和均值/方差偏移，是评估方法一致性的金标准指标：

$$\text{CCC} = \frac{2\rho\sigma_y\sigma_{\hat{y}}}{\sigma_y^2 + \sigma_{\hat{y}}^2 + (\mu_y - \mu_{\hat{y}})^2}$$

其中 $\rho$ 为Pearson相关系数，$\sigma_y$ 和 $\sigma_{\hat{y}}$ 分别为真实值与预测值的标准差，$\mu_y$ 和 $\mu_{\hat{y}}$ 分别为两者的均值。CCC的取值范围为 $[-1, 1]$，当且仅当预测值与真实值完全一致时取值为1。

② **三分类准确率**（Three-Class Classification Accuracy）：将连续评分区间 $[1.0, 5.0]$ 等距划分为低质量（$[1.0, 2.33]$）、中质量（$(2.33, 3.67]$）和高质量（$(3.67, 5.0]$）三个等级，计算模型预测等级与真实标注等级的分类一致率。该指标反映了模型在粗粒度质量分档判别上的准确性。

③ **二次加权Kappa系数**（Quadratic Weighted Kappa, QWK）：在考虑评分等级有序性的前提下衡量评估者间一致性，对偏差较大的错误赋予更高的惩罚权重。本研究将连续评分等距离散化为5个档位后计算QWK：

$$\text{QWK} = 1 - \frac{\sum_{i,j} w_{ij} O_{ij}}{\sum_{i,j} w_{ij} E_{ij}}$$

其中 $O_{ij}$ 为观测混淆矩阵，$E_{ij}$ 为期望混淆矩阵，$w_{ij} = (i-j)^2 / (K-1)^2$ 为二次权重，$K=5$ 为评分档位数。

**表3-7 评估指标体系总结**

| 指标类别 | 指标名称 | 符号 | 取值范围 | 最优方向 | 衡量对象 |
|---------|---------|:----:|:-------:|:------:|---------|
| 误差度量 | 均方误差 | MSE | $[0, +\infty)$ | ↓ | 预测偏差的平方均值 |
| 误差度量 | 均方根误差 | RMSE | $[0, +\infty)$ | ↓ | 与原始量纲一致的误差 |
| 误差度量 | 平均绝对误差 | MAE | $[0, +\infty)$ | ↓ | 稳健的绝对偏差均值 |
| 误差度量 | 中位数绝对误差 | MEDAE | $[0, +\infty)$ | ↓ | 抗离群值的中心偏差 |
| 相关性度量 | 决定系数 | $R^2$ | $(-\infty, 1]$ | ↑ | 方差解释比例 |
| 相关性度量 | Pearson相关系数 | $r$ | $[-1, 1]$ | ↑ | 线性相关强度 |
| 相关性度量 | Spearman等级相关 | $\rho$ | $[-1, 1]$ | ↑ | 单调排序一致性 |
| 相关性度量 | Kendall秩相关 | $\tau$ | $[-1, 1]$ | ↑ | 成对排序一致性 |
| 一致性度量 | 一致性相关系数 | CCC | $[-1, 1]$ | ↑ | 数值与趋势双重一致 |
| 一致性度量 | 三分类准确率 | Acc | $[0, 1]$ | ↑ | 粗粒度分档判别正确率 |
| 一致性度量 | 二次加权Kappa | QWK | $[-1, 1]$ | ↑ | 有序等级一致性 |

注：↑表示指标值越高越好，↓表示指标值越低越好。

此外，为评估指标估计的统计稳健性，本研究采用Bootstrap自举法构建各指标的95\%置信区间。具体而言，对测试集的预测结果进行200次有放回随机重采样（固定随机种子以保证可复现性），分别计算每次重采样后的各项评估指标值，取2.5\%和97.5\%分位数作为置信区间的下界和上界。

### 3.3.3 实验环境与训练配置

**（1）硬件与软件环境**

本研究的全部实验在配备NVIDIA GPU的深度学习工作站上完成。表3-8列出了实验所依赖的核心软硬件环境配置信息。

**表3-8 实验环境配置**

| 配置项 | 详细参数 |
|-------|---------|
| 操作系统 | Windows 10/11 Professional 64位 |
| GPU | NVIDIA GeForce RTX 4060 Laptop（8GB GDDR6） |
| CUDA版本 | 12.1 |
| CPU | Intel Core i7-12700H（14核20线程） |
| 内存 | 32GB DDR5 |
| 深度学习框架 | PyTorch 2.1.0 + CUDA 12.1 |
| Python版本 | 3.10.11 |
| 关键依赖库 | NumPy 1.24、SciPy 1.11、scikit-learn 1.3、Transformers 4.35 |

**（2）训练配置**

为系统性地评估模型在不同架构规模与训练策略下的性能表现，本研究设计了两套完整的实验配置方案：标准配置和优化配置。两套配置在模型架构、训练超参数和数据预处理三个层面存在差异，表3-9对其进行了详细对比。

**表3-9 标准配置与优化配置的超参数对比**

| 配置项 | 标准配置 | 优化配置 |
|-------|:-------:|:-------:|
| **模型架构** | | |
| 编码器/隐藏维度 $d_{\text{embed}}$ | 256 | 512 |
| Transformer层数 $L$ | 3 | 6 |
| 注意力头数 $H$ | 8 | 16 |
| 前馈维度 $d_{\text{ff}}$ | 1,024 | 2,048 |
| 预测头隐藏维度 | [128, 64] | [256, 128] |
| Dropout率 | 0.2 / 0.3 | 0.2 / 0.3 |
| **训练超参数** | | |
| 优化器 | Adam | AdamW |
| 学习率 $\eta$ | $1 \times 10^{-4}$ | $1 \times 10^{-3}$ |
| 权重衰减 $\lambda$ | $1 \times 10^{-4}$ | $1 \times 10^{-2}$ |
| 训练轮次 | 30 | 150 |
| 批次大小 | 8 | 2 |
| 梯度累积步数 | 4 | 8 |
| 等效批次大小 | 32 | 16 |
| 学习率调度 | 余弦退火 | 余弦退火 + Warmup |
| 早停轮次（patience） | 10 | 15 |
| 混合精度训练（AMP） | 是 | 是 |
| 梯度裁剪阈值 | — | 1.0 |
| 标签平滑系数 | — | 0.1 |
| **数据预处理** | | |
| 特征标准化 | Z-score | Z-score |
| NaN值处理 | 中位数插补 | 中位数插补 |
| PCA降维 | 4,237→367维 | 4,237→367维 |
| **动态权重策略** | 不确定性加权 | 不确定性加权 |
| 一致性正则模式 | 相关性（corr） | 相关性（corr） |
| 一致性损失权重 $\lambda_c$ | 0.05 | 0.1 |

两套配置的核心差异体现在以下三个方面：

第一，在模型容量方面，优化配置将编码维度从256提升至512，Transformer层数从3层增至6层，注意力头数从8增至16，显著增强了模型的跨模态特征交互与表达能力。

第二，在训练策略方面，优化配置采用了AdamW优化器（解耦权重衰减正则化）替代标准Adam，并将学习率提高一个数量级（$10^{-4} \rightarrow 10^{-3}$）配合更强的权重衰减（$10^{-4} \rightarrow 10^{-2}$），以实现更高效的参数空间探索。同时，优化配置引入了梯度裁剪（阈值1.0）和标签平滑（系数0.1）两项正则化技术，分别用于防止梯度爆炸和缓解标签噪声对训练的干扰。

第三，在训练规模方面，优化配置将最大训练轮次从30轮扩展至150轮，并配合更大的早停耐心值（15轮），为更深的模型架构提供充分的收敛时间。

两套配置均通过统一的YAML配置文件进行管理（标准配置对应 `config/config.yaml`，优化配置对应 `config/optimized_config.yaml`），切换配置仅需修改命令行参数，无需修改任何模型或训练代码，体现了本研究工程实现的良好可配置性。

在训练过程中，所有实验均设定固定随机种子（seed=42），包括Python内置随机数生成器、NumPy随机数生成器以及PyTorch的CPU和CUDA随机数生成器，以确保在相同硬件环境和配置条件下实验结果的完全可复现性。

### 3.3.4 对比实验与消融实验设计

为系统性地量化本研究所提出的各关键算法模块对整体评估性能的贡献，本节设计了一组覆盖模态特征、融合架构、降维策略与训练策略四个维度的消融实验方案。所有消融实验均在优化配置的基础上进行单一变量控制，即每次仅移除或替换一个目标模块，其余设置保持一致，以确保性能差异能够准确归因于被移除的模块。表3-10对消融实验的完整设计方案进行了总结。

**表3-10 消融实验设计方案**

| 实验编号 | 实验配置 | 控制变量 | 验证目标 |
|:-------:|---------|---------|---------|
| E0 | 完整模型（Full Model） | — | 基准性能 |
| E1 | 去除视觉特征（w/o Visual） | 移除ResNet101视觉模态 | 视觉特征的贡献 |
| E2 | 去除音频特征（w/o Audio） | 移除HuBERT音频模态 | 音频特征的贡献 |
| E3 | 去除关键点特征（w/o Keypoint） | 移除MediaPipe关键点模态 | 面部几何特征的贡献 |
| E4 | 去除AU特征（w/o AU） | 移除面部动作单元模态 | 动作单元特征的贡献 |
| E5 | 去除Transformer融合（w/o Transformer） | 以直接拼接+MLP替代Transformer | 跨模态注意力机制的贡献 |
| E6 | 去除PCA降维（w/o PCA） | 使用原始4,237维特征直接输入 | PCA降维的贡献 |
| E7 | 固定权重策略（Fixed Weight） | 以固定权重替代不确定性加权 | 动态权重策略的贡献 |
| E8 | 标准配置（Standard Config） | 使用3层/8头/256维配置 | 模型架构深度/宽度的贡献 |

上述消融实验涵盖了以下四个核心验证维度：

**① 模态特征贡献验证**（E1–E4）：通过逐一移除各模态特征输入，量化视觉、音频、面部关键点和动作单元四类模态特征对评估性能的独立贡献比例。其中，对于单模态去除实验，被移除模态对应的嵌入层输出以零向量替代，其余三路模态正常参与融合编码。

**② 融合架构贡献验证**（E5）：将Transformer跨模态编码器替换为简单的特征拼接+两层MLP结构，验证基于注意力机制的深层跨模态交互融合相比浅层特征拼接的性能优势。

**③ 降维策略贡献验证**（E6）：跳过PCA降维步骤，直接将原始4,237维特征经标准化后输入模型，验证特征降维在缓解维度灾难与提升训练效率方面的有效性。

**④ 训练策略贡献验证**（E7–E8）：分别检验动态任务权重策略和模型架构深度/宽度对评估性能的影响。其中E7将不确定性加权策略替换为固定权重策略（各任务权重均为1.0），E8采用更小的标准配置模型。

此外，本研究还设计了两套模型配置（标准配置 vs 优化配置）的全面对比实验，从训练效率（收敛速度、训练时间）和评估精度（各维度评估指标）两个角度综合评价不同模型规模的性价比权衡。

### 3.3.5 主观评价实验设计

为验证模型自动化评分与人类主观感知之间的一致性，本研究设计并实施了一项结构化的主观评价实验。

**（1）实验设计**

主观评价实验采用被试内设计（Within-Subject Design），以消除个体间差异对实验结果的影响。实验采用A/B对比范式，参与者在每个评价试次中同时观看随机呈现的两段视频（分别由不同生成方法或不同质量水平产生），并对每段视频从唇形同步、表情自然度、音频质量、跨模态一致性和整体感知质量五个维度进行1.0至5.0分的独立评分（精度为0.1分）。视频的左右呈现位置在试次间随机化，标签（A/B）与实际生成方法之间的对应关系对参与者不可见，以实现单盲控制。

**（2）参与者**

实验招募了在校大学生作为评价参与者。所有参与者均具备正常的视觉和听觉能力，且具有一定的视频内容消费经验。在正式实验开始前，参与者接受了约10分钟的培训，内容包括五个评估维度的定义说明、评分标准示例以及评分工具的使用方法。

**（3）实验材料**

实验视频素材从EmotionTalk数据集中按照分层抽样原则选取，确保所选视频在整体质量分布（低/中/高质量段）、说话人性别（男/女）和生成方法类型上具有代表性。每位参与者需完成全部配对视频的评价任务，实验总时长约30–45分钟，中间设置休息环节以避免评价疲劳效应。

**（4）实验环境与质量控制**

实验通过本研究开发的Web端主观评价平台进行在线数据收集。参与者在安静的实验室环境中使用标准耳机和显示器（分辨率不低于1920×1080）完成评价任务。为保证数据质量，实验设计了以下质量控制措施：① 在评价序列中嵌入注意力检查试次（使用质量差异极为显著的视频对），若参与者在注意力检查试次上的评分偏差超过预设阈值，则标记该参与者的数据待复核；② 记录每个试次的完成时间，对响应时间异常短（低于3秒）或异常长（超过120秒）的试次进行标记。

**（5）统计分析方法**

主观评价实验的分析方案包括以下内容：计算各评估维度上评估者间一致性指标（组内相关系数ICC），以验证人工评分的可靠性；以多位评估者评分的均值作为人工主观评分的汇总值，计算模型自动评分与人工评分之间的Pearson相关系数和Spearman等级相关系数，量化两者的线性关联强度与排序一致性；此外，通过配对样本t检验或Wilcoxon符号秩检验分析A/B配对视频在各评估维度上的评分差异是否达到统计显著性水平（$p < 0.05$）。
