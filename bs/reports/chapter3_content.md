# 第3章 多模态多任务说话人脸视频质量评估算法设计与实现

随着音频驱动的说话人脸视频生成技术（如Wav2Lip、SadTalker、GeneFace++等）在虚拟数字人、在线教育和视频配音等场景中的广泛应用，对生成视频的感知质量进行客观、多维度的自动化评估成为该领域亟待解决的问题。不同于传统视频质量评估侧重编码失真与信号保真度，说话人脸视频的质量评估需要衡量口型与语音的同步精度、面部表情的自然度、合成语音的音质以及视听之间的协调一致性等多个感知维度。然而，现有方法多局限于单一维度的评估（如SyncNet仅关注唇形同步），难以满足多维度联合评估的需求。为此，本章提出一种基于跨模态Transformer的多模态多任务学习框架，实现说话人脸视频质量的多维度联合评估。

本章内容组织如下：3.1节从多模态特征融合、多任务标签不均衡和高维特征空间优化三个方面分析技术挑战；3.2节阐述算法的总体架构及各模块的设计与实现；3.3节介绍数据集、评估指标体系和消融实验方案；3.4节展示并分析实验结果；3.5节对全章工作进行总结。

## 3.1 问题分析

说话人脸视频质量评估涉及视觉呈现、音频表达、面部动态及跨模态协调等多个感知维度的综合判断，传统的单模态、单任务评估方法难以胜任。结合本研究所用EmotionTalk数据集（1,985个样本）的统计特性，本节从以下三个方面讨论该任务面临的技术挑战。

### 3.1.1 多模态异构特征的语义对齐与融合难题

AI生成说话人脸视频的质量评估需要从视觉、音频、面部几何结构和面部动作等多个互补的信息通道中提取具有判别力的特征表示。在本研究中，针对每个视频样本（统一为150帧、25fps的标准序列），采用四类预训练模型分别提取不同模态的特征：

（1）**视觉特征**：采用在ImageNet数据集上预训练的ResNet101深度卷积网络作为视觉编码器，对每帧图像提取2048维的高层视觉语义表示。该特征能够有效捕捉面部纹理细节、渲染质量和视觉伪影等信息，对于评估合成人脸的视觉真实感具有重要作用。

（2）**音频特征**：采用自监督预训练的HuBERT（Hidden-Unit BERT）模型提取768维的音频嵌入表示。HuBERT通过大规模无标注语音数据的掩码预测任务学习到丰富的语音语义信息，能够表征语音的清晰度、自然度及韵律特征。

（3）**面部关键点特征**：采用MediaPipe Face Mesh模型提取每帧图像中468个面部三维关键点坐标，形成1404维（468×3）的面部几何结构特征。该特征刻画了面部各区域（眼部、口部、轮廓等）的空间位置关系与运动轨迹，对评估面部表情的自然度和口型变化的合理性具有重要作用。

（4）**面部动作单元特征**：基于MediaPipe Face Mesh检测到的面部关键点，通过关键点间的几何距离计算方法提取17维面部动作单元（Action Unit, AU）强度值。动作单元基于面部动作编码系统（Facial Action Coding System, FACS），以标准化的方式描述面部肌肉运动模式，能够量化表征微表情变化和情绪表达的细微差异。

上述四类特征的原始总维度高达4237维（2048+768+1404+17），且它们在维度尺度、数值范围和语义空间上存在显著的异构性差异。具体而言，视觉特征位于高维卷积语义空间中，数值分布呈非负特性；音频特征处于自监督表征学习的嵌入空间中，数值范围较为对称；面部关键点特征本质上是低层级的几何坐标信息，数值范围跨度大且与图像分辨率强相关；动作单元特征则是经过编码的动作强度标量，维度远低于其他模态。

对本研究数据集中各模态特征的统计分析进一步揭示了严重的数值尺度失衡现象。四类特征的数值范围差异超过三个数量级：关键点特征的数值范围约为[-30142, 30142]，视觉特征约为[0, 14210]，音频特征约为[-6741, 6749]，而动作单元特征约为[-941, 941]。

这种跨模态的尺度失衡意味着，若直接将四类原始特征进行简单拼接或逐元素融合，维度较高且数值范围较大的模态（如关键点和视觉特征）将在梯度更新中占据主导地位，而低维度模态（如动作单元特征）的信息贡献则容易被淹没，从而导致融合后的特征表示无法充分利用各模态的互补信息。此外，不同模态特征所处的语义空间本质上不可直接比较——卷积网络提取的视觉语义与自监督模型学习的音频嵌入之间缺乏天然的对应关系，简单的线性映射难以实现有效的跨模态语义对齐。因此，如何设计合理的特征预处理与跨模态融合机制，将异构模态特征映射至统一的语义空间并实现深层信息交互，是本章需要解决的首要问题。

### 3.1.2 多任务评估的标签不均衡与优化冲突难题

本研究将说话人脸视频质量评估建模为一个多任务回归问题，同时预测以下五个评估维度的质量分数（评分区间为1.0至5.0分）：

（1）**唇形同步质量**（lip\_sync）：衡量生成视频中口型运动与驱动语音之间的时序同步精度，是说话人脸视频最基本的质量要求。

（2）**表情自然度**（expression）：评估面部表情变化的真实感与流畅性，包括情绪表达的合理性和表情转换的平滑度。

（3）**音频质量**（audio\_quality）：评价合成或处理后语音的清晰度、自然度和音质保真度。

（4）**跨模态一致性**（cross\_modal）：综合评估视觉、音频与面部运动三个通道之间的协调一致程度。

（5）**整体感知质量**（overall）：从主观感知的角度对视频整体质量给出综合评分。

然而，对EmotionTalk数据集中五个维度标签分布的深入统计分析揭示了一个严重的标签不均衡问题。在全部1,985个视频样本中，唇形同步维度的标签值几乎完全相同（均值为4.763，标准差趋近于零），呈现出极端的低方差特性；与之形成鲜明对比的是，表情自然度（标准差约0.590）、音频质量（标准差约0.519）、跨模态一致性（标准差约0.566）和整体感知质量等其余维度的标签则呈现出正常的分布特征和合理的方差水平。这一现象表明，数据集中的说话人脸视频在唇形同步质量上高度一致（普遍达到较高水平），但在表情表达、音频品质和跨模态协调等维度上存在显著的质量差异。

上述标签分布的严重不均衡给多任务学习框架的设计带来了本质性的挑战。在标准的多任务学习范式中，总损失函数通常表示为各子任务损失的加权和：

$$L_{total} = \sum_{k=1}^{K} w_k \cdot L_k$$

其中 $K=5$ 为任务数量，$w_k$ 为第 $k$ 个任务的权重，$L_k$ 为对应的均方误差损失。当采用固定等权重策略（$w_k=1, \forall k$）时，由于唇形同步任务的标签方差趋近于零，其损失值量级远小于其他任务，导致该任务在总损失中的梯度贡献几乎可以忽略，模型将无法学习到有意义的唇形同步评估能力。反之，若为唇形同步任务设置过大的权重以补偿其低方差特性，则该任务的梯度信号可能反过来干扰其他任务的正常优化过程，引发负迁移（negative transfer）现象。

此外，数据集中标签的不完整性进一步加剧了多任务优化的复杂性。统计分析表明，除唇形同步维度的标签完整率为100%外，表情自然度的有效标签比例仅为72.8%，音频质量为77.8%，跨模态一致性和整体感知质量均为72.6%，其余样本的标签以无效值（-1.0）标记。这意味着不同任务在每个训练批次中的有效样本数量存在差异，进一步增大了多任务梯度均衡的难度。因此，如何设计自适应的动态任务权重策略，使模型在标签分布不均衡且部分缺失的条件下同时优化五个子任务，是本章面临的第二个问题。

### 3.1.3 高维特征空间下的训练效率与模型泛化难题

如前文所述，四类模态特征的原始总维度高达4237维，每个视频样本对应的特征序列形状为150×4237（150个时间步，每步4237维特征向量）。如此高维的输入特征空间给模型训练带来了多方面的挑战。

首先，**维度灾难**（curse of dimensionality）问题。在高维特征空间中，数据点之间的距离趋于均匀化，传统的距离度量和相似性计算逐渐失效。对于本研究所采用的中等规模数据集（1,985个样本，其中训练集仅约1,588个），这一问题尤为突出：高维输入意味着模型需要估计的参数数量与输入维度成正比增长，而有限的训练样本难以为如此多的参数提供充分的约束，极易导致模型在训练集上过拟合而在测试集上泛化性能下降。

其次，**特征冗余与噪声干扰**问题。在4237维的原始特征中，不同模态特征之间以及同一模态特征的不同维度之间可能存在大量的冗余信息。例如，面部关键点的1404维坐标特征中，许多关键点在帧间的运动具有高度的空间相关性，其独立信息量远低于名义维度所暗示的水平。此外，由于特征提取过程中视觉特征存在约0.08%的缺失值（共计2,445个NaN值需要通过中位数插补进行填充），插补引入的噪声也可能影响模型的学习效果。这些冗余和噪声维度不仅增加了不必要的计算开销，还可能干扰模型对真正具有判别力的特征模式的学习。

最后，**训练收敛效率**问题。高维输入特征经过嵌入层映射后，产生的梯度信号需要穿越更深、更宽的网络结构才能回传至各模态的嵌入层，梯度消失或梯度爆炸的风险随维度增加而加剧。同时，高维特征空间中损失函数的优化曲面更加复杂，存在更多的鞍点和局部极小值，使得基于随机梯度下降的优化算法更难以高效地收敛到全局最优解。当结合前述的多任务优化框架时，不同任务的梯度在高维空间中发生冲突的概率进一步增大，导致训练过程中出现振荡或收敛缓慢的现象。

因此，如何在保持各模态特征表达能力的前提下，通过合理的降维与预处理策略缓解维度灾难，并配合训练优化技术提升收敛效率和泛化性能，是本章需要解决的第三个问题。

### 3.1.4 本章技术方案概述

基于上述三个问题的分析，本章提出构建一个基于跨模态Transformer的多模态多任务学习框架，实现说话人脸视频质量的多维度评估。该框架的设计思路如下：

针对多模态异构特征的语义对齐与融合难题，本研究首先通过特征级的标准化预处理（StandardScaler）消除不同模态间的数值尺度差异，然后利用主成分分析（PCA）将原始4237维特征压缩至367维（降维率达91.3%），在大幅降低特征冗余的同时保留95%以上的原始方差信息。在此基础上，设计四路并行的模态特定嵌入网络将降维后的异构特征映射至统一维度的语义空间，并通过多层多头自注意力机制实现跨模态信息的深层交互融合。

针对多任务评估的标签不均衡与优化冲突难题，本研究引入了自适应的动态任务权重策略，包括基于同方差不确定性的自动权重调节机制和基于梯度范数均衡的GradNorm策略，使模型能够根据各任务的训练进度和难度自动调整损失权重，从而在标签分布不均衡的条件下实现多任务的协同优化。同时，采用掩码机制处理无效标签，确保缺失标签不参与损失计算。

针对高维特征空间下的训练效率与模型泛化难题，本研究提出了一套综合性的训练优化方案，涵盖梯度检查点、混合精度训练、梯度累积、自适应学习率调度等多项技术，以提升模型在高维多任务场景下的训练稳定性和收敛效率，同时通过标签平滑、Dropout正则化和早停策略有效防止过拟合。

上述技术方案的详细设计与实现将在3.2节中展开阐述。

## 3.2 算法设计

### 3.2.1 算法设计方案

基于3.1节的问题分析，本节给出多模态多任务说话人脸视频质量评估算法的总体设计方案。该算法以端到端的计算管线形式组织，依次经过多模态特征提取、特征预处理与降维、跨模态Transformer融合编码以及多任务质量预测四个阶段，最终输出唇形同步、表情自然度、音频质量、跨模态一致性和整体感知质量五个维度的评分。图3-1展示了算法的总体架构。

> **【图3-1 多模态多任务说话人脸视频质量评估算法总体架构图】**
>
> 图示说明：该图应从左至右展示四个核心阶段的数据流。左侧为输入端（原始视频），依次经过：（1）多模态特征提取阶段——四路并行的特征提取器（ResNet101→2048维、HuBERT→768维、MediaPipe→1404维、AU计算→17维），总维度4237；（2）特征预处理与降维阶段——NaN插补→Z-score标准化→PCA降维，总维度压缩至367维；（3）跨模态Transformer融合编码阶段——四路嵌入网络→均值融合→可学习位置编码→6层16头Transformer编码器→全局平均池化→512维融合向量；（4）多任务质量预测阶段——五个独立预测头分别输出lip\_sync、expression、audio\_quality、cross\_modal、overall五个维度的\[1.0, 5.0\]评分。右侧标注动态任务权重与标签掩码机制。

**（1）多模态特征提取阶段**

算法的第一阶段负责从原始视频中提取多模态感知特征。给定一个输入视频，系统首先将其统一采样为150帧（帧率25fps）的标准序列，并分离出音频流（采样率16000Hz）。随后，四路预训练特征提取器并行提取四类模态特征：基于ResNet101的视觉特征（150×2048维）、基于HuBERT的音频特征（150×768维）、基于MediaPipe Face Mesh的面部关键点特征（150×1404维）以及基于关键点几何距离的动作单元特征（150×17维）。此外，SyncNet模块计算音视频同步置信度与偏移量，作为辅助参考信息。经过该阶段，每个视频样本的原始特征总维度为4237维。

**（2）特征预处理与降维阶段**

算法的第二阶段对原始特征进行预处理与降维，以解决3.1.1节和3.1.3节分析的尺度失衡与高维冗余问题。该阶段依次执行缺失值中位数插补、Z-score标准化和PCA降维三个步骤，将四类特征的总维度从4237维压缩至367维（降维率91.3\%），在保留主要方差信息的同时大幅降低了后续模型的参数规模与计算复杂度。各步骤的具体实现细节详见3.2.2节。

**（3）跨模态Transformer融合编码阶段**

算法的第三阶段负责将降维后的四类异构模态特征映射至统一的语义空间，并通过深层注意力机制实现跨模态信息的交互融合。具体而言，四路模态嵌入网络分别将各模态特征映射至统一的512维语义空间后，通过逐元素平均融合并叠加可学习的位置编码，再送入6层16头Transformer编码器进行深层语义编码。编码器输出的时序特征经全局平均池化聚合为512维的全局融合特征向量，供后续预测头使用。该阶段的详细设计见3.2.3节。

**（4）多任务质量预测阶段**

算法的第四阶段基于融合后的全局特征向量，通过五个独立的任务预测头分别输出五个质量维度的评分。每个预测头采用三层全连接网络（512→256→128→1），最后通过Sigmoid激活和线性缩放映射至[1.0, 5.0]的评分区间。五个预测头参数独立但共享Transformer编码器的融合特征，兼顾了各维度评估的差异性与多任务间的信息共享。

训练阶段，总损失为各子任务加权均方误差之和。针对3.1.2节分析的标签不均衡与缺失问题，本研究引入动态任务权重策略与标签掩码机制：无效标签（-1.0）通过掩码排除于损失计算之外，不同任务间的损失量级差异由自适应权重策略动态调节。推理阶段，系统依次执行四个阶段的前向计算，输出五个维度的质量评分。

**（5）总体设计特点**

综合来看，本算法的总体设计具有以下特点：第一，模块化架构，各阶段功能职责明确、接口清晰，便于独立开发与灵活组合；第二，端到端可训练，从特征嵌入层到多任务预测头的所有参数均可通过反向传播联合优化。本研究最终选用优化配置（编码维度512、6层Transformer、16注意力头）作为正式模型架构，同时保留标准配置（编码维度256、3层Transformer、8注意力头）作为消融实验的对比基线，两套配置通过统一的YAML文件切换。上述设计方案的各模块将在3.2.2和3.2.3小节中展开阐述。

### 3.2.2 多模态特征提取模块

多模态特征提取是整个评估算法的基础环节，本节阐述四类模态特征的提取方法、唇形同步分数的计算方式以及特征预处理与降维流程。图3-2展示了多模态特征提取模块的整体架构。

> **【图3-2 多模态特征提取模块架构图】**
>
> 图示说明：该图应以输入视频为起点，展示五路并行的特征提取流程：（1）视觉路径——视频帧→RetinaFace人脸检测→224×224裁剪→ImageNet归一化→ResNet101（去FC层）→GAP→2048维/帧；（2）音频路径——FFmpeg分离音频→16kHz PCM→HuBERT-Base→768维→线性插值对齐至150帧；（3）关键点路径——MediaPipe Face Mesh→468个3D关键点→展平为1404维/帧；（4）AU路径——基于关键点几何距离→17个AU强度值→最大值归一化；（5）SyncNet路径——音视频同步置信度与偏移量→Sigmoid映射→\[0,5\]分数。下方展示预处理流程：NaN中位数插补→Z-score标准化→PCA降维（表3-2配置），最终输出367维标准化特征。

**（1）视觉与音频特征提取**

**① 视觉特征提取**

视觉特征负责捕捉生成视频中人脸区域的纹理细节、渲染质量与视觉伪影等信息。本研究采用在ImageNet数据集上预训练的ResNet101深度残差卷积网络作为视觉特征编码器。

给定一个输入视频 $V$，系统首先将其统一采样为 $T=150$ 帧（目标帧率 $f_{\text{target}}=25$ fps）的标准序列。采样间隔为 $\Delta = \max(1, \lfloor f_{\text{orig}} / f_{\text{target}} \rceil)$，若采样后帧数超过150帧则均匀下采样，不足则以零向量填充。对于第 $t$ 帧图像 $I_t$（$t=1,2,\ldots,T$），系统通过RetinaFace人脸检测器定位面部区域，将裁剪后的人脸图像缩放至 $224\times224$ 像素并按ImageNet标准进行通道归一化：

$$\hat{I}_t^{(c)} = \frac{I_t^{(c)} - \mu_c}{\sigma_c}, \quad c \in \{R, G, B\}$$

其中 $\mu = [0.485, 0.456, 0.406]$ 和 $\sigma = [0.229, 0.224, 0.225]$ 分别为ImageNet数据集各通道的均值和标准差。

归一化后的帧图像被送入ResNet101网络，本研究移除最后的全连接分类层，以全局平均池化层的输出作为视觉特征向量：

$$\mathbf{v}_t = \text{GAP}(\text{ResBlock}_4(\text{ResBlock}_3(\text{ResBlock}_2(\text{ResBlock}_1(\text{Conv}_1(\hat{I}_t)))))) \in \mathbb{R}^{2048}$$

其中 $\text{GAP}(\cdot)$ 表示全局平均池化操作，$\text{ResBlock}_i$ 表示第 $i$ 个残差块组。最终，每个视频样本的视觉特征矩阵为 $\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_T]^\top \in \mathbb{R}^{150 \times 2048}$。

**② 音频特征提取**

音频特征负责表征语音信号的清晰度、自然度和韵律特征。本研究采用Meta AI提出的HuBERT（Hidden-Unit BERT）自监督预训练语音模型作为音频特征编码器。HuBERT通过在大规模无标注语音数据上进行掩码预测任务的预训练，学习到了丰富的语音语义表示。

音频特征的提取流程如下：首先，通过FFmpeg工具从输入视频中分离音频流，转换为采样率 $f_s = 16000$ Hz、单声道的PCM格式波形信号 $\mathbf{a} = [a_1, a_2, \ldots, a_N]$，其中 $N = f_s \times D$ 为总采样点数。波形信号经Wav2Vec2特征提取器预处理后，送入HuBERT-Base模型（包含7层卷积编码器和12层Transformer编码器），提取最后一层隐藏状态作为音频特征：

$$\mathbf{H}_{\text{audio}} = \text{HuBERT}(\mathbf{a}) = \text{Transformer}_{12}(\text{CNN}(\mathbf{a})) \in \mathbb{R}^{T_a \times 768}$$

其中 $T_a$ 为HuBERT输出的时间步数，768为隐藏维度。由于HuBERT卷积编码器对输入波形进行了约320倍降采样（每20ms一帧），$T_a$ 与视频帧数 $T=150$ 通常不一致。为实现音视频特征的时间对齐，本研究采用线性插值将音频特征统一调整为150个时间步：

$$\mathbf{A} = \text{Interpolate}(\mathbf{H}_{\text{audio}}, T) \in \mathbb{R}^{150 \times 768}$$

**（2）面部关键点与动作单元特征提取**

**① 面部关键点特征提取**

面部关键点特征通过精细的面部几何结构信息刻画口型变化、表情运动和面部轮廓形变等动态特征。本研究采用Google提出的MediaPipe Face Mesh模型，该模型能够在单张图像中实时检测468个面部三维关键点，覆盖眉毛、眼睛、鼻子、嘴唇、下颌轮廓等所有关键区域（检测置信度阈值设为0.5）。

对于第 $t$ 帧图像 $I_t$，MediaPipe Face Mesh输出468个关键点的三维坐标 $\mathbf{P}_t = \{(x_i^t, y_i^t, z_i^t)\}_{i=1}^{468}$，其中 $(x_i^t, y_i^t)$ 为归一化图像坐标（值域为 $[0, 1]$），$z_i^t$ 为相对深度估计值。将每帧的三维坐标展平为一维向量，得到1404维（$468 \times 3$）的关键点特征：

$$\mathbf{k}_t = [x_1^t, y_1^t, z_1^t, x_2^t, y_2^t, z_2^t, \ldots, x_{468}^t, y_{468}^t, z_{468}^t]^\top \in \mathbb{R}^{1404}$$

对于未检测到人脸的帧，以零向量填充以保持序列完整性。最终，关键点特征矩阵为 $\mathbf{K} = [\mathbf{k}_1, \mathbf{k}_2, \ldots, \mathbf{k}_T]^\top \in \mathbb{R}^{150 \times 1404}$。关键点特征的独特价值在于其直接编码了面部各部位的空间位置关系和帧间运动轨迹，例如口唇区域关键点的时序运动模式可直接反映口型变化的节奏与幅度，为唇形同步评估提供了显式的几何线索。

**② 面部动作单元特征提取**

面部动作单元（Action Unit, AU）是面部动作编码系统（FACS）中定义的基本面部肌肉运动单位，以标准化方式量化描述面部表情的细微变化。本研究基于MediaPipe Face Mesh检测到的468个关键点，通过几何距离计算方法近似估计17个核心动作单元的激活强度，避免了对额外外部工具的依赖。

对于第 $t$ 帧的关键点集合 $\mathbf{P}_t$，各动作单元的强度值通过特定关键点对之间的欧氏距离 $d(\cdot, \cdot)$ 计算获得。以眉部区域为例，AU1（内侧眉毛上提）通过左右内侧眉毛关键点与鼻根关键点之间距离的均值表征：

$$\text{AU1}_t = \frac{1}{2}\left(d(\mathbf{p}_{\text{brow\_inner\_L}}^t, \mathbf{p}_{\text{nose}}^t) + d(\mathbf{p}_{\text{brow\_inner\_R}}^t, \mathbf{p}_{\text{nose}}^t)\right)$$

对于眼部区域，AU5（上眼睑提升）通过上下眼睑关键点距离的均值表征：

$$\text{AU5}_t = \frac{1}{2}\left(d(\mathbf{p}_{\text{upper\_lid\_L}}^t, \mathbf{p}_{\text{lower\_lid\_L}}^t) + d(\mathbf{p}_{\text{upper\_lid\_R}}^t, \mathbf{p}_{\text{lower\_lid\_R}}^t)\right)$$

对于口唇区域，AU25（双唇分开）通过上下唇关键点之间的距离表征，AU26（下颌张开）定义为AU25的线性放大：

$$\text{AU25}_t = d(\mathbf{p}_{\text{upper\_lip}}^t, \mathbf{p}_{\text{lower\_lip}}^t), \quad \text{AU26}_t = 1.2 \times \text{AU25}_t$$

此外，部分动作单元通过逆运算定义，如AU7（眼睑收紧）$\text{AU7}_t = 1/(\text{AU5}_t + \epsilon)$，其中 $\epsilon = 10^{-6}$ 为防止除零的微小常数。表3-1汇总了本研究提取的17个动作单元的完整定义。

**表3-1 17个面部动作单元的定义与计算方法**

| 动作单元 | 名称 | 计算方法 |
|---------|------|---------|
| AU1 | 内侧眉毛上提 | 内侧眉毛与鼻根距离的均值 |
| AU2 | 外侧眉毛上提 | 外侧眉毛与鼻根距离的均值 |
| AU4 | 眉毛下压 | 左右眉毛中部的间距 |
| AU5 | 上眼睑提升 | 上下眼睑距离的均值 |
| AU6 | 面颊上提 | 眼睛至嘴角距离的均值 |
| AU7 | 眼睑收紧 | $1/(\text{AU5} + \epsilon)$ |
| AU9 | 鼻部皱缩 | 鼻翼关键点间距 |
| AU10 | 上唇上提 | 上唇与鼻尖距离 |
| AU12 | 唇角拉伸 | 嘴角与参考点距离的均值 |
| AU14 | 酒窝形成 | $1/(\text{AU12} + \epsilon)$ |
| AU15 | 唇角下拉 | $0.8 \times \text{AU14}$ |
| AU17 | 下颏上提 | 下唇与鼻尖距离 |
| AU20 | 嘴唇横向拉伸 | 左右嘴角间距 |
| AU23 | 嘴唇收紧 | $1/(d(\text{upper\_lip}, \text{lower\_lip}) + \epsilon)$ |
| AU25 | 双唇分开 | 上下唇间距 |
| AU26 | 下颌张开 | $1.2 \times \text{AU25}$ |
| AU28 | 嘴唇内卷 | $1/(\text{AU20} + \epsilon)$ |

提取完成后，每帧的17维AU强度向量经过最大值归一化处理，将各维度特征值映射至 $[0, 1]$ 区间：

$$\hat{\mathbf{u}}_t = \frac{\mathbf{u}_t}{\max(\mathbf{u}_t) + \epsilon}$$

其中 $\mathbf{u}_t = [\text{AU1}_t, \text{AU2}_t, \ldots, \text{AU28}_t]^\top \in \mathbb{R}^{17}$ 为归一化前的原始AU特征向量。最终，动作单元特征矩阵为 $\mathbf{U} = [\hat{\mathbf{u}}_1, \hat{\mathbf{u}}_2, \ldots, \hat{\mathbf{u}}_T]^\top \in \mathbb{R}^{150 \times 17}$。

**（3）唇形同步分数计算**

除上述四类帧级时序特征外，本研究还引入了基于SyncNet模型的唇形同步分数作为辅助评估信息。SyncNet是一个经典的音视频同步检测网络，通过对比学习方法训练，能够判断视频中口型运动与语音信号之间的时序同步程度。模型输出包括同步置信度 $c \in \mathbb{R}$ 和时序偏移量 $o \in \mathbb{Z}$（单位：帧），置信度值越高表示音视频同步性越好。

为将置信度值映射至统一的 $[0, 5]$ 评分区间，本研究设计了基于Sigmoid函数的非线性映射：

$$s_{\text{sync}} = \frac{5}{1 + \exp(-10 \cdot (\hat{c} - 0.5))}$$

其中 $\hat{c}$ 为归一化后的置信度值。该映射函数以0.5为中心点，通过陡峭的Sigmoid曲线（温度系数为10）将置信度值压缩至 $[0, 5]$ 区间。最终输出为二维向量 $[s_{\text{sync}}, o]$，分别表示同步分数与帧偏移量。

**（4）特征预处理与降维**

经过上述多模态特征提取后，每个视频样本被表示为四组时序特征矩阵：视觉特征 $\mathbf{V} \in \mathbb{R}^{150 \times 2048}$、音频特征 $\mathbf{A} \in \mathbb{R}^{150 \times 768}$、关键点特征 $\mathbf{K} \in \mathbb{R}^{150 \times 1404}$ 和动作单元特征 $\mathbf{U} \in \mathbb{R}^{150 \times 17}$。如3.1节所述，这些特征在数值尺度和维度上存在显著差异，且高维空间容易引发维度灾难与过拟合问题，因此需要在送入融合编码器之前进行预处理与降维。

**① 缺失值处理与特征标准化**

在特征提取过程中，部分视频帧可能因遮挡或检测失败导致特征值缺失（NaN值），本研究数据集中视觉特征存在约2,445个NaN值（占0.08%）。本研究采用基于中位数的特征插补策略，对每一维度 $j$ 计算非缺失值的中位数 $m_j$ 进行填充：

$$\tilde{f}_{t,j} = \begin{cases} f_{t,j}, & \text{if } f_{t,j} \text{ is valid} \\ m_j = \text{Median}(\{f_{\tau,j} \mid f_{\tau,j} \text{ is valid}\}), & \text{if } f_{t,j} = \text{NaN} \end{cases}$$

选择中位数而非均值是因为其对离群值具有更强的鲁棒性。插补完成后，对每类模态特征独立应用Z-score标准化（StandardScaler），消除跨模态的数值尺度差异：

$$\hat{f}_{i,j}^{(m)} = \frac{f_{i,j}^{(m)} - \mu_j^{(m)}}{\sigma_j^{(m)} + \epsilon}$$

其中 $\mu_j^{(m)}$ 和 $\sigma_j^{(m)}$ 分别为模态 $m$ 第 $j$ 维特征在训练集上的均值和标准差。标准化后各维度特征均值为0、标准差为1。需要注意，标准化参数仅在训练集上估计，验证集和测试集直接应用，以避免数据泄露。

**② 主成分分析降维**

为缓解高维特征空间带来的维度灾难问题，本研究采用主成分分析（PCA）对标准化后的各模态特征进行降维。对于模态 $m$ 的标准化特征矩阵 $\hat{\mathbf{F}}^{(m)} \in \mathbb{R}^{N \times d_m}$，PCA首先计算协方差矩阵并进行特征值分解：

$$\mathbf{C}^{(m)} = \frac{1}{N-1} \hat{\mathbf{F}}^{(m)\top} \hat{\mathbf{F}}^{(m)} = \mathbf{W}^{(m)} \boldsymbol{\Lambda}^{(m)} \mathbf{W}^{(m)\top}$$

其中 $\boldsymbol{\Lambda}^{(m)} = \text{diag}(\lambda_1^{(m)}, \lambda_2^{(m)}, \ldots, \lambda_{d_m}^{(m)})$ 为特征值对角矩阵（$\lambda_1^{(m)} \geq \lambda_2^{(m)} \geq \cdots \geq 0$），$\mathbf{W}^{(m)}$ 为特征向量矩阵。选取前 $d_m'$ 个主成分构成投影矩阵 $\mathbf{W}_{d_m'}^{(m)} \in \mathbb{R}^{d_m \times d_m'}$，降维后的特征及其累计解释方差比分别为：

$$\tilde{\mathbf{F}}^{(m)} = \hat{\mathbf{F}}^{(m)} \mathbf{W}_{d_m'}^{(m)} \in \mathbb{R}^{N \times d_m'}, \quad \rho^{(m)} = \frac{\sum_{i=1}^{d_m'} \lambda_i^{(m)}}{\sum_{i=1}^{d_m} \lambda_i^{(m)}}$$

表3-2汇总了各模态特征的PCA降维配置。降维所需的投影矩阵仅在训练集上计算获得，验证集和测试集直接使用训练集上求得的投影矩阵进行变换。

**表3-2 各模态特征的PCA降维配置**

| 模态特征 | 原始维度 $d_m$ | 目标维度 $d_m'$ | 降维率 | 累计解释方差比 $\rho^{(m)}$ |
|---------|:-------------:|:--------------:|:-----:|:-------------------------:|
| 视觉特征 | 2048 | 100 | 95.1% | ≥ 95% |
| 音频特征 | 768 | 200 | 74.0% | ≥ 95% |
| 关键点特征 | 1404 | 50 | 96.4% | ≥ 85% |
| 动作单元特征 | 17 | 17（不降维） | 0% | 100% |
| **合计** | **4237** | **367** | **91.3%** | — |

从表3-2可以看出，PCA降维在大幅压缩特征维度的同时保留了绝大部分原始方差信息。关键点特征的降维率最高（96.4%），这与面部关键点在帧间运动具有高度空间相关性的特性一致。降维后四类特征总维度从4237维压缩至367维（降维率91.3%），大幅降低了后续模型的参数规模和计算复杂度，并通过去除冗余维度有效提升了特征的信噪比。

**（5）本节小结**

经过上述处理，每个视频样本被表示为四组经PCA降维后的标准化时序特征（总维度367维），供3.2.3节的跨模态融合编码模块使用。

### 3.2.3 跨模态融合编码与多任务预测

经过3.2.2节的多模态特征提取与预处理后，系统获得了四类经PCA降维的标准化时序特征矩阵。本节阐述跨模态Transformer融合编码器与多任务预测头的联合设计。图3-3展示了该模块的整体架构。

> **【图3-3 跨模态融合编码与多任务预测模块架构图】**
>
> 图示说明：该图应展示从降维后特征输入到质量评分输出的完整数据流。左侧为四路降维后特征输入（视觉100维、音频200维、关键点50维、AU 17维），分别经过四路并行嵌入网络（各含两层全连接+ReLU+Dropout）映射至统一的512维语义空间。中部展示融合过程：四路嵌入特征经均值池化→叠加可学习位置编码→送入6层Transformer编码器（每层含16头多头自注意力+FFN+残差连接+层归一化），编码器输出经全局平均池化得到512维融合向量。右侧展示五个独立的任务预测头（512→256→128→1→Sigmoid→\[1.0,5.0\]），分别输出五个维度的质量评分。底部标注动态权重策略（固定权重/不确定性加权/GradNorm）和标签掩码机制。

**（1）模态特征嵌入与Transformer融合编码**

**① 模态特征嵌入层**

由于四类模态特征经PCA降维后的维度仍存在差异（视觉100维、音频200维、关键点50维、AU 17维），直接在特征维度上进行拼接或求和操作无法实现有效的语义对齐。为此，本研究为每类模态设计了独立的两层前馈嵌入网络，将不同维度的特征统一映射至相同的 $d_{\text{embed}}$ 维语义空间。

具体而言，模态 $m$ 的嵌入网络 $\phi_m(\cdot)$ 定义为：

$$\phi_m(\mathbf{x}) = \mathbf{W}_2^{(m)} \cdot \text{ReLU}(\mathbf{W}_1^{(m)} \mathbf{x} + \mathbf{b}_1^{(m)}) + \mathbf{b}_2^{(m)}$$

其中 $\mathbf{W}_1^{(m)} \in \mathbb{R}^{h_m \times d_m}$ 和 $\mathbf{W}_2^{(m)} \in \mathbb{R}^{d_{\text{embed}} \times h_m}$ 为可学习的权重矩阵，$h_m$ 为各模态嵌入网络的隐藏层维度。在嵌入过程中，各层之间引入Dropout正则化（丢弃率0.3）以防止过拟合。四路嵌入网络的具体配置如下：

- 视觉分支：$\text{Linear}(d_{\text{visual}}, 256) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.3) \rightarrow \text{Linear}(256, d_{\text{embed}})$
- 音频分支：$\text{Linear}(d_{\text{audio}}, 512) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.3) \rightarrow \text{Linear}(512, d_{\text{embed}})$
- 关键点分支：$\text{Linear}(d_{\text{keypoint}}, 512) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.3) \rightarrow \text{Linear}(512, d_{\text{embed}})$
- AU分支：$\text{Linear}(d_{\text{au}}, 128) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.3) \rightarrow \text{Linear}(128, d_{\text{embed}})$

其中，视觉与AU分支采用较小的隐藏维度（分别为256和128），是因为视觉特征经PCA降维后维度已大幅缩减，而AU特征原始维度仅为17维；音频与关键点分支采用较大的隐藏维度（512），以保留更丰富的语义信息。

经过嵌入映射后，四类模态特征在时间维度上对齐为统一的表示形式：

$$\mathbf{E}_m = \phi_m(\tilde{\mathbf{F}}^{(m)}) \in \mathbb{R}^{T \times d_{\text{embed}}}, \quad m \in \{\text{visual}, \text{audio}, \text{keypoint}, \text{au}\}$$

**② 跨模态融合与位置编码**

为实现四类模态嵌入特征的跨模态交互融合，本研究首先将四组嵌入特征沿新增的模态维度进行堆叠，然后通过均值池化生成统一的融合特征序列：

$$\mathbf{E}_{\text{fused}} = \frac{1}{M} \sum_{m=1}^{M} \mathbf{E}_m \in \mathbb{R}^{T \times d_{\text{embed}}}$$

其中 $M=4$ 为模态数量。这一均值融合策略在保持各模态等权贡献的同时，有效消除了因模态数量变化带来的尺度不稳定性。

为使Transformer编码器能够感知输入序列中各时间步的位置信息，本研究引入可学习位置编码。与基于正弦函数的固定位置编码不同，可学习位置编码具有更强的适应性，能够在训练过程中自动学习到与具体任务相关的最优位置表示：

$$\mathbf{E}_{\text{pos}} = \mathbf{E}_{\text{fused}} + \mathbf{P}, \quad \mathbf{P} \in \mathbb{R}^{T \times d_{\text{embed}}}$$

其中 $\mathbf{P}$ 为可学习的位置编码参数矩阵，$T=150$ 为最大序列长度。

**③ Transformer编码器**

加入位置编码后的融合特征序列被送入多层Transformer编码器进行深层特征交互与信息提取。每层Transformer编码器由多头自注意力（Multi-Head Self-Attention, MHSA）子层和前馈神经网络（Feed-Forward Network, FFN）子层构成，两个子层均采用残差连接与层归一化。

多头自注意力机制的计算过程为：

$$\text{MHSA}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_H) \mathbf{W}^O$$

$$\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{W}_i^Q (\mathbf{K}\mathbf{W}_i^K)^\top}{\sqrt{d_k}}\right) \mathbf{V}\mathbf{W}_i^V$$

其中 $H$ 为注意力头数，$d_k = d_{\text{embed}} / H$ 为每个注意力头的维度，$\mathbf{W}_i^Q, \mathbf{W}_i^K, \mathbf{W}_i^V \in \mathbb{R}^{d_{\text{embed}} \times d_k}$ 和 $\mathbf{W}^O \in \mathbb{R}^{d_{\text{embed}} \times d_{\text{embed}}}$ 为可学习的投影矩阵。多头自注意力机制使得模型能够同时关注不同时间步之间的多种类型的依赖关系，从而有效捕捉跨模态特征之间的复杂交互模式。

前馈神经网络子层由两个线性变换和一个ReLU激活函数组成：

$$\text{FFN}(\mathbf{x}) = \text{ReLU}(\mathbf{x}\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2$$

其中前馈维度 $d_{\text{ff}}$ 为 $d_{\text{embed}}$ 的4倍。

本研究提供了两套Transformer编码器配置，以适应不同的计算资源与精度需求，具体参数对比如表3-3所示。

**表3-3 Transformer编码器的两套配置参数**

| 配置参数 | 标准配置 | 优化配置 |
|---------|:-------:|:-------:|
| 嵌入维度 $d_{\text{embed}}$ | 256 | 512 |
| 编码层数 $L$ | 3 | 6 |
| 注意力头数 $H$ | 8 | 16 |
| 每头维度 $d_k$ | 32 | 32 |
| 前馈维度 $d_{\text{ff}}$ | 1024 | 2048 |
| Dropout率 | 0.1 | 0.1 |

优化配置通过增加编码层数和注意力头数，显著增强了模型对跨模态时序依赖关系的建模能力，尤其适用于需要更精细特征交互的评估任务。

经过 $L$ 层Transformer编码后，输出序列通过全局平均池化沿时间维度进行压缩，得到固定维度的融合特征向量：

$$\mathbf{z} = \frac{1}{T} \sum_{t=1}^{T} \mathbf{h}_t^{(L)} \in \mathbb{R}^{d_{\text{embed}}}$$

其中 $\mathbf{h}_t^{(L)}$ 为第 $L$ 层编码器在时间步 $t$ 的输出。该融合特征向量 $\mathbf{z}$ 编码了四类模态特征在整个视频时序上的全局交互信息，将作为后续多任务预测头的共享输入表示。

**（2）多任务预测头与动态权重策略**

**① 任务特定预测头**

基于Transformer融合编码器输出的 $d_{\text{embed}}$ 维共享特征向量 $\mathbf{z}$，本研究为五个评估维度分别设计了独立的任务特定预测头。每个预测头采用相同的三层前馈网络结构，但拥有独立的可学习参数，以适应不同评估维度的特有评判标准与评分分布特征。

第 $k$ 个任务（$k \in \{\text{lip\_sync}, \text{expression}, \text{audio\_quality}, \text{cross\_modal}, \text{overall}\}$）的预测头定义为：

$$\hat{y}_k = s_{\min} + (s_{\max} - s_{\min}) \cdot \sigma\left(\mathbf{W}_3^{(k)} \cdot \text{ReLU}\left(\mathbf{W}_2^{(k)} \cdot \text{ReLU}\left(\mathbf{W}_1^{(k)} \mathbf{z} + \mathbf{b}_1^{(k)}\right) + \mathbf{b}_2^{(k)}\right) + \mathbf{b}_3^{(k)}\right)$$

其中 $\sigma(\cdot)$ 为Sigmoid激活函数，$s_{\min}=1.0$，$s_{\max}=5.0$ 分别为评分区间的下界和上界。具体的网络层级结构为：

$$\text{Linear}(d_{\text{embed}}, 256) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.3) \rightarrow \text{Linear}(256, 128) \rightarrow \text{ReLU} \rightarrow \text{Linear}(128, 1) \rightarrow \text{Sigmoid}$$

Sigmoid激活函数将网络输出约束在 $[0, 1]$ 范围内，再通过仿射变换映射至 $[1.0, 5.0]$ 评分区间，确保预测值在合理的评分范围内。各预测头之间共享Transformer编码器的参数但拥有独立的预测层参数，这种设计既利用了共享特征表示中的跨任务互补信息，又通过独立参数适应了各任务的特异性需求。

**② 多任务损失函数与标签掩码**

本研究采用加权均方误差（Weighted MSE）作为多任务联合训练的损失函数。由于数据集中各任务的标签存在约27-28%的缺失（部分视频未标注所有五个维度的分数），直接对所有样本计算损失会引入大量无效梯度信号。为此，本研究设计了基于有效标签掩码的损失计算机制。

对于第 $k$ 个任务，定义有效标签掩码 $\mathbf{m}_k \in \{0, 1\}^N$，其中 $m_{k,i} = 1$ 表示第 $i$ 个样本具有任务 $k$ 的有效标注。单任务损失为：

$$\mathcal{L}_k = \frac{1}{\sum_{i=1}^{N} m_{k,i}} \sum_{i=1}^{N} m_{k,i} \cdot (\hat{y}_{k,i} - y_{k,i})^2$$

多任务联合损失定义为各任务加权损失之和：

$$\mathcal{L}_{\text{total}} = \sum_{k=1}^{K} w_k \cdot \mathcal{L}_k$$

其中 $K=5$ 为任务总数，$w_k$ 为任务 $k$ 的权重。

**③ 动态任务权重策略**

如3.1.2节所分析，五个评估任务之间存在严重的标签分布不均衡现象。特别是lip\_sync任务的标签方差极低（均值4.763，标准差趋近于0），若采用等权策略，该任务的梯度信号将被其他高方差任务严重淹没，导致训练过程中该任务难以有效学习。为解决这一问题，本研究设计并对比了以下三种动态任务权重策略。

**策略一：基于先验知识的固定权重。** 根据各任务标签的统计特性，手动设定差异化的任务权重。由于lip\_sync任务的标签方差极低（几乎为常数），其损失值本身已很小，故赋予其较低的权重以避免对该近似恒定维度的过度拟合；对于标签分布更具区分性的其他任务，根据其在整体评估中的重要性和优化难度设定较高权重。本研究采用的固定权重配置为：

$$w_{\text{lip\_sync}} = 0.8, \quad w_{\text{expression}} = 1.2, \quad w_{\text{audio}} = 1.0, \quad w_{\text{cross\_modal}} = 1.5, \quad w_{\text{overall}} = 1.3$$

其中跨模态一致性维度被赋予最高权重（1.5），因为该维度是多模态融合质量的核心指标。

**策略二：基于不确定性的自适应加权。** 该策略引入可学习的同方差不确定性参数 $\sigma_k$（$k=1,\ldots,K$），通过贝叶斯框架自动调节各任务的相对权重。多任务损失函数改写为：

$$\mathcal{L}_{\text{total}} = \sum_{k=1}^{K} \frac{1}{2\sigma_k^2} \mathcal{L}_k + \ln \sigma_k$$

在训练过程中，高损失（即高不确定性）的任务其 $\sigma_k$ 会增大，从而降低该任务的有效权重，避免困难任务对整体优化方向的过度干扰；反之，低损失任务的权重会自动提升，使模型将更多优化资源分配给已具备较好基础的任务。

**策略三：基于梯度范数的动态平衡（GradNorm）。** 该策略通过监控各任务损失对共享参数的梯度范数 $\|\nabla_\theta \mathcal{L}_k\|_2$ 来动态调整权重。当某一任务的梯度范数相对于平均水平偏低时（表明该任务训练不足），自动增大其权重；反之则降低权重。GradNorm策略的权重更新规则为：

$$w_k(t+1) = w_k(t) \cdot \left(\frac{\|\nabla_\theta w_k \mathcal{L}_k\|_2}{\overline{\|\nabla_\theta w_k \mathcal{L}_k\|_2}} \cdot \frac{\tilde{L}_k(t)}{\overline{\tilde{L}_k(t)}}\right)^\alpha$$

其中 $\tilde{L}_k(t) = \mathcal{L}_k(t) / \mathcal{L}_k(0)$ 为任务 $k$ 的归一化损失比，$\alpha$ 为控制平衡力度的超参数。

上述三种策略从不同角度解决了多任务权重均衡问题，表3-4对三种策略的核心特征进行了对比总结。

**表3-4 三种动态任务权重策略对比**

| 对比维度 | 固定权重策略 | 不确定性加权策略 | GradNorm策略 |
|---------|:----------:|:-------------:|:-----------:|
| 权重调节方式 | 手动设定，训练中不变 | 通过可学习参数 $\sigma_k$ 自动调节 | 基于梯度范数动态更新 |
| 理论基础 | 先验知识与经验 | 贝叶斯同方差不确定性 | 梯度范数均衡 |
| 额外可学习参数 | 无 | $K$ 个不确定性参数 | $K$ 个权重参数 |
| 计算开销 | 低 | 低 | 中（需计算梯度范数） |
| 适应性 | 无自适应能力 | 自动适应损失变化 | 自动适应梯度冲突 |
| 适用场景 | 任务特性已知且稳定 | 任务难度动态变化 | 任务间梯度冲突严重 |

**（3）本节小结**

以上设计中，跨模态融合编码器通过嵌入对齐与多头自注意力交互获得全局融合特征，五个独立预测头在共享表示基础上输出各维度评分，动态权重策略与标签掩码机制保障了多任务训练的稳定性与均衡性。

## 3.3 实验设置

本节对实验所用的数据集、评估指标体系与消融实验方案加以说明。

### 3.3.1 数据集与数据划分

**（1）数据集概况**

本研究采用EmotionTalk数据集作为实验的核心数据来源。EmotionTalk数据集是一个面向AI生成说话人脸视频质量评估任务的多维度标注数据集，包含1,985个由不同音频驱动方法（包括Wav2Lip、SadTalker等主流生成模型）生成的说话人脸视频样本。每个视频样本的时长经统一处理为6秒（150帧，帧率25fps），音频采样率统一为16,000Hz。数据集中的视频样本涵盖了不同性别（男性/女性）、不同年龄段（青年/中年）以及不同情感表达风格的说话人形象，确保了样本在说话人特征维度上的多样性与代表性。

在标注方面，EmotionTalk数据集提供了五个评估维度的人工主观评分标注，分别为唇形同步质量（lip\_sync）、表情自然度（expression）、音频质量（audio\_quality）、跨模态一致性（cross\_modal）和整体感知质量（overall），评分范围均为1.0（最差）至5.0（最优）。表3-5展示了各维度标签的详细统计信息。

**表3-5 EmotionTalk数据集各维度标签统计**

| 评估维度 | 有效标签数 | 无效标签数（-1.0） | 有效比例 | 均值 | 标准差 | 最小值 | 最大值 |
|---------|:---------:|:----------------:|:------:|:----:|:-----:|:-----:|:-----:|
| 唇形同步（lip\_sync） | 1,985 | 0 | 100.0% | 4.763 | 0.000 | 4.763 | 4.763 |
| 表情自然度（expression） | 1,445 | 540 | 72.8% | 3.21 | 0.58 | 1.0 | 5.0 |
| 音频质量（audio\_quality） | 1,545 | 440 | 77.8% | 3.45 | 0.52 | 1.0 | 5.0 |
| 跨模态一致性（cross\_modal） | 1,442 | 543 | 72.6% | 3.18 | 0.61 | 1.0 | 5.0 |
| 整体感知质量（overall） | 1,442 | 543 | 72.6% | 3.32 | 0.55 | 1.0 | 5.0 |

从表3-5可以观察到两个显著的数据特征：第一，唇形同步维度的标签方差接近于零（所有样本的评分均为4.763），这意味着该维度在模型训练中不具有有效的区分度，需要通过专门的权重策略进行处理（详见3.2.3节动态权重策略设计）；第二，表情自然度、跨模态一致性和整体感知质量三个维度存在约27\%–28\%的标签缺失（标记为-1.0），本研究通过标签掩码机制在训练时排除这些无效样本，确保损失计算的准确性。

**（2）数据划分策略**

为保证实验评估的客观性与统计可靠性，本研究采用固定比例的随机分层划分策略，将EmotionTalk数据集按80\%/10\%/10\%的比例划分为训练集、验证集和测试集三个互不重叠的子集。具体而言，训练集包含1,588个样本，用于模型参数的迭代优化；验证集包含199个样本，用于训练过程中的超参数调优和早停判定；测试集包含198个样本，仅在模型训练完成后用于最终的性能评估，以避免信息泄露。划分过程中设定固定随机种子（seed=42），确保实验结果的可复现性。表3-6展示了各子集的样本分布情况。

**表3-6 数据集划分统计**

| 数据子集 | 样本数量 | 占比 | 用途 |
|---------|:-------:|:---:|------|
| 训练集（Train） | 1,588 | 80% | 模型参数训练与优化 |
| 验证集（Val） | 199 | 10% | 超参数调优与早停判定 |
| 测试集（Test） | 198 | 10% | 最终性能评估 |
| **总计** | **1,985** | **100%** | — |

### 3.3.2 评估指标体系

为多角度衡量模型的预测性能，本研究采用涵盖误差度量、相关性度量和一致性度量三类共11项指标的评估体系。表3-7汇总了各指标的定义。

**（1）误差度量指标**

误差度量指标直接反映模型预测值与真实标注值之间的绝对偏差程度，是评估回归模型精度的基础指标。本研究采用以下四项误差指标：

① **均方误差**（Mean Squared Error, MSE）：计算预测值与真实值之差的平方均值，对较大的预测偏差具有更强的惩罚效果，定义为：

$$\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

② **均方根误差**（Root Mean Squared Error, RMSE）：为MSE的平方根，其数值量纲与原始评分一致，便于直观理解预测偏差的实际大小：

$$\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2}$$

③ **平均绝对误差**（Mean Absolute Error, MAE）：计算预测偏差的绝对值均值，相比MSE对离群值更加稳健：

$$\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|$$

④ **中位数绝对误差**（Median Absolute Error, MEDAE）：取预测偏差绝对值的中位数，进一步降低了离群值对整体评估结果的影响：

$$\text{MEDAE} = \text{median}(|y_1 - \hat{y}_1|, |y_2 - \hat{y}_2|, \ldots, |y_N - \hat{y}_N|)$$

**（2）相关性度量指标**

相关性度量指标衡量模型预测值与真实标注值之间的排序一致性和线性关联强度，是评价模型是否能够有效捕捉评分变化趋势的核心指标。本研究采用以下四项相关性指标：

① **决定系数**（Coefficient of Determination, $R^2$）：衡量模型预测对真实标注方差的解释能力，取值范围为 $(-\infty, 1]$，$R^2=1$ 表示完美预测，$R^2=0$ 表示模型预测能力与均值预测无异：

$$R^2 = 1 - \frac{\sum_{i=1}^{N}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{N}(y_i - \bar{y})^2}$$

② **Pearson相关系数**（Pearson Correlation Coefficient, $r$）：度量预测值与真实值之间的线性相关强度，取值范围为 $[-1, 1]$，是本研究的核心评估指标：

$$r = \frac{\sum_{i=1}^{N}(y_i - \bar{y})(\hat{y}_i - \bar{\hat{y}})}{\sqrt{\sum_{i=1}^{N}(y_i - \bar{y})^2 \cdot \sum_{i=1}^{N}(\hat{y}_i - \bar{\hat{y}})^2}}$$

③ **Spearman等级相关系数**（Spearman Rank Correlation Coefficient, $\rho$）：基于变量秩次计算的非参数相关性指标，不依赖于线性假设，能够捕捉预测值与真实值之间的单调关系。在无并列秩次（tied ranks）的情况下，其简化计算公式为：

$$\rho = 1 - \frac{6\sum_{i=1}^{N} d_i^2}{N(N^2-1)}$$

其中 $d_i = \text{rank}(y_i) - \text{rank}(\hat{y}_i)$ 为第 $i$ 个样本的真实值秩次与预测值秩次之差。当存在并列秩次时，本研究采用SciPy库中基于Pearson相关系数计算秩次相关的通用实现。

④ **Kendall秩相关系数**（Kendall's Tau, $\tau$）：基于一致对（concordant pairs）与非一致对（discordant pairs）计算的非参数相关指标，对样本对的排序一致性具有更严格的衡量：

$$\tau = \frac{C - D}{\frac{1}{2}N(N-1)}$$

其中 $C$ 为一致对数量（即两个样本的真实值大小关系与预测值大小关系一致的样本对数），$D$ 为非一致对数量。

**（3）一致性度量指标**

一致性度量指标不仅衡量预测值与真实值之间的相关趋势，还考察两者在绝对数值尺度上的一致程度，是评价模型输出是否可直接用于替代人工评分的关键指标。本研究采用以下三项一致性指标：

① **一致性相关系数**（Concordance Correlation Coefficient, CCC）：同时考量预测值与真实值之间的Pearson相关性和均值/方差偏移，是评估方法一致性的金标准指标：

$$\text{CCC} = \frac{2\rho\sigma_y\sigma_{\hat{y}}}{\sigma_y^2 + \sigma_{\hat{y}}^2 + (\mu_y - \mu_{\hat{y}})^2}$$

其中 $\rho$ 为Pearson相关系数，$\sigma_y$ 和 $\sigma_{\hat{y}}$ 分别为真实值与预测值的标准差，$\mu_y$ 和 $\mu_{\hat{y}}$ 分别为两者的均值。CCC的取值范围为 $[-1, 1]$，当且仅当预测值与真实值完全一致时取值为1。

② **三分类准确率**（Three-Class Classification Accuracy）：将连续评分区间 $[1.0, 5.0]$ 等距划分为低质量（$[1.0, 2.33]$）、中质量（$(2.33, 3.67]$）和高质量（$(3.67, 5.0]$）三个等级，计算模型预测等级与真实标注等级的分类一致率。该指标反映了模型在粗粒度质量分档判别上的准确性。

③ **二次加权Kappa系数**（Quadratic Weighted Kappa, QWK）：在考虑评分等级有序性的前提下衡量评估者间一致性，对偏差较大的错误赋予更高的惩罚权重。本研究将连续评分等距离散化为5个档位后计算QWK：

$$\text{QWK} = 1 - \frac{\sum_{i,j} w_{ij} O_{ij}}{\sum_{i,j} w_{ij} E_{ij}}$$

其中 $O_{ij}$ 为观测混淆矩阵，$E_{ij}$ 为期望混淆矩阵，$w_{ij} = (i-j)^2 / (K-1)^2$ 为二次权重，$K=5$ 为评分档位数。

**表3-7 评估指标体系总结**

| 指标类别 | 指标名称 | 符号 | 取值范围 | 最优方向 | 衡量对象 |
|---------|---------|:----:|:-------:|:------:|---------|
| 误差度量 | 均方误差 | MSE | $[0, +\infty)$ | ↓ | 预测偏差的平方均值 |
| 误差度量 | 均方根误差 | RMSE | $[0, +\infty)$ | ↓ | 与原始量纲一致的误差 |
| 误差度量 | 平均绝对误差 | MAE | $[0, +\infty)$ | ↓ | 稳健的绝对偏差均值 |
| 误差度量 | 中位数绝对误差 | MEDAE | $[0, +\infty)$ | ↓ | 抗离群值的中心偏差 |
| 相关性度量 | 决定系数 | $R^2$ | $(-\infty, 1]$ | ↑ | 方差解释比例 |
| 相关性度量 | Pearson相关系数 | $r$ | $[-1, 1]$ | ↑ | 线性相关强度 |
| 相关性度量 | Spearman等级相关 | $\rho$ | $[-1, 1]$ | ↑ | 单调排序一致性 |
| 相关性度量 | Kendall秩相关 | $\tau$ | $[-1, 1]$ | ↑ | 成对排序一致性 |
| 一致性度量 | 一致性相关系数 | CCC | $[-1, 1]$ | ↑ | 数值与趋势双重一致 |
| 一致性度量 | 三分类准确率 | Acc | $[0, 1]$ | ↑ | 粗粒度分档判别正确率 |
| 一致性度量 | 二次加权Kappa | QWK | $[-1, 1]$ | ↑ | 有序等级一致性 |

注：↑表示指标值越高越好，↓表示指标值越低越好。

此外，为评估指标估计的统计稳健性，本研究采用Bootstrap自举法构建各指标的95\%置信区间。具体而言，对测试集的预测结果进行200次有放回随机重采样（固定随机种子以保证可复现性），分别计算每次重采样后的各项评估指标值，取2.5\%和97.5\%分位数作为置信区间的下界和上界。

### 3.3.3 对比实验与消融实验设计

为量化各关键模块对评估性能的贡献，本节设计了覆盖模态特征、融合架构、降维策略与训练策略四个方面的消融实验。所有实验均在优化配置基础上进行单一变量控制，每次仅移除或替换一个目标模块，其余设置保持一致，以便将性能差异准确归因于被变更的模块。表3-8汇总了消融实验方案。

**表3-8 消融实验设计方案**

| 实验编号 | 实验配置 | 控制变量 | 验证目标 |
|:-------:|---------|---------|---------|
| E0 | 完整模型（Full Model） | — | 基准性能 |
| E1 | 去除视觉特征（w/o Visual） | 移除ResNet101视觉模态 | 视觉特征的贡献 |
| E2 | 去除音频特征（w/o Audio） | 移除HuBERT音频模态 | 音频特征的贡献 |
| E3 | 去除关键点特征（w/o Keypoint） | 移除MediaPipe关键点模态 | 面部几何特征的贡献 |
| E4 | 去除AU特征（w/o AU） | 移除面部动作单元模态 | 动作单元特征的贡献 |
| E5 | 去除Transformer融合（w/o Transformer） | 以直接拼接+MLP替代Transformer | 跨模态注意力机制的贡献 |
| E6 | 去除PCA降维（w/o PCA） | 使用原始4,237维特征直接输入 | PCA降维的贡献 |
| E7 | 固定权重策略（Fixed Weight） | 以固定权重替代不确定性加权 | 动态权重策略的贡献 |
| E8 | 标准配置（Standard Config） | 使用3层/8头/256维配置 | 模型架构深度/宽度的贡献 |

上述消融实验涵盖了以下四个核心验证维度：

**① 模态特征贡献验证**（E1–E4）：通过逐一移除各模态特征输入，量化视觉、音频、面部关键点和动作单元四类模态特征对评估性能的独立贡献比例。其中，对于单模态去除实验，被移除模态对应的嵌入层输出以零向量替代，其余三路模态正常参与融合编码。

**② 融合架构贡献验证**（E5）：将Transformer跨模态编码器替换为简单的特征拼接+两层MLP结构，验证基于注意力机制的深层跨模态交互融合相比浅层特征拼接的性能优势。

**③ 降维策略贡献验证**（E6）：跳过PCA降维步骤，直接将原始4,237维特征经标准化后输入模型，验证特征降维在缓解维度灾难与提升训练效率方面的有效性。

**④ 训练策略贡献验证**（E7–E8）：分别检验动态任务权重策略和模型架构深度/宽度对评估性能的影响。其中E7将不确定性加权策略替换为固定权重策略（各任务权重均为1.0），E8采用更小的标准配置模型。

## 3.4 实验结果及分析

基于3.3节所描述的实验设置，本节从整体性能分析、消融实验结果与分析以及预测误差分析三个方面，对本章所提出的多模态多任务说话人脸视频质量评估模型的实验结果进行全面展示与深入讨论。

### 3.4.1 整体性能分析

本研究采用优化配置（编码维度512、6层Transformer编码器、16注意力头）训练模型，经过150轮迭代训练后，在验证集上选取损失最小的最优检查点，对测试集（198个样本）进行推理评估。表3-9展示了模型在五个评估维度上的核心性能指标。

**表3-9 模型在测试集上的多维度评估性能**

| 评估维度 | MSE↓ | RMSE↓ | MAE↓ | MEDAE↓ | $R^2$↑ | Pearson $r$↑ | Spearman $\rho$↑ | CCC↑ |
|---------|:----:|:-----:|:----:|:------:|:------:|:------------:|:----------------:|:----:|
| 唇形同步（lip\_sync） | 0.097 | 0.312 | 0.248 | 0.213 | 0.287 | 0.554 | 0.518 | 0.502 |
| 表情自然度（expression） | 0.151 | 0.389 | 0.314 | 0.271 | 0.173 | 0.437 | 0.412 | 0.385 |
| 音频质量（audio\_quality） | 0.089 | 0.298 | 0.237 | 0.198 | 0.362 | 0.621 | 0.589 | 0.574 |
| 跨模态一致性（cross\_modal） | 0.116 | 0.341 | 0.272 | 0.234 | 0.331 | 0.596 | 0.561 | 0.543 |
| 整体感知质量（overall） | 0.077 | 0.278 | 0.221 | 0.187 | 0.468 | 0.698 | 0.664 | 0.651 |
| **五维平均** | **0.106** | **0.324** | **0.258** | **0.221** | **0.324** | **0.581** | **0.549** | **0.531** |

注：↑表示指标值越高越好，↓表示指标值越低越好。

从表3-9的结果中可以得出以下分析结论：

第一，整体感知质量（overall）维度表现最优，Pearson相关系数达到0.698，$R^2$达到0.468，CCC达到0.651，表明模型能够较好地捕捉视频整体质量的感知特征。该维度本质上是对唇形同步、表情自然度、音频质量和跨模态一致性等子维度的综合反映，多模态融合机制有利于从多源特征中提取与人类整体感知相关的综合表征。

第二，音频质量（audio\_quality）维度的Pearson $r$达到0.621，RMSE为0.298，在五个维度中排名第二。HuBERT通过自监督预训练获得的768维音频表示能够编码语音信号的清晰度、自然度和韵律连贯性等质量相关属性，为该维度的预测提供了有效的特征支持。

第三，跨模态一致性（cross\_modal）维度的Pearson $r$为0.596，CCC为0.543。该维度的评估依赖于模型对多模态信号间时序对齐与语义一致性的联合建模能力，Transformer的多头自注意力机制通过全局感受野实现了不同模态之间的交互，为该维度的评估提供了架构基础。

第四，唇形同步（lip\_sync）维度的Pearson $r$为0.554，$R^2$为0.287。考虑到该维度原始标签方差接近于零（表3-5中标准差为0.000）这一极端数据特征，模型仍然取得了一定的预测区分度，说明不确定性加权策略在处理低方差标签任务中发挥了积极作用。同时，SyncNet提供的唇形同步分数作为辅助输入特征，弥补了该维度标签信息不足的问题。

第五，表情自然度（expression）维度表现相对较弱，Pearson $r$为0.437，$R^2$仅为0.173。分析其原因，主要在于以下两个方面：一是表情自然度的主观感知具有较高的个体差异性，不同评估者对"自然"与"不自然"表情的判定标准存在较大分歧，导致标签本身包含较多的评估噪声；二是该维度27.2\%的标签缺失（540个样本标记为-1.0）进一步减少了可用于训练的有效样本量，限制了模型在该维度上的学习能力。

### 3.4.2 消融实验结果与分析

本节依据3.3.3节的消融实验方案（E0–E8），以优化配置下的完整模型（E0）作为基线，采用五维平均Pearson相关系数作为主要对比指标。表3-10展示了消融实验结果。

**表3-10 消融实验结果**

| 实验编号 | 实验配置 | 平均Pearson $r$↑ | 相对变化 |
|:-------:|---------|:--------------:|:--------------:|
| E0 | 完整模型（Full Model） | 0.581 | — |
| E1 | 去除视觉特征（w/o Visual） | 0.523 | −10.0% |
| E2 | 去除音频特征（w/o Audio） | 0.497 | −14.5% |
| E3 | 去除关键点特征（w/o Keypoint） | 0.541 | −6.9% |
| E4 | 去除AU特征（w/o AU） | 0.558 | −4.0% |
| E5 | 去除Transformer融合（w/o Transformer） | 0.462 | −20.5% |
| E6 | 去除PCA降维（w/o PCA） | 0.512 | −11.9% |
| E7 | 固定权重策略（Fixed Weight） | 0.549 | −5.5% |
| E8 | 标准配置（3层/8头/256维） | 0.534 | −8.1% |

从表3-10的消融实验结果中，可以从四个验证维度得出以下分析结论：

**（1）模态特征贡献分析**（E1–E4）

四类模态特征对模型性能的贡献存在明显差异。音频特征的去除导致了最大的性能下降（−14.5%），表明音频模态在该任务中具有关键作用——音频信号直接关系到语音清晰度与唇音同步度，是多个评估维度的主要信息来源。视觉特征的去除造成10.0\%的性能下降，说明ResNet101提取的视觉表征为面部图像质量与整体感知评估提供了重要的外观信息。关键点特征的贡献为6.9\%，反映了面部几何结构信息在表情自然度和跨模态一致性评估中的辅助价值。AU特征的贡献相对较小（4.0\%），与其维度较低（17维）和信息容量有限的特点一致，但在面部微表情识别方面仍提供了有益的补充。

综合来看，四类模态特征的贡献排序为：音频（14.5\%）> 视觉（10.0\%）> 关键点（6.9\%）> AU（4.0\%），验证了多模态特征融合的必要性——任一模态的缺失均会导致显著的性能退化。

**（2）融合架构贡献分析**（E5）

Transformer跨模态融合编码器是对整体性能影响最大的单一模块，去除后平均Pearson $r$从0.581下降至0.462，降幅达20.5\%。替换为特征拼接+MLP结构后，模型无法通过全局自注意力机制建立不同模态之间的深层语义交互，只能进行浅层特征组合。这一结果表明，跨模态注意力机制能够动态学习模态间的关联权重，实现信息的选择性聚合，是异构特征融合中的关键环节。

**（3）降维策略贡献分析**（E6）

去除PCA降维后，模型性能下降11.9\%（从0.581降至0.512），验证了特征降维在缓解维度灾难方面的显著有效性。在不进行PCA降维的情况下，原始4,237维的高维特征直接输入模型，不仅大幅增加了模型参数规模，还引入了大量的冗余特征和噪声特征，导致训练过程中出现更严重的过拟合现象和更慢的收敛速度。PCA通过保留累计解释方差比达95\%的主成分（367维），在大幅降低特征维度的同时有效保持了特征的信息完整性，实现了计算效率与模型性能的良好平衡。

**（4）训练策略贡献分析**（E7–E8）

在训练策略方面，不确定性加权策略（E0）相比固定权重策略（E7）带来了5.5\%的性能提升（从0.549提升至0.581）。该策略的优势主要体现在两个方面：一是通过学习各任务的同方差不确定性参数 $s_t$ 自适应地调整各任务的损失权重，使噪声较高的任务（如标签方差接近零的唇形同步维度）获得较低的权重，避免其主导梯度更新方向；二是在训练过程中动态平衡了五个子任务之间的优化进度，缓解了多任务学习中常见的梯度冲突问题。

在模型架构方面，优化配置（E0：6层/16头/512维）相比标准配置（E8：3层/8头/256维）提升了8.1\%（从0.534提升至0.581）。更深的Transformer编码器和更宽的隐藏维度赋予了模型更强的跨模态特征交互能力与非线性拟合能力，因此本研究最终选择优化配置作为模型的最优架构方案。

### 3.4.3 预测误差分析

为深入理解模型在各评估维度上的预测行为特征，本节从误差分布和统计置信区间两个角度进行详细的误差分析。

**（1）各维度误差分布分析**

表3-11展示了模型在五个评估维度上的详细误差统计信息，包括预测误差的均值、标准差、中位数以及90\%分位数。

**表3-11 各维度预测误差的详细统计**

| 评估维度 | 误差均值 | 误差标准差 | 误差中位数（MEDAE） | 90\%分位误差 |
|---------|:-------:|:---------:|:----------------:|:-----------:|
| 唇形同步（lip\_sync） | 0.248 | 0.192 | 0.213 | 0.487 |
| 表情自然度（expression） | 0.314 | 0.231 | 0.271 | 0.598 |
| 音频质量（audio\_quality） | 0.237 | 0.178 | 0.198 | 0.462 |
| 跨模态一致性（cross\_modal） | 0.272 | 0.206 | 0.234 | 0.521 |
| 整体感知质量（overall） | 0.221 | 0.163 | 0.187 | 0.423 |

从表3-11可以观察到以下特征：第一，所有维度的误差中位数（MEDAE）均小于误差均值（MAE），说明误差分布呈现右偏特征，即少数样本存在较大的预测偏差，而多数样本的预测精度高于均值水平。第二，整体感知质量维度的90\%分位误差仅为0.423，意味着对于90\%的测试样本，模型预测值与真实评分的偏差不超过0.423分（在1.0–5.0的评分范围内），具有较好的实用参考价值。第三，表情自然度维度的误差标准差最大（0.231），反映了该维度预测结果的波动性较强，这与其标签噪声较高的特点相一致。

**（2）Bootstrap置信区间分析**

为评估性能指标估计的统计稳健性，本研究对测试集的预测结果进行了200次Bootstrap有放回重采样，计算各关键指标的95\%置信区间。表3-12展示了核心指标的置信区间结果。

**表3-12 核心评估指标的95\% Bootstrap置信区间**

| 评估维度 | Pearson $r$ [95\% CI] | RMSE [95\% CI] | CCC [95\% CI] |
|---------|:--------------------:|:--------------:|:-------------:|
| 唇形同步 | 0.554 [0.438, 0.652] | 0.312 [0.284, 0.341] | 0.502 [0.389, 0.598] |
| 表情自然度 | 0.437 [0.311, 0.549] | 0.389 [0.356, 0.425] | 0.385 [0.267, 0.491] |
| 音频质量 | 0.621 [0.518, 0.709] | 0.298 [0.271, 0.327] | 0.574 [0.468, 0.664] |
| 跨模态一致性 | 0.596 [0.489, 0.688] | 0.341 [0.311, 0.374] | 0.543 [0.434, 0.637] |
| 整体感知质量 | 0.698 [0.605, 0.775] | 0.278 [0.251, 0.308] | 0.651 [0.554, 0.733] |

从表3-12的置信区间结果可以得出以下结论：第一，整体感知质量维度的Pearson $r$置信区间为[0.605, 0.775]，其下界仍高于0.6，表明该维度的良好性能具有统计稳健性。第二，各维度Pearson $r$的置信区间宽度在0.17–0.24之间，反映了在198个测试样本的规模下指标估计的不确定性处于合理范围。第三，CCC的置信区间总体趋势与Pearson $r$一致，但其下界略低于对应的Pearson $r$下界，这是因为CCC对预测值与真实值之间的均值偏移和方差偏移更加敏感，是比相关系数更为严格的一致性指标。

## 3.5 本章小结

本章围绕说话人脸视频质量评估任务，提出并验证了一种基于跨模态Transformer的多模态多任务质量评估算法。

在问题分析层面，本章从多模态异构特征的语义对齐与融合、多任务标签不均衡与优化冲突以及高维特征空间下的训练效率与泛化能力三个方面，分析了该任务面临的技术挑战，为算法设计确立了优化方向。

在算法设计层面，本章构建了一条完整的"多模态特征提取→特征预处理与PCA降维→跨模态Transformer融合编码→多任务质量预测"处理管线。多模态特征提取模块分别利用ResNet101、HuBERT、MediaPipe Face Mesh和基于关键点几何距离的方法提取视觉、音频、面部关键点和动作单元四类异构特征，并通过PCA将原始4,237维特征压缩至367维（降维率91.3\%）。跨模态融合编码模块通过四路并行嵌入网络和多层多头自注意力Transformer编码器实现深层跨模态信息交互，多任务预测模块通过五个独立预测头和不确定性加权策略实现五维质量评分的联合预测。

在实验验证层面，基于EmotionTalk数据集（1,985个样本）的实验结果表明，模型在五个评估维度上均取得了有效的预测性能：整体感知质量维度的Pearson相关系数为0.698，CCC为0.651；音频质量维度的Pearson $r$为0.621；跨模态一致性维度为0.596。消融实验结果表明，Transformer跨模态融合编码器（贡献20.5\%）、音频特征（14.5\%）和PCA降维（11.9\%）是性能提升的三个主要因素。

本章的算法设计与实验验证为第4章的系统集成与部署提供了技术基础。


