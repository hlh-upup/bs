#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å¿«é€Ÿè¯„ä¼°æ¨¡å‹æ•ˆæœçš„è„šæœ¬
ç”¨äºå¯¹æ¯”åŸå§‹æ¨¡å‹å’Œä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½
"""

import os
import sys
import torch
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import pearsonr, spearmanr
import pickle

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils import load_config, get_device
from models import ImprovedMultiTaskTalkingFaceEvaluator, MultiTaskTalkingFaceEvaluator
from data import create_dataloaders_from_pkl

def load_model_results(experiment_dir, device):
    """åŠ è½½å®éªŒç»“æœ"""
    results = {}
    
    # åŠ è½½é…ç½®
    config_path = os.path.join(experiment_dir, "config.yaml")
    if os.path.exists(config_path):
        from utils import load_config
        results['config'] = load_config(config_path)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    best_model_path = os.path.join(experiment_dir, "checkpoints", "best_model.pth")
    if os.path.exists(best_model_path):
        checkpoint = torch.load(best_model_path, map_location=device)
        results['checkpoint'] = checkpoint
        results['best_epoch'] = checkpoint.get('epoch', 'unknown')
        results['best_val_loss'] = checkpoint.get('val_loss', float('inf'))
    
    # æŸ¥æ‰¾è¯„ä¼°ç»“æœ
    results_dir = os.path.join(experiment_dir, "results")
    if os.path.exists(results_dir):
        result_files = [f for f in os.listdir(results_dir) if f.endswith('.pkl') or f.endswith('.csv')]
        if result_files:
            results_path = os.path.join(results_dir, result_files[0])
            if results_path.endswith('.pkl'):
                with open(results_path, 'rb') as f:
                    results['metrics'] = pickle.load(f)
            else:
                results['metrics'] = pd.read_csv(results_path)
    
    return results

def evaluate_model(model, test_loader, device, config):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch in test_loader:
            # å‡†å¤‡æ•°æ®
            visual_features = batch['visual_features'].to(device)
            audio_features = batch['audio_features'].to(device)
            keypoints = batch.get('keypoints', torch.zeros(visual_features.size(0), 0).to(device))
            au_features = batch.get('au_features', torch.zeros(visual_features.size(0), 0).to(device))
            
            targets = {
                'lip_sync': batch['lip_sync_score'].to(device),
                'expression': batch['expression_score'].to(device),
                'audio_quality': batch['audio_quality_score'].to(device),
                'cross_modal': batch['cross_modal_score'].to(device),
                'overall': batch.get('overall_score', torch.zeros_like(batch['lip_sync_score'])).to(device)
            }
            
            # é¢„æµ‹
            if hasattr(model, 'compute_loss'):
                predictions, losses = model(
                    visual_features=visual_features,
                    audio_features=audio_features,
                    keypoint_features=keypoints,
                    au_features=au_features,
                    targets=targets
                )
            else:
                predictions = model(
                    visual_features=visual_features,
                    audio_features=audio_features,
                    keypoint_features=keypoints,
                    au_features=au_features
                )
            
            all_predictions.append(predictions)
            all_targets.append(targets)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = {}
    tasks = ['lip_sync', 'expression', 'audio_quality', 'cross_modal', 'overall']
    
    for task in tasks:
        task_preds = []
        task_targets = []
        
        for pred_dict, target_dict in zip(all_predictions, all_targets):
            if task in pred_dict and task in target_dict:
                task_preds.extend(pred_dict[task].cpu().numpy().flatten())
                task_targets.extend(target_dict[task].cpu().numpy().flatten())
        
        if len(task_preds) > 0:
            task_preds = np.array(task_preds)
            task_targets = np.array(task_targets)
            
            # è®¡ç®—å„ç§æŒ‡æ ‡
            mse = mean_squared_error(task_targets, task_preds)
            mae = mean_absolute_error(task_targets, task_preds)
            r2 = r2_score(task_targets, task_preds)
            pearson, _ = pearsonr(task_targets, task_preds)
            spearman, _ = spearmanr(task_targets, task_preds)
            
            metrics[task] = {
                'mse': mse,
                'mae': mae,
                'r2': r2,
                'pearson': pearson,
                'spearman': spearman,
                'predictions': task_preds,
                'targets': task_targets
            }
    
    return metrics

def create_comparison_report(original_metrics, improved_metrics, output_dir):
    """åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š"""
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    tasks = ['lip_sync', 'expression', 'audio_quality', 'cross_modal', 'overall']
    
    comparison_data = []
    for task in tasks:
        if task in original_metrics and task in improved_metrics:
            orig = original_metrics[task]
            impr = improved_metrics[task]
            
            # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
            mse_improvement = (orig['mse'] - impr['mse']) / orig['mse'] * 100
            mae_improvement = (orig['mae'] - impr['mae']) / orig['mae'] * 100
            r2_improvement = (impr['r2'] - orig['r2']) / max(abs(orig['r2']), 0.001) * 100
            pearson_improvement = (impr['pearson'] - orig['pearson']) / max(abs(orig['pearson']), 0.001) * 100
            
            comparison_data.append({
                'Task': task,
                'Orig_MSE': f"{orig['mse']:.4f}",
                'Impr_MSE': f"{impr['mse']:.4f}",
                'MSE_Improvement': f"{mse_improvement:.1f}%",
                'Orig_MAE': f"{orig['mae']:.4f}",
                'Impr_MAE': f"{impr['mae']:.4f}",
                'MAE_Improvement': f"{mae_improvement:.1f}%",
                'Orig_R2': f"{orig['r2']:.4f}",
                'Impr_R2': f"{impr['r2']:.4f}",
                'R2_Improvement': f"{r2_improvement:.1f}%",
                'Orig_Pearson': f"{orig['pearson']:.4f}",
                'Impr_Pearson': f"{impr['pearson']:.4f}",
                'Pearson_Improvement': f"{pearson_improvement:.1f}%",
            })
    
    df = pd.DataFrame(comparison_data)
    
    # ä¿å­˜å¯¹æ¯”è¡¨æ ¼
    comparison_path = os.path.join(output_dir, 'model_comparison.csv')
    df.to_csv(comparison_path, index=False)
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    create_visualization(original_metrics, improved_metrics, output_dir)
    
    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    report_path = os.path.join(output_dir, 'performance_comparison_report.txt')
    generate_text_report(df, original_metrics, improved_metrics, report_path)
    
    return df, comparison_path

def create_visualization(original_metrics, improved_metrics, output_dir):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    
    tasks = ['lip_sync', 'expression', 'audio_quality', 'cross_modal', 'overall']
    metrics_names = ['mse', 'mae', 'r2', 'pearson']
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.flatten()
    
    for idx, metric_name in enumerate(metrics_names):
        ax = axes[idx]
        
        orig_values = []
        impr_values = []
        task_labels = []
        
        for task in tasks:
            if task in original_metrics and task in improved_metrics:
                orig_values.append(original_metrics[task][metric_name])
                impr_values.append(improved_metrics[task][metric_name])
                task_labels.append(task.replace('_', ' ').title())
        
        x = np.arange(len(task_labels))
        width = 0.35
        
        bars1 = ax.bar(x - width/2, orig_values, width, label='Original', alpha=0.8)
        bars2 = ax.bar(x + width/2, impr_values, width, label='Improved', alpha=0.8)
        
        ax.set_xlabel('Tasks')
        ax.set_ylabel(metric_name.upper())
        ax.set_title(f'{metric_name.upper()} Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(task_labels, rotation=45)
        ax.legend()
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars1:
            height = bar.get_height()
            ax.annotate(f'{height:.3f}',
                       xy=(bar.get_x() + bar.get_width() / 2, height),
                       xytext=(0, 3), textcoords="offset points",
                       ha='center', va='bottom', fontsize=8)
        
        for bar in bars2:
            height = bar.get_height()
            ax.annotate(f'{height:.3f}',
                       xy=(bar.get_x() + bar.get_width() / 2, height),
                       xytext=(0, 3), textcoords="offset points",
                       ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
    plt.close()

def generate_text_report(df, original_metrics, improved_metrics, report_path):
    """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("æ¨¡å‹æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š\n")
        f.write("="*60 + "\n\n")
        
        # æ€»ä½“æ”¹è¿›æƒ…å†µ
        f.write("ğŸ“Š æ€»ä½“æ”¹è¿›æƒ…å†µ\n")
        f.write("-"*40 + "\n")
        
        avg_mse_improvement = df['MSE_Improvement'].str.replace('%', '').astype(float).mean()
        avg_mae_improvement = df['MAE_Improvement'].str.replace('%', '').astype(float).mean()
        avg_r2_improvement = df['R2_Improvement'].str.replace('%', '').astype(float).mean()
        avg_pearson_improvement = df['Pearson_Improvement'].str.replace('%', '').astype(float).mean()
        
        f.write(f"å¹³å‡MSEæ”¹è¿›: {avg_mse_improvement:.1f}%\n")
        f.write(f"å¹³å‡MAEæ”¹è¿›: {avg_mae_improvement:.1f}%\n")
        f.write(f"å¹³å‡RÂ²æ”¹è¿›: {avg_r2_improvement:.1f}%\n")
        f.write(f"å¹³å‡Pearsonæ”¹è¿›: {avg_pearson_improvement:.1f}%\n\n")
        
        # è¯¦ç»†å¯¹æ¯”è¡¨æ ¼
        f.write("ğŸ“ˆ è¯¦ç»†æ€§èƒ½å¯¹æ¯”\n")
        f.write("-"*40 + "\n")
        f.write(df.to_string(index=False))
        f.write("\n\n")
        
        # å…³é”®å‘ç°
        f.write("ğŸ” å…³é”®å‘ç°\n")
        f.write("-"*40 + "\n")
        
        best_improvement_task = df.loc[df['Pearson_Improvement'].str.replace('%', '').astype(float).idxmax(), 'Task']
        best_improvement = df.loc[df['Pearson_Improvement'].str.replace('%', '').astype(float).idxmax(), 'Pearson_Improvement']
        
        f.write(f"â€¢ æ”¹è¿›æœ€æ˜¾è‘—çš„ä»»åŠ¡: {best_improvement_task} (Pearsonç›¸å…³ç³»æ•°æå‡ {best_improvement})\n")
        
        if avg_mse_improvement > 20:
            f.write("â€¢ æ•´ä½“æ€§èƒ½æ˜¾è‘—æå‡ï¼Œä¼˜åŒ–æ•ˆæœæ˜æ˜¾\n")
        elif avg_mse_improvement > 10:
            f.write("â€¢ æ•´ä½“æ€§èƒ½æœ‰è‰¯å¥½æå‡\n")
        else:
            f.write("â€¢ æ€§èƒ½æå‡æœ‰é™ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–\n")
        
        # å»ºè®®
        f.write("\nğŸ’¡ å»ºè®®\n")
        f.write("-"*40 + "\n")
        f.write("1. ç»§ç»­ç›‘æ§æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°\n")
        f.write("2. è€ƒè™‘é’ˆå¯¹è¡¨ç°è¾ƒå·®çš„ä»»åŠ¡è¿›è¡Œä¸“é—¨ä¼˜åŒ–\n")
        f.write("3. å¯ä»¥å°è¯•æ›´å¤§çš„æ¨¡å‹æˆ–æ›´é•¿çš„è®­ç»ƒæ—¶é—´\n")
        f.write("4. å»ºè®®æ”¶é›†æ›´å¤šæ•°æ®ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½\n")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹è¯„ä¼°æ¨¡å‹æ•ˆæœå¯¹æ¯”...")
    
    # è®¾ç½®è·¯å¾„
    original_dir = "experiments_original"
    improved_dir = "experiments_improved"
    output_dir = "model_comparison"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    device = get_device(True)
    
    # 1. å°è¯•åŠ è½½å·²æœ‰çš„å®éªŒç»“æœ
    print("ğŸ“‚ æ£€æŸ¥å·²æœ‰å®éªŒç»“æœ...")
    original_results = load_model_results(original_dir, device)
    improved_results = load_model_results(improved_dir, device)
    
    # 2. å¦‚æœæ²¡æœ‰å®Œæ•´ç»“æœï¼Œåˆ™è¿è¡Œè¯„ä¼°
    if 'metrics' not in original_results or 'metrics' not in improved_results:
        print("ğŸš€ è¿è¡Œæ¨¡å‹è¯„ä¼°...")
        
        # åŠ è½½æ•°æ®
        dataset_path = "datasets/ac.pkl"
        if os.path.exists(dataset_path):
            config = load_config("config/optimized_config.yaml")
            _, _, test_loader = create_dataloaders_from_pkl(dataset_path, config)
            
            # è¯„ä¼°åŸå§‹æ¨¡å‹
            if 'metrics' not in original_results and 'checkpoint' in original_results:
                print("è¯„ä¼°åŸå§‹æ¨¡å‹...")
                original_model = MultiTaskTalkingFaceEvaluator(config['model'])
                original_model.load_state_dict(original_results['checkpoint']['model_state_dict'])
                original_model.to(device)
                original_metrics = evaluate_model(original_model, test_loader, device, config)
                original_results['metrics'] = original_metrics
            
            # è¯„ä¼°æ”¹è¿›æ¨¡å‹
            if 'metrics' not in improved_results and 'checkpoint' in improved_results:
                print("è¯„ä¼°æ”¹è¿›æ¨¡å‹...")
                improved_model = ImprovedMultiTaskTalkingFaceEvaluator(config['model'])
                improved_model.load_state_dict(improved_results['checkpoint']['model_state_dict'])
                improved_model.to(device)
                improved_metrics = evaluate_model(improved_model, test_loader, device, config)
                improved_results['metrics'] = improved_metrics
    
    # 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    if 'metrics' in original_results and 'metrics' in improved_results:
        print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
        df, comparison_path = create_comparison_report(
            original_results['metrics'], 
            improved_results['metrics'], 
            output_dir
        )
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_dir}")
        print(f"ğŸ“„ å¯¹æ¯”è¡¨æ ¼: {comparison_path}")
        print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨: {output_dir}/performance_comparison.png")
        print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Š: {output_dir}/performance_comparison_report.txt")
        
        # æ˜¾ç¤ºå…³é”®ç»“æœ
        print("\n" + "="*50)
        print("ğŸ¯ å…³é”®ç»“æœæ‘˜è¦:")
        print("="*50)
        print(df[['Task', 'Orig_Pearson', 'Impr_Pearson', 'Pearson_Improvement']].to_string(index=False))
        
    else:
        print("âŒ æ— æ³•æ‰¾åˆ°å®Œæ•´çš„å®éªŒç»“æœï¼Œè¯·ç¡®ä¿å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒ")
        print("ğŸ’¡ å»ºè®®å…ˆè¿è¡Œ:")
        print("  python train_improved.py --config_path config/optimized_config.yaml")

if __name__ == "__main__":
    main()