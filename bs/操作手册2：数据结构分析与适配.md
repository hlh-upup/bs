# ğŸ“‹ æ“ä½œæ‰‹å†Œ2ï¼šæ•°æ®ç»“æ„åˆ†æä¸é€‚é…

## ğŸ” å‘ç°çš„æ•°æ®ç»“æ„

é€šè¿‡å®‰å…¨æ£€æŸ¥ï¼Œæˆ‘ä»¬å‘ç°äº†å®é™…çš„æ•°æ®ç»“æ„ï¼š

### ğŸ“Š å®é™…æ•°æ®ç»“æ„
```
æ•°æ®é›†: dict
â”œâ”€â”€ 'train': list (953ä¸ªæ ·æœ¬)
â”œâ”€â”€ 'val': list (239ä¸ªæ ·æœ¬)  
â””â”€â”€ 'test': list (410ä¸ªæ ·æœ¬)

æ¯ä¸ªæ ·æœ¬: dict
â”œâ”€â”€ 'video_id': str
â”œâ”€â”€ 'split': str
â”œâ”€â”€ 'features': dict
â”‚   â”œâ”€â”€ 'visual': numpy.ndarray
â”‚   â”œâ”€â”€ 'audio': numpy.ndarray
â”‚   â”œâ”€â”€ 'keypoint': numpy.ndarray
â”‚   â””â”€â”€ 'au': numpy.ndarray
â””â”€â”€ 'labels': dict
    â”œâ”€â”€ 'lip_sync': numpy.ndarray
    â”œâ”€â”€ 'expression': numpy.ndarray
    â”œâ”€â”€ 'audio_quality': numpy.ndarray
    â”œâ”€â”€ 'cross_modal': numpy.ndarray
    â””â”€â”€ 'overall': numpy.ndarray
```

## ğŸ¯ éœ€è¦è§£å†³çš„é—®é¢˜

### 1. æ•°æ®ç»“æ„é€‚é…é—®é¢˜
- **é—®é¢˜**: åŸè„šæœ¬å‡è®¾æ•°æ®æ ¼å¼ä¸º `data[split]['features']`ï¼Œå®é™…ä¸º `data[split][i]['features']`
- **è§£å†³æ–¹æ¡ˆ**: é‡æ–°è®¾è®¡æ•°æ®å¤„ç†æµç¨‹ï¼Œé€‚é…åˆ—è¡¨ç»“æ„

### 2. ç‰¹å¾ç»´åº¦ç¡®è®¤
- **éœ€è¦ç¡®è®¤**: æ¯ä¸ªç‰¹å¾çš„å®é™…ç»´åº¦
- **å¤„ç†æ–¹å¼**: é€ä¸ªæ ·æœ¬æ£€æŸ¥ç‰¹å¾å½¢çŠ¶

### 3. æ ‡ç­¾æ ¼å¼ç¡®è®¤
- **éœ€è¦ç¡®è®¤**: æ ‡ç­¾æ˜¯å•ä¸ªå€¼è¿˜æ˜¯æ•°ç»„
- **å¤„ç†æ–¹å¼**: æ£€æŸ¥æ ‡ç­¾æ•°æ®ç±»å‹å’ŒèŒƒå›´

## ğŸ› ï¸ åˆ›å»ºé€‚é…çš„æ•°æ®å¤„ç†è„šæœ¬

### æ­¥éª¤1ï¼šåˆ›å»ºæ•°æ®ç»“æ„åˆ†æå™¨
```bash
# è¿è¡Œåˆ†æå™¨ï¼ˆåªè¯»æ“ä½œï¼Œé›¶é£é™©ï¼‰
python -c "
import pickle
import numpy as np
from pathlib import Path

# åˆ›å»ºåˆ†ææŠ¥å‘Š
Path('reports').mkdir(exist_ok=True)

print('=== æ•°æ®ç»“æ„è¯¦ç»†åˆ†æ ===')
data = pickle.load(open('datasets/ac.pkl', 'rb'))

report = []
report.append('# æ•°æ®ç»“æ„è¯¦ç»†åˆ†ææŠ¥å‘Š\\n\\n')

for split in ['train', 'val', 'test']:
    if split in data:
        split_data = data[split]
        report.append(f'## {split.upper()} æ•°æ®é›† (å…±{len(split_data)}ä¸ªæ ·æœ¬)\\n\\n')
        
        if len(split_data) > 0:
            sample = split_data[0]
            
            # åŸºæœ¬ä¿¡æ¯
            report.append('### åŸºæœ¬ä¿¡æ¯\\n')
            report.append(f'- **video_id**: {sample.get(\"video_id\", \"N/A\")}\\n')
            report.append(f'- **split**: {sample.get(\"split\", \"N/A\")}\\n')
            
            # ç‰¹å¾åˆ†æ
            if 'features' in sample:
                features = sample['features']
                report.append('### ç‰¹å¾ç»´åº¦\\n')
                for key, value in features.items():
                    report.append(f'- **{key}**: shape={value.shape}, dtype={value.dtype}\\n')
                    
                    # æ£€æŸ¥NaNå€¼
                    if hasattr(value, 'size'):
                        nan_count = np.sum(np.isnan(value)) if np.issubdtype(value.dtype, np.floating) else 0
                        report.append(f'  - NaNå€¼: {nan_count}\\n')
            
            # æ ‡ç­¾åˆ†æ
            if 'labels' in sample:
                labels = sample['labels']
                report.append('### æ ‡ç­¾ä¿¡æ¯\\n')
                for key, value in labels.items():
                    if hasattr(value, 'shape'):
                        report.append(f'- **{key}**: shape={value.shape}, dtype={value.dtype}\\n')
                        
                        # æ£€æŸ¥æ ‡ç­¾å€¼èŒƒå›´
                        if value.size > 0:
                            report.append(f'  - å€¼èŒƒå›´: [{np.min(value)}, {np.max(value)}]\\n')
                            unique_vals = np.unique(value)
                            if len(unique_vals) <= 10:
                                report.append(f'  - å”¯ä¸€å€¼: {unique_vals}\\n')
                    else:
                        report.append(f'- **{key}**: {type(value)} - {value}\\n')
        
        report.append('\\n')

# ä¿å­˜æŠ¥å‘Š
with open('reports/data_structure_detailed.md', 'w') as f:
    f.writelines(report)

print('âœ“ è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜: reports/data_structure_detailed.md')
"
```

### æ­¥éª¤2ï¼šæ‰§è¡Œç»“æ„åˆ†æ
```bash
python -c "
import pickle
import numpy as np

print('=== æ‰§è¡Œæ•°æ®ç»“æ„åˆ†æ ===')
data = pickle.load(open('datasets/ac.pkl', 'rb'))

for split in ['train', 'val', 'test']:
    if split in data:
        split_data = data[split]
        print(f'{split}: {len(split_data)} samples')
        
        if len(split_data) > 0:
            sample = split_data[0]
            print(f'  Features:')
            
            if 'features' in sample:
                features = sample['features']
                for key, value in features.items():
                    print(f'    {key}: {value.shape}')
                    
                    # æ£€æŸ¥NaNå€¼
                    if np.issubdtype(value.dtype, np.floating):
                        nan_count = np.sum(np.isnan(value))
                        if nan_count > 0:
                            print(f'      NaN: {nan_count}')
            
            print(f'  Labels:')
            if 'labels' in sample:
                labels = sample['labels']
                for key, value in labels.items():
                    print(f'    {key}: {value.shape}, range: [{np.min(value)}, {np.max(value)}]')

print('=== æ•°æ®ç»“æ„åˆ†æå®Œæˆ ===')
"
```

## ğŸ¯ åˆ›å»ºé€‚é…çš„æ•°æ®å¤„ç†è„šæœ¬

```bash
python -c "
# åˆ›å»ºé€‚é…çš„æ•°æ®å¤„ç†è„šæœ¬
with open('scripts/data_processing_real.py', 'w') as f:
    f.write('''
#!/usr/bin/env python
import pickle
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealDataProcessor:
    def __init__(self):
        self.scalers = {}
        self.pca_models = {}
        
    def load_and_analyze(self, input_path):
        \"\"\"åŠ è½½å¹¶åˆ†ææ•°æ®ç»“æ„\"\"\"
        logger.info(f\"åŠ è½½æ•°æ®: {input_path}\")
        with open(input_path, 'rb') as f:
            data = pickle.load(f)
        
        # åˆ†ææ•°æ®ç»“æ„
        analysis = {}
        for split in ['train', 'val', 'test']:
            if split in data:
                split_data = data[split]
                analysis[split] = {
                    'count': len(split_data),
                    'first_sample': self.analyze_sample(split_data[0]) if split_data else None
                }
        
        return data, analysis
    
    def analyze_sample(self, sample):
        \"\"\"åˆ†æå•ä¸ªæ ·æœ¬\"\"\"
        result = {}
        
        if 'features' in sample:
            features = sample['features']
            result['features'] = {k: v.shape for k, v in features.items()}
        
        if 'labels' in sample:
            labels = sample['labels']
            result['labels'] = {k: {'shape': v.shape, 'range': [float(np.min(v)), float(np.max(v))]} 
                              for k, v in labels.items()}
        
        return result
    
    def process_data(self, data):
        \"\"\"å¤„ç†æ•°æ®\"\"\"
        processed = {}
        
        for split in ['train', 'val', 'test']:
            if split in data:
                logger.info(f\"å¤„ç† {split} æ•°æ®...\")
                split_data = data[split]
                
                # æ”¶é›†æ‰€æœ‰ç‰¹å¾å’Œæ ‡ç­¾
                video_ids = [s['video_id'] for s in split_data]
                splits = [s['split'] for s in split_data]
                
                # ç‰¹å¾å¤„ç†
                feature_keys = list(split_data[0]['features'].keys())
                features = {}
                for key in feature_keys:
                    # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„è¯¥ç‰¹å¾
                    feature_list = [s['features'][key] for s in split_data]
                    features[key] = np.array(feature_list)
                    logger.info(f\"  {split} {key}: {features[key].shape}\")
                
                # æ ‡ç­¾å¤„ç†
                label_keys = list(split_data[0]['labels'].keys())
                labels = {}
                for key in label_keys:
                    label_list = [s['labels'][key] for s in split_data]
                    labels[key] = np.array(label_list)
                    logger.info(f\"  {split} {key}: {labels[key].shape}\")
                
                processed[split] = {
                    'video_ids': video_ids,
                    'splits': splits,
                    'features': features,
                    'labels': labels
                }
        
        return processed
    
    def save_processed(self, data, output_path):
        \"\"\"ä¿å­˜å¤„ç†åçš„æ•°æ®\"\"\"
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'wb') as f:
            pickle.dump(data, f)
        logger.info(f\"ä¿å­˜å®Œæˆ: {output_path}\")

if __name__ == \"__main__\":
    processor = RealDataProcessor()
    data, analysis = processor.load_and_analyze('datasets/ac.pkl')
    
    # ä¿å­˜åˆ†æç»“æœ
    with open('reports/real_data_structure.pkl', 'wb') as f:
        pickle.dump(analysis, f)
    
    # å¤„ç†æ•°æ®
    processed = processor.process_data(data)
    processor.save_processed(processed, 'datasets/ac_real_processed.pkl')
''')

print('âœ“ é€‚é…çš„æ•°æ®å¤„ç†è„šæœ¬å·²åˆ›å»º: scripts/data_processing_real.py')
"
```

## ğŸ”§ æ‰§è¡Œé€‚é…çš„æ•°æ®å¤„ç†

```bash
python scripts/data_processing_real.py
```

## ğŸš¨ å¦‚æœå‡ºç°é—®é¢˜å¦‚ä½•å›æ»š

### é›¶é£é™©å›æ»šæ–¹æ¡ˆ
1. **æ‰€æœ‰æ“ä½œåªåˆ›å»ºæ–°æ–‡ä»¶ï¼Œä¸ä¿®æ”¹åŸå§‹æ–‡ä»¶**
2. **æ‰€æœ‰å¤„ç†ç»“æœä¿å­˜ä¸ºæ–°æ–‡ä»¶å**
3. **åŸå§‹æ•°æ®å®Œå…¨ä¸å—å½±å“**

### å›æ»šæŒ‡ä»¤
```bash
# ç”±äºæ‰€æœ‰æ“ä½œéƒ½æ˜¯åˆ›å»ºæ–°æ–‡ä»¶ï¼Œæ— éœ€å›æ»š
# åŸå§‹æ–‡ä»¶ä¿æŒä¸å˜ï¼Œåªéœ€åˆ é™¤æ–°åˆ›å»ºçš„æ–‡ä»¶å³å¯
rm datasets/ac_real_processed.pkl
rm datasets/ac_processed.pkl
rm reports/*.md
```

---

**å½“å‰çŠ¶æ€**: âœ… å·²åˆ›å»ºé€‚é…è„šæœ¬ï¼Œå‡†å¤‡æ‰§è¡Œå®‰å…¨çš„æ•°æ®å¤„ç†