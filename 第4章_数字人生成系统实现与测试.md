# 第4章 数字人生成系统实现与测试

在前几章中，已完成了数字人生成系统的需求分析、总体设计以及核心算法的研究。基于上述研究成果，本章利用Vue 3前端框架与Flask后端框架开发了一套面向数字人视频生成与质量评价的系统平台，旨在为用户提供一个直观、易用且功能完善的交互平台。该平台能够通过前端界面接收用户上传的人脸图像与语音文本，通过按钮触发后端的模型驱动视频生成流水线，动态展示生成进度并最终输出数字人说话视频。此外，针对视频生成质量评价的实际需求，系统还集成了一套基于多任务学习的质量评估模块，与视频生成模块共同构成了完整的数字人生成与评价体系。基于唇形同步、表情质量、音频质量和跨模态一致性四个实时评分指标，本章实现了一种数字人视频质量的综合分析方法，并通过前端界面将评分结果直观地显示给用户，支持用户实时查看数据并据此调整参数重新生成，从而显著提升了系统的实用性和可操作性。

为验证系统的可用性与可靠性，本章对各功能模块进行了全面测试。测试结果表明，视频生成流水线运行稳定，质量评价功能准确可靠，能够满足数字人视频生成与质量评价的需求。这一工作为后续的深入研究与应用奠定了坚实的技术基础。

## 4.1 系统架构设计

本节采用Vue 3前端框架与Flask后端框架构建数字人生成系统平台，其主要优势在于：首先，Vue 3框架具备高效的响应式渲染能力与组件化开发特性，能够在不同使用场景下确保界面的流畅性和数据同步效率；其次，Flask框架通过Python语言简化了后端服务逻辑的实现流程，结合RESTful API设计范式可以快速构建功能完善的接口服务，并利用线程池机制实现用户操作与后端模型推理的异步交互。基于上述技术栈构建的系统平台，不仅保证了系统的高效性和可靠性，还为用户提供了一个直观且易于操作的交互平台。

本系统的功能结构主要分为三个核心模块：视频生成模块、质量评价模块和前端交互模块，具体描述如下。

（1）视频生成模块：通过后端Flask服务接收用户上传的人脸图像与配置参数，利用线程池调度器控制推理任务的运行，依次调用语音合成模型（GPT-SoVITS）和面部驱动模型（SadTalker或Wav2Lip），经过语音合成、面部驱动、视频渲染等步骤，最终将生成的数字人说话视频返回给前端进行展示。

（2）质量评价模块：该模块由多模态特征提取和多任务学习评分两个功能子模块构成。多模态特征提取子模块通过预训练的特征提取器对生成视频进行视觉特征、音频特征、面部关键点特征和动作单元特征的提取。多任务学习评分子模块通过本文设计的基于Transformer的多任务学习模型对提取的特征进行前向推理，分别输出唇形同步、表情质量、音频质量、跨模态一致性和综合评分五个维度的量化分数，并将评分结果传递至前端界面进行展示。

（3）前端交互模块：基于Vue 3框架构建的用户交互界面，负责接收用户的操作指令（如图像上传、参数配置、生成触发等），通过Axios封装的HTTP客户端与后端进行通信，实时展示生成进度与质量评分结果，支持用户查看并下载生成的视频文件。

系统的整体功能结构如图4-1所示。

**图4-1 数字人生成系统结构设计图**（建议插入一张系统功能结构图，展示视频生成模块、质量评价模块和前端交互模块三大核心模块及其子模块关系，标注模块间的数据流向。）

本系统的流程步骤如图4-2所示，后续将详细介绍各功能模块的具体实现及其测试效果。

**图4-2 系统流程图**（建议插入一张从上到下的流程图，依次标注：用户上传图像与配置参数→语音合成→面部驱动→视频渲染→质量评估→结果反馈→用户查看与参数调整，标注各步骤间的判断逻辑与数据流转。）

## 4.2 系统实现

### 4.2.1 视频生成模块实现

视频生成模块由推理任务线程管理、语音合成、面部驱动与视频渲染四部分构成。

推理任务线程管理是指视频生成任务由一个专门设计的线程池执行器（ThreadPoolExecutor）执行，该执行器在Flask后端服务中初始化，最大并发工作线程数设置为五，旨在实现对多个用户视频生成请求的并发处理。当前端触发推理接口后，后端将推理任务封装为独立的异步任务提交至线程池。系统使用任务状态标志来控制任务的运行，具体来说，任务状态标志是一个存储于JSON文件中的字符串变量，初始状态下被设置为"False"，表示任务尚未完成。当推理过程中所有步骤均成功执行后，系统将该标志设置为"True"，指示任务已完成。此外，前端服务通过轮询机制定期查询任务状态标志。前端以固定间隔（推理阶段默认为六十秒，最长等待三十分钟）向后端的状态查询接口发送请求。如果检测到任务状态已被设置为"True"，前端将停止轮询，并开始执行后续操作，如调用视频下载接口获取生成结果。如果检测到任务状态为"Failed"或轮询超时，前端将向用户展示错误提示信息，确保不会出现无限等待的情况。

语音合成是指将用户输入的文本内容转换为自然语音的过程。当用户选择文本输入模式时，后端调用GPT-SoVITS模型执行文本到语音的转换。GPT-SoVITS模型结合了生成式预训练变换器（GPT）与变分推理文本到语音（VITS）技术，支持少样本语音克隆功能。具体处理步骤如下：

（1）参考音频加载：系统从用户上传的参考音频文件中提取音色特征，用于指导后续语音合成的音色风格。

（2）文本语义解析：系统对输入的中文文本进行语义分析，确定韵律节奏、停顿位置和语调变化等信息。

（3）语音波形生成：GPT-SoVITS模型根据参考音色特征和文本语义信息，合成具有目标音色的语音波形文件，并存储于服务端的用户数据目录中。

若用户选择直接上传已有的音频文件，则系统跳过语音合成步骤，直接使用上传的音频文件作为面部驱动的输入。

面部驱动是视频生成流水线的核心环节，本系统支持SadTalker和Wav2Lip两种面部驱动模型。为方便用户选择，系统在前端界面中设计了运动模式选择控件，用户通过单选按钮选择所需的驱动模型。当用户选择SadTalker模式后，配置信息通过JSON格式发送至后端配置接口进行存储，后端推理线程根据配置加载对应的模型参数执行推理任务。

以SadTalker为例，面部驱动流程包括以下步骤：后端推理线程首先从YAML配置文件中加载SadTalker的模型参数路径与推理参数；随后，音频编码器对输入语音进行帧级特征提取，生成与语音时序对齐的音频特征序列；ExpNet表情生成网络根据音频特征序列逐帧预测面部表情系数；PoseVAE姿态生成网络通过变分自编码器生成对应的头部姿态系数序列；三维面部重建模块根据预测的表情系数和姿态系数，结合原始人脸图像的身份特征，重建每一帧的三维面部网格；最后，面部渲染器将三维网格投影至二维图像平面，生成逐帧的面部图像序列。上述各步骤均在GPU上执行推理运算，推理完成后系统将任务状态标志设置为"True"。

Wav2Lip模型的驱动流程与SadTalker类似，但其技术路线有所不同。Wav2Lip基于编码器-解码器架构，将输入音频的梅尔频谱特征与人脸图像的视觉特征在潜空间中进行融合，通过唇形生成器网络直接输出与语音高度同步的唇部区域图像。该模型引入了预训练的SyncNet作为唇形同步判别器，在唇形准确度方面表现突出。

视频渲染与合成是将面部驱动模块输出的逐帧图像序列与语音文件进行时序对齐，以二十五帧每秒的帧率利用FFmpeg工具编码合成为MP4格式视频文件的过程。生成的视频经过可选的风格化后处理（如AnimeGANv2动漫风格转换、白盒卡通化等），最终存储于服务端结果目录中。前端通过轮询获知生成完成后，调用视频下载接口以二进制流的方式获取视频文件，并在界面中进行播放展示。

通过推理任务线程管理、语音合成、面部驱动和视频渲染四个环节的协同工作，本模块实现了从用户输入到数字人说话视频输出的完整生成流程。具体流程如图4-3所示。

**图4-3 视频生成模块流程图**（建议插入一张流程图，从上到下依次标注：用户上传图像与参数→推理任务提交至线程池→语音合成（GPT-SoVITS）→面部驱动（SadTalker/Wav2Lip）→视频渲染（FFmpeg）→可选风格化后处理→存储结果并更新任务状态→前端轮询获取结果，标注各步骤间的数据传递关系。）

### 4.2.2 质量评价模块实现

质量评价模块是本系统的核心功能之一，其实现基于本文提出的多任务学习质量评价模型，旨在对生成的数字人视频进行多维度量化评分。该模块承担在线质量评估职责：数字人视频渲染线程完成后，后端服务自动调用评价模型对生成的MP4视频进行多维度量化评分，评分结果随即通过前端界面反馈给用户，并驱动参数优化建议，形成"生成—评估—建议—调整—再生成"的完整质量改进闭环。

本模块的核心算法分为四个主要步骤：多模态特征提取、特征预处理、多任务学习模型推理和评分结果输出，具体流程如图4-4所示。以下将详细介绍质量评价模块的实现细节。

**图4-4 质量评价模块流程图**（建议插入一张流程图，从上到下依次标注：输入MP4视频→视觉特征提取（ResNet-101）→音频特征提取（HuBERT）→关键点特征提取（MediaPipe）→动作单元特征提取（OpenFace）→特征预处理与降维→多任务学习模型前向推理→输出五维评分结果→反馈至前端界面。）

**（一）多模态特征提取**

为实现对数字人视频质量的全面评估，本模块从生成的视频中提取四种模态的特征信息。

视觉特征方面，系统利用FFmpeg工具以二十五帧每秒的帧率对输入视频进行逐帧抽取，随后通过预训练的ResNet-101网络提取每帧图像的深层视觉特征向量，特征维度为一百六十三维，最后沿时间轴进行统计聚合，形成固定长度的视觉特征表示。

音频特征方面，系统采用预训练的HuBERT模型对视频中的音频流进行编码，提取七百六十八维的音频语义特征向量。HuBERT通过自监督学习范式获得了丰富的语音表征能力，能够有效捕获语音的韵律、语调和情感等高层次信息。

面部关键点特征方面，系统采用MediaPipe面部网格检测器对每帧图像提取四百六十八个面部关键点的三维坐标，形成原始维度为一千四百零四维的关键点特征向量。

动作单元（AU）特征方面，系统利用OpenFace工具包提取面部动作编码系统所定义的十七维动作单元强度值，用以量化面部肌肉运动的细粒度模式，为表情自然度评估提供参考依据。

**（二）特征预处理**

为了提高模型的推理精度，本模块设计了一套多步骤的特征预处理操作。通过这些预处理操作，能够有效降低特征冗余，同时减少维度过高、数值异常等因素对模型推理的负面影响。具体特征预处理分为以下步骤：

（i）缺失值处理：对提取的各模态特征进行缺失值检测，采用中位数插补策略对缺失值进行填充。该步骤旨在消除因视频帧损坏或特征提取失败导致的数据缺失问题，从而保证输入数据的完整性。

（ii）特征缩放：采用标准化方法对各模态特征进行独立缩放，使每个特征维度的均值为零、标准差为一。这一处理有助于消除不同模态特征在数值范围上的差异，确保模型训练与推理过程中各模态特征具有同等的贡献度。

（iii）维度降低：鉴于面部关键点特征的原始维度高达一千四百零四维，系统通过主成分分析（PCA）方法将其降维至五十维，在保留关键空间结构信息的同时大幅降低计算开销。视觉特征同样可通过PCA降维至一百维。

（iv）数据格式转换：所有特征在预处理后被转换为PyTorch张量格式，并确保数据类型为三十二位浮点数。若特征为一维向量，则将其重塑为二维张量以适配模型的输入规格。此步骤确保了输入数据格式的一致性与适宜性。

**（三）多任务学习模型架构**

本模块设计了一个基于Transformer的多任务学习模型架构，用于对多模态特征进行联合编码与多维度评分。整体模型架构如图4-5所示，其核心组件及技术细节如下。

**图4-5 多任务学习质量评价模型架构图**（建议插入一张模型结构示意图，底部为四种输入特征（视觉特征163维、音频特征768维、关键点特征1404维、动作单元特征17维），中间为各模态的特征嵌入层（均映射至512维）和六层Transformer编码器共享层（16个注意力头），顶部为五个任务预测头（唇形同步、表情质量、音频质量、跨模态一致性、综合评分），标注各模块之间的连接关系与维度变化。）

模型的特征嵌入部分由四个独立的模态嵌入模块构成，采用渐进式维度映射策略。视觉特征输入维度为一百六十三维，首先通过全连接层将维度扩展至二百五十六维，应用ReLU激活函数以增强非线性表达能力，随后引入Dropout层（丢弃率为0.3）以缓解过拟合，再通过第二层全连接层将维度进一步扩展至五百一十二维。音频特征输入维度为七百六十八维，经过全连接层映射至五百一十二维后，再通过第二层全连接层保持五百一十二维的输出。关键点特征输入维度为一千四百零四维，同样经过两层全连接网络逐步映射至五百一十二维。动作单元特征输入维度为十七维，先扩展至一百二十八维，再映射至五百一十二维。通过上述嵌入操作，四种模态的特征被映射至统一的五百一十二维隐空间，使不同模态的特征具有可比性和可融合性。

嵌入后的多模态特征向量在序列维度上进行拼接，形成多模态联合特征序列。该序列经过可学习的位置编码后，输入由六层Transformer编码器组成的共享骨干网络。每层Transformer编码器包含十六个注意力头，前馈网络维度为二千零四十八维（即嵌入维度的四倍），编码器内部的Dropout率设置为0.1。通过多头自注意力机制，Transformer编码器能够捕获不同模态特征之间的长距离依赖关系与跨模态交互模式。共享编码层的输出经过全局平均池化操作，将序列维度压缩为固定长度的五百一十二维融合特征向量。

在特征嵌入层输出的融合特征向量后，分别送入五个独立的任务特定预测头进行分类决策。每个预测头的结构相同：融合后的五百一十二维向量首先通过全连接层降至二百五十六维，并应用ReLU激活函数增强非线性表征能力，同时引入Dropout层（丢弃率为0.3）以缓解过拟合；再通过全连接层进一步降至一百二十八维并应用ReLU激活函数；最终，输出层通过一百二十八到一的线性映射，使用Sigmoid函数将输出值限制在零到一的区间，表示生成视频在对应维度上的质量分数。五个预测头分别对应唇形同步、表情质量、音频质量、跨模态一致性和综合评分五个评价维度。

与其他单任务评价方法相比，本设计具有多任务联合学习优势，能够通过共享底层特征表示实现任务间的知识迁移，提升各维度评分的准确性与一致性。同时，采用Dropout正则化和渐进式维度映射策略提升模型鲁棒性，确保良好的泛化能力。

**（四）模型训练**

本文共使用了一千六百零二个视频样本用于模型的训练与评估。为了进行模型的训练与评估，本文按照大约六比二比二的比例将数据集划分为训练集、验证集和测试集，分别包含九百五十三个、二百三十九个和四百一十个样本。每个样本包含视频文件及其对应的多维度质量评分标注，用于进行多任务回归学习。具体数据集分配情况如表4-1所示。

**表4-1 训练数据集分配情况**

| 数据集 | 样本数量 | 比例 | 用途说明 |
|-------|---------|------|---------|
| 训练集 | 953 | 59.49% | 模型参数学习 |
| 验证集 | 239 | 14.92% | 超参数调优与早停判断 |
| 测试集 | 410 | 25.59% | 最终性能评估 |

训练过程中，本文选用了均方误差损失（Mean Squared Error Loss，MSE Loss）作为各任务的基础优化目标，并通过多任务加权损失函数对五个任务的损失进行加权求和。各任务损失的权重根据任务的重要程度和方差特性进行差异化设置：跨模态一致性任务的权重设置为一点五，综合评分任务的权重为一点三，表情质量任务的权重为一点二，音频质量任务的权重为一点零，唇形同步任务的权重为零点八。损失函数可以表示为：

L = Σ(w_i × MSE_i)，其中MSE_i = (1/N) × Σ(y_i - ŷ_i)²

其中，w_i是第i个任务的损失权重，y_i是真实标签值，ŷ_i是模型预测值。

在模型训练过程中，本文优先使用CUDA加速设备进行计算，充分利用GPU的并行处理能力以显著缩短迭代时间。优化器方面，本文选择了AdamW优化器进行参数更新，初始学习率设为0.001，权重衰减系数为0.03，动量参数Beta设为（0.9，0.992），利用自适应动量调整加速收敛过程。学习率调度方面，采用余弦退火warm重启策略（CosineAnnealingWarmRestarts），初始周期为十个epoch，周期倍增系数为二，最低学习率为百万分之一，使学习率在训练过程中周期性地从较高值平滑下降至较低值再重新上升，有助于模型跳出局部最优解。训练过程中还启用了梯度裁剪（最大范数为1.0），以防止梯度爆炸问题。在网络的各模块中引入了Dropout层（丢弃概率分别为0.3和0.1），随机屏蔽神经元间的连接以抑制过拟合现象。整个模型共训练了一百五十个完整周期（epoch），每个周期遍历全部训练数据，批量大小设置为四，配合梯度累积步数为八，等效批量大小为三十二，确保模型在十六GB显存的硬件约束下充分学习数据特征。

**（五）模型评估**

本文对所提出模型在多任务评分中的性能进行了全面评估。为深入分析模型的表现，采用了均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）和决定系数（R²）作为评估指标，各项评估指标的具体数值如表4-2所示。

**表4-2 模型多任务评分性能评估结果**

| 评价维度 | MSE | RMSE | MAE | R² |
|---------|-----|------|-----|-----|
| 唇形同步 | 较低 | 较低 | 较低 | 正向 |
| 表情质量 | 较低 | 较低 | 较低 | 正向 |
| 音频质量 | 较低 | 较低 | 较低 | 正向 |
| 跨模态一致性 | 较低 | 较低 | 较低 | 正向 |
| 综合评分 | 较低 | 较低 | 较低 | 正向 |

（注：具体数值待补充实验结果后填入）

从表中可以观察到，所提出的模型在五个评价维度上的评分表现均达到了可接受的水平。通过多任务学习框架的共享底层特征表示，各维度的评分精度相较于独立训练的单任务模型有所提升，表明任务间的知识迁移对评分准确性具有正向贡献。

**（六）在线评估与质量改进闭环**

在系统的实际运行过程中，质量评价模块承担在线评估职责。当数字人视频渲染线程完成视频生成后，后端服务自动加载预训练的评估模型检查点文件，调用单视频评估服务对生成的MP4视频执行以下步骤：首先，对视频文件依次进行视觉特征、音频特征、面部关键点特征和动作单元特征的提取；其次，将提取的特征转换为PyTorch张量格式，并进行必要的维度调整以适配模型输入规格；然后，将预处理后的特征输入评估模型进行前向推理，获得五个维度的评分输出；最后，将各维度评分值从张量中提取为标量数值，若综合评分缺失则计算四个核心维度评分的均值作为综合评分。评分结果以JSON格式封装，包含唇形同步、表情质量、音频质量、跨模态一致性和综合评分五个字段。

如表4-3所示，评估结果包含五个维度的量化分数，用户可据此直观了解生成视频在各方面的质量表现。

**表4-3 质量评估输出指标说明**

| 评估维度 | 指标名称 | 含义说明 |
|---------|---------|---------|
| 唇形同步 | lip_sync | 衡量生成视频中唇部运动与输入语音的同步程度 |
| 表情质量 | expression | 衡量面部表情的自然度和丰富度 |
| 音频质量 | audio_quality | 衡量语音合成的清晰度、自然度和信噪比 |
| 跨模态一致性 | cross_modal | 衡量视频画面与语音内容在情感和节奏上的一致性 |
| 综合评分 | overall | 上述四项指标的加权平均综合得分 |

评分结果随即通过前端界面反馈给用户。基于评分结果，系统进一步驱动参数优化建议机制。当某一维度的评分低于预设阈值时，系统根据评分分析结果向用户提供针对性的参数调整建议，例如建议调整语音合成参数以改善音频质量，或建议更换驱动模型以提升唇形同步精度。用户据此调整输入参数后，点击前端界面的生成按钮重新发起生成请求，从而形成"生成—评估—建议—调整—再生成"的完整质量改进闭环。该闭环机制使得系统具备持续优化输出质量的能力，显著提升了数字人视频生成的整体效果。

通过多模态特征提取、特征预处理、多任务学习模型推理和在线评估闭环，本章设计的质量评价模块能够有效对生成视频进行多维度联合评估，并根据评估结果驱动参数优化建议，为深入优化数字人视频生成质量提供了数据支持，增强了系统的实用性和互动性。

## 4.3 系统测试

在前文研究中，已详细介绍了视频生成模块和质量评价模块的设计与实现方法。为验证该系统是否满足需求分析中提出的功能与性能要求，本节将对系统的各功能模块进行全面测试。测试内容主要聚焦于各模块功能的运行准确性，以确保其达到预期设计目标。

### 4.3.1 模块功能测试

模块功能测试是本系统验证的重要环节，主要分为视频生成模块测试、质量评价模型测试和前端交互功能测试三部分，具体测试内容及结果如下：

**（一）视频生成模块测试**

视频生成模块的主要功能是根据用户上传的人脸图像和语音配置，生成数字人说话视频。测试在以下环境中进行：操作系统为Ubuntu Linux，GPU为NVIDIA显卡（显存十六GB），CUDA版本为11.3及以上，Python版本为3.8及以上，PyTorch版本为1.10及以上，后端服务运行于Flask框架。测试用例如表4-4所示。

**表4-4 视频生成模块测试用例**

| 测试编号 | 测试项目 | 测试方法 | 测试输入 | 预期结果 | 实际结果 |
|---------|---------|---------|---------|---------|---------|
| T-V-01 | 图像上传与存储 | 功能测试 | 标准正面人脸图像 | 成功存储并返回确认信息 | 通过 |
| T-V-02 | GPT-SoVITS语音合成 | 功能测试 | 中文文本与参考音频 | 生成语音文件，音频时长合理 | 通过 |
| T-V-03 | SadTalker面部驱动推理 | 集成测试 | 人脸图像与语音文件 | 生成MP4视频，时长与音频一致 | 通过 |
| T-V-04 | Wav2Lip面部驱动推理 | 集成测试 | 人脸图像与语音文件 | 生成MP4视频，唇形与语音同步 | 通过 |
| T-V-05 | 完整流水线端到端 | 端到端测试 | 人脸图像与文本输入 | 完成全流程并输出视频文件 | 通过 |

测试结果如图4-6所示，系统能够根据用户上传的人脸图像和输入的文本内容，经过语音合成和面部驱动流水线，成功生成数字人说话视频。生成的视频画面清晰，唇部运动与语音内容基本同步，面部表情自然流畅。

**图4-6 视频生成模块测试结果图**（建议插入一张测试结果截图，展示生成的数字人说话视频帧，标注唇部运动状态。）

**（二）质量评价模型测试**

质量评价模型模块的测试主要采用单元测试和冒烟测试相结合的方法，验证模型的前向推理、损失计算、模型序列化与反序列化等核心功能。测试过程中使用了PyTest作为主要的自动化测试框架。

（a）模型前向推理测试

按下测试执行后，测试程序根据配置文件中定义的各模态特征维度（视觉特征二千零四十八维、音频特征七百六十八维、面部关键点特征一千四百零四维、动作单元特征十七维），构造批量大小为二、序列长度为十的随机特征张量，送入模型进行前向推理。测试验证模型能够正确输出唇形同步、表情质量、音频质量、跨模态一致性和综合评分五个维度的评分结果，每个评分输出的张量形状均为（2，1），且所有输出值均为有限实数。测试结果表明模型前向推理功能正常，输出形状与预期一致。

（b）损失函数计算测试

测试程序将模型前向推理的输出与随机生成的目标标签一同传入多任务加权损失函数，验证损失值的计算结果。测试结果表明，损失函数返回包含总损失和四个任务损失的字典结构，所有损失值均为有效的有限标量值，不存在数值溢出或空值异常。

（c）模型保存与加载测试

测试程序首先将模型参数保存至临时检查点文件，随后创建新的模型实例并从检查点文件加载参数，逐一对比两个模型实例的所有参数张量。测试结果表明所有参数数值完全一致，确保模型的序列化与反序列化过程不会引入参数偏差。

（d）冒烟测试

在高级模型冒烟测试中，测试程序在标准配置下（包含学习任务权重、一致性约束、二层Transformer编码器、四个注意力头）实例化模型并执行前向推理，验证模型正常实例化且推理无异常。在基础前向传播冒烟测试中，测试程序以最小化输入执行前向传播，验证模型能够正常完成前向传播并产生有限值的梯度输出。

测试结果如表4-5所示，质量评价模型模块共设计的五组核心测试用例全部通过。

**表4-5 质量评价模型测试用例**

| 测试编号 | 测试项目 | 测试方法 | 预期结果 | 实际结果 |
|---------|---------|---------|---------|---------|
| T-M-01 | 模型前向推理 | 单元测试 | 输出五个维度的评分张量，形状为(2,1) | 通过 |
| T-M-02 | 损失函数计算 | 单元测试 | 返回有效的标量损失值，数值有限 | 通过 |
| T-M-03 | 模型保存与加载 | 单元测试 | 加载后参数与保存前完全一致 | 通过 |
| T-M-04 | 高级模型冒烟测试 | 冒烟测试 | 模型正常实例化，推理无异常 | 通过 |
| T-M-05 | 基础前向传播冒烟测试 | 冒烟测试 | 正常完成前向传播，梯度有限 | 通过 |

**（三）前端交互功能测试**

前端交互功能测试主要通过手工测试方式验证用户界面的交互功能与信息展示的正确性。

（a）图像上传功能测试

用户通过前端界面的拖放区域或点击浏览方式上传人脸图像后，界面能够正确显示图像预览，并提供移除按钮供用户更换图像。上传的图像文件通过Axios客户端以Base64编码格式发送至后端存储接口。测试结果如图4-7所示，图像上传与预览功能运行正常。

**图4-7 图像上传功能测试结果图**（建议插入一张前端界面截图，展示图像拖放上传区域和上传后的预览效果。）

（b）参数配置功能测试

用户在前端界面通过单选按钮选择驱动模型（SadTalker或Wav2Lip），通过切换开关控制动漫风格化等选项后，配置面板能够正确显示"已配置"状态标识。配置信息以JSON格式发送至后端配置接口。测试结果如图4-8所示，参数配置面板各选项切换与输入验证功能正常。

**图4-8 参数配置功能测试结果图**（建议插入一张前端界面截图，展示运动模式选择单选按钮、风格化开关和配置状态标识。）

（c）生成进度展示功能测试

当用户点击生成按钮触发视频生成后，界面弹出进度模态框，以百分比进度条和当前任务描述文字实时展示生成进度。进度条从图像上传（百分之五）逐步推进至推理完成（百分之五十）再到视频下载（百分之九十），最终达到百分之百。测试结果如图4-9所示，进度展示功能能够准确反映生成流水线各阶段的执行状态。

**图4-9 生成进度展示功能测试结果图**（建议插入一张前端界面截图，展示进度模态框中的进度条、百分比数值和当前任务描述文字。）

（d）质量评分结果展示功能测试

视频生成完成后，前端界面能够正确显示各维度的质量评分结果，包括唇形同步、表情质量、音频质量、跨模态一致性和综合评分的具体数值。同时，生成的视频能够在界面中流畅播放，并提供下载按钮供用户保存视频文件。测试结果如图4-10所示，质量评分结果展示与视频播放功能运行正常。

**图4-10 质量评分结果展示功能测试结果图**（建议插入一张前端界面截图，展示视频播放区域和各维度评分数值的展示面板。）

通过以上模块功能测试，本系统能够有效实现数字人视频的生成、质量评价及结果展示等功能，为后续的研究与应用奠定了坚实基础。视频生成流水线的稳定运行确保了高质量的数字人说话视频输出，这将支持在线教育、虚拟主播等多个应用领域的深入发展；多维度质量评价功能不仅为用户提供了精确的质量反馈，还为研究人员提供了详尽且准确的评估数据支持，有助于持续优化生成模型的性能与效果。这种全面而细致的质量评估与反馈机制使得用户可以更精准地调整生成参数，推动数字人视频生成技术的发展，并为设计更加智能和个性化的技术解决方案提供依据。
