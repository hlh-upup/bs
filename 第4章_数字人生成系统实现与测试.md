# 第4章 数字人生成系统实现与测试

在前几章完成了数字人生成系统的需求分析与总体设计之后，本章将重点介绍系统的具体实现过程与测试验证工作。首先，本章对系统的整体架构进行详细阐述，明确各层级与模块之间的职责划分与数据流转关系；其次，围绕模型驱动数字人视频生成和模型质量评价两个核心功能模块，从算法原理、模型选型、驱动流程和关键技术要点等方面进行深入描述；最后，通过系统测试环节，验证各模块功能的正确性与系统整体的稳定性。

## 4.1 系统架构设计

本节对数字人生成系统的整体架构进行详细描述，阐明系统的分层设计理念、主要功能模块及模块间的数据交互关系。

本系统采用前后端分离的三层架构设计，自上而下依次为表示层、业务逻辑层和数据与模型层。三层架构各司其职、松耦合、高内聚，有效保障了系统的可维护性与可扩展性。系统架构如图4-1所示。

**图4-1 数字人生成系统整体架构图**（建议插入一张自上而下分层的系统架构示意图，最上方为表示层，中间为业务逻辑层，最下方为数据与模型层，各层之间用箭头标注数据流向与接口调用关系。）

**（一）表示层**

表示层采用Vue 3框架结合TypeScript进行开发，使用Vite作为构建工具，以Pinia进行全局状态管理。该层主要承担用户交互界面的渲染与操作响应功能，包括数字人形象上传、语音文本配置、推理参数设置、视频生成进度展示以及质量评分结果的可视化呈现。前端通过Axios封装的HTTP客户端与后端进行通信，所有异步请求均设置了较长的超时时间（最长可达一小时），以适配视频生成等耗时较长的任务场景。前端还实现了轮询机制，以固定间隔（默认十秒）查询后端异步任务的执行状态，最长等待时间为三十分钟，确保用户能够实时获取任务进度。

**（二）业务逻辑层**

业务逻辑层基于Flask框架构建RESTful API服务，运行于5000端口，启用了跨域资源共享（CORS）支持。该层是系统的核心调度中心，负责接收前端请求并协调各功能模块完成具体任务。主要业务模块包括：数字人视频推理模块、语音合成模块、视频质量评估模块以及视频后处理模块。业务逻辑层采用线程池执行器实现异步推理任务调度，支持并发处理多个用户的视频生成请求，同时内置了完善的日志记录与异常处理机制。该层提供的核心API接口涵盖图像上传、配置设定、参考音频与文本上传、推理触发、任务状态查询、视频下载等功能，形成完整的请求处理链路。

**（三）数据与模型层**

数据与模型层包含系统运行所依赖的全部模型资源与数据处理组件。模型资源主要包括三大类：一是语音合成模型（VITS/GPT-SoVITS），负责将文本转换为自然语音；二是面部驱动模型（SadTalker、Wav2Lip），负责根据语音信号驱动静态人脸图像生成说话视频；三是多任务学习评估模型，负责对生成视频进行多维度质量评分。数据处理组件则涵盖特征提取管线（包括视觉特征、音频特征、面部关键点特征及动作单元特征的提取）以及数据预处理管线（包括缺失值插补、异常值处理、特征缩放与降维等）。

**（四）模块间数据流与交互关系**

系统的典型工作流程如下：用户通过前端界面上传人脸图像并配置生成参数后，前端服务将依次调用后端的图像上传接口、配置接口和参考音频文本上传接口，完成任务初始化。随后，前端触发推理接口，后端业务逻辑层接收请求后，首先调用语音合成模型将输入文本转换为语音文件，再将语音文件与人脸图像一并传入面部驱动模型，经过音频特征提取、面部关键点检测、运动系数估计和视频渲染等步骤，最终生成数字人说话视频。视频生成完成后，后端自动调用质量评估模型对生成的视频进行多维度评分，评分结果与视频文件一同返回前端供用户查看。整个数据流形成了"上传—配置—合成—驱动—生成—评估—反馈"的完整闭环。

综上所述，本系统通过分层架构实现了前端交互、后端调度与模型推理的有效解耦，各模块职责清晰，接口规范统一，为后续的系统实现奠定了坚实的架构基础。

## 4.2 系统实现

本节围绕数字人生成系统的两个核心功能模块，即模型驱动数字人视频生成与模型质量评价，详细阐述其实现原理、技术方案与关键流程。

### 4.2.1 模型驱动数字人视频生成

本小节详细介绍数字人视频生成的核心驱动流程、算法原理、模型选型及关键实现技术点，阐明从静态人脸图像到动态说话视频的完整生成过程。

**（一）核心算法原理与模型选型**

数字人视频生成的核心目标是根据一段输入语音信号驱动一张静态人脸图像，使其产生与语音内容同步的唇部运动和面部表情变化，最终输出流畅自然的说话人脸视频。本系统支持两种主流的面部驱动模型：SadTalker和Wav2Lip，二者基于不同的技术路线实现语音到视频的映射。

SadTalker模型采用基于三维可变形模型（3DMM）系数的驱动策略。该模型首先通过音频编码器提取输入语音的深层特征表示，然后利用ExpNet表情生成网络和PoseVAE姿态生成网络分别预测面部表情系数和头部姿态系数。表情生成网络将音频特征映射为三维可变形模型的表情参数序列，PoseVAE则通过变分自编码器建模头部运动的多样性分布，从而生成自然的头部摆动。最后，模型利用面部渲染模块将预测的三维系数还原为逐帧的二维面部图像，并通过面部增强技术提升生成视频的清晰度与真实感。SadTalker在表情丰富度和头部运动自然度方面具有显著优势。

Wav2Lip模型则聚焦于唇形同步精度的优化。该模型基于编码器-解码器架构，将输入音频的梅尔频谱特征与人脸图像的视觉特征在潜空间中进行融合，通过精确的唇形生成器网络直接输出与语音高度同步的唇部区域图像。Wav2Lip引入了预训练的SyncNet作为唇形同步判别器，在训练过程中提供像素级的唇形对齐监督信号，确保生成结果具有优秀的音画同步性。该模型在唇形准确度方面表现突出，尤其适合对口型精度要求较高的应用场景。

语音合成方面，本系统集成了GPT-SoVITS模型作为文本到语音的转换引擎。GPT-SoVITS结合了生成式预训练变换器（GPT）与变分推理文本到语音（VITS）技术，支持少样本语音克隆功能，用户仅需提供少量参考语音即可合成具有特定音色特征的语音输出，从而实现个性化的数字人语音效果。

**（二）视频生成流水线**

数字人视频的生成过程由一条多阶段流水线驱动，各阶段依次执行，前一阶段的输出作为后一阶段的输入。完整的生成流水线如图4-2所示。

**图4-2 数字人视频生成流水线示意图**（建议插入一张从左到右的流程图，依次标注以下各阶段：图像上传与预处理→参数配置→语音合成→面部驱动→视频渲染→后处理与输出，每个阶段下方标注所使用的关键模型或技术。）

第一阶段为图像上传与预处理。用户通过前端界面上传一张正面人脸图像，后端接收图像文件后进行格式校验与存储。系统对上传图像进行人脸检测与对齐操作，采用RetinaFace检测器定位人脸区域，确保输入图像满足后续模型的输入要求。

第二阶段为参数配置。用户在前端界面选择所需的驱动模型（SadTalker或Wav2Lip）、设定视频分辨率、帧率等参数，并上传参考音频或输入待合成的文本内容。前端将配置信息以JSON格式发送至后端配置接口进行持久化存储。

第三阶段为语音合成。若用户选择文本输入模式，后端将调用GPT-SoVITS模型将文本转换为语音文件。合成过程中，系统根据用户提供的参考音频提取音色特征，结合输入文本的语义信息，生成具有目标音色的语音波形文件。若用户直接上传音频文件，则跳过此阶段。

第四阶段为面部驱动。该阶段是视频生成的核心环节。后端将语音文件与预处理后的人脸图像一并输入所选的面部驱动模型。以SadTalker为例，驱动流程包括以下步骤：首先，音频编码器对输入语音进行帧级特征提取，生成与语音时序对齐的音频特征序列；其次，ExpNet根据音频特征序列逐帧预测面部表情系数，PoseVAE生成对应的头部姿态系数序列；然后，三维面部重建模块根据预测的表情系数和姿态系数，结合原始人脸图像的身份特征，重建每一帧的三维面部网格；最后，面部渲染器将三维网格投影至二维图像平面，生成逐帧的面部图像序列。上述各步骤的推理过程均在GPU上异步执行，后端通过线程池调度器管理推理任务，避免阻塞主服务进程。

第五阶段为视频渲染与合成。系统将面部驱动模块输出的逐帧图像序列与合成语音进行时序对齐，以二十五帧每秒的帧率利用FFmpeg工具编码合成为MP4格式视频文件。在此过程中，系统进行音视频同步校准，确保唇部运动与语音内容精确对齐。

第六阶段为后处理与输出。生成的视频经过可选的风格化处理（如动漫风格转换等），最终存储于服务端结果目录中。前端通过轮询任务状态接口获知生成完成后，调用视频下载接口获取最终结果并在界面中播放展示。

**（三）关键实现技术点**

在实现过程中，系统针对以下关键技术难点进行了专项优化。

在异步任务调度方面，由于数字人视频生成涉及多个深度学习模型的串行推理，单次生成耗时可达数分钟甚至更长。系统采用线程池执行器实现异步任务管理，每个生成请求被封装为独立的异步任务提交至线程池，后端通过任务标识符追踪每个任务的执行状态。前端以轮询方式定期查询任务进度，轮询间隔根据处理阶段动态调整（推理阶段为六十至九十秒），有效平衡了实时性与服务端负载。

在显存优化方面，系统面临十六GB GPU显存的硬件约束。为确保多模型推理过程的稳定运行，系统采取了模型按需加载与显存动态释放策略，在语音合成阶段完成后释放语音模型所占显存，再加载面部驱动模型进行推理，避免多模型同时驻留显存导致内存溢出。

在容错处理方面，系统在推理流水线的每个阶段均设置了异常捕获与日志记录机制。当某一阶段发生异常时，系统将错误信息写入日志并向前端返回明确的错误提示，避免因单一环节故障导致整个请求链路中断。

综上，本系统通过多模型协同驱动、异步流水线调度和多层次优化策略，实现了从文本或语音输入到高质量数字人说话视频输出的完整生成流程。

### 4.2.2 模型质量评价

本小节介绍基于多任务学习的数字人视频质量评价模型的实现细节，该模型在系统中承担在线质量评估职责，为视频生成质量的持续改进提供定量依据。

**（一）评价模型概述**

数字人视频生成的质量评价是保障系统输出可用性的关键环节。传统的单维度评价方法难以全面反映生成视频在唇形同步、表情自然度、音频质量和跨模态一致性等方面的综合表现。为此，本系统设计并实现了一种基于多任务学习框架的视频质量评价模型，能够同时对上述四个维度进行联合评分，并输出综合质量分数。

该评价模型的核心架构采用共享底层（Shared-Bottom）多任务学习范式，以Transformer编码器作为共享特征提取骨干网络，多个任务特定的预测头分别负责各维度的评分输出。模型架构如图4-3所示。

**图4-3 多任务学习质量评价模型架构图**（建议插入一张模型结构示意图，底部为四种输入特征（视觉特征、音频特征、面部关键点特征、动作单元特征），中间为各模态的特征嵌入层和Transformer编码器共享层，顶部为五个任务预测头（唇形同步、表情质量、音频质量、跨模态一致性、综合评分），标注各模块之间的连接关系。）

**（二）多模态特征提取**

评价模型的输入包含四种模态的特征信息，均通过离线预提取的方式获得。

视觉特征方面，系统利用FFmpeg工具以二十五帧每秒的帧率对输入视频进行逐帧抽取，随后通过预训练的ResNet-101网络提取每帧图像的二千零四十八维深层视觉特征向量，最后沿时间轴进行统计聚合（均值与标准差），形成固定长度的视觉特征表示。

音频特征方面，系统采用预训练的HuBERT模型对视频中的音频流进行编码，提取七百六十八维的音频语义特征向量。HuBERT通过自监督学习范式获得了丰富的语音表征能力，能够有效捕获语音的韵律、语调和情感等高层次信息。

面部关键点特征方面，系统采用MediaPipe面部网格检测器对每帧图像提取四百六十八个面部关键点的三维坐标，形成原始维度为一千四百零四维的关键点特征向量。鉴于原始维度较高，系统通过主成分分析（PCA）方法将其降维至五十维，在保留关键空间结构信息的同时大幅降低了计算开销。

动作单元（AU）特征方面，系统利用OpenFace工具包提取面部动作编码系统所定义的动作单元强度值，用以量化面部肌肉运动的细粒度模式。动作单元特征能够捕获眨眼、皱眉、嘴角上扬等微表情动作，为表情自然度评估提供重要参考依据。

**（三）模型结构与评分机制**

评价模型的网络结构由三部分组成：模态特征嵌入层、Transformer共享编码层和任务特定预测头。

在模态特征嵌入层中，四种模态的原始特征分别经过独立的全连接网络映射至统一的隐空间维度，使不同模态的特征具有可比性和可融合性。嵌入后的多模态特征向量在序列维度上进行拼接，形成多模态联合特征序列。

在Transformer共享编码层中，联合特征序列经过位置编码后，输入由六层Transformer编码器组成的共享骨干网络。每层Transformer编码器包含十六个注意力头，通过多头自注意力机制捕获不同模态特征之间的长距离依赖关系与跨模态交互模式。共享编码层的输出经过全局平均池化操作，压缩为固定长度的融合特征向量。

在任务特定预测头中，融合特征向量被分别送入五个独立的多层感知器预测头，分别输出唇形同步质量分数、表情质量分数、音频质量分数、跨模态一致性分数和综合质量分数。每个预测头输出一个标量评分值，反映生成视频在对应维度上的质量水平。

模型训练采用多任务加权损失函数，各任务损失的权重根据任务的重要程度和方差特性进行差异化设置。其中，跨模态一致性任务的权重设置为一点五，表情质量任务的权重为一点二，其余任务权重均为一点零。训练优化器选用AdamW，学习率调度采用余弦退火策略，训练数据集包含一千六百零二个样本，按照训练集、验证集和测试集进行六比二比二的比例划分。

**（四）在线评估与质量改进闭环**

在系统的实际运行过程中，质量评价模型承担在线评估职责。当数字人视频渲染线程完成视频生成后，后端服务自动加载预训练的评估模型检查点文件，调用单视频评估服务对生成的MP4视频进行多维度量化评分。评估服务首先对视频文件进行特征提取（包括视觉特征、音频特征、面部关键点特征和动作单元特征），将提取的特征输入评估模型进行前向推理，获得各维度评分及综合评分。评分结果以JSON格式封装，随即通过前端界面反馈给用户。

如表4-1所示，评估结果包含五个维度的量化分数，用户可据此直观了解生成视频在各方面的质量表现。

**表4-1 质量评估输出指标说明**

| 评估维度 | 指标名称 | 含义说明 |
|---------|---------|---------|
| 唇形同步 | lip_sync | 衡量生成视频中唇部运动与输入语音的同步程度 |
| 表情质量 | expression | 衡量面部表情的自然度和丰富度 |
| 音频质量 | audio_quality | 衡量语音合成的清晰度、自然度和信噪比 |
| 跨模态一致性 | cross_modal | 衡量视频画面与语音内容在情感和节奏上的一致性 |
| 综合评分 | overall | 上述四项指标的加权平均综合得分 |

基于评分结果，系统进一步驱动参数优化建议机制。当某一维度的评分低于预设阈值时，系统根据评分分析结果向用户提供针对性的参数调整建议，例如建议调整语音合成参数以改善音频质量，或建议更换驱动模型以提升唇形同步精度。用户据此调整输入参数后重新发起生成请求，从而形成"生成—评估—建议—调整—再生成"的完整质量改进闭环。该闭环机制使得系统具备持续优化输出质量的能力，显著提升了数字人视频生成的整体效果。

综上，质量评价模型通过多任务学习框架实现了对生成视频的多维度联合评估，并通过在线评估与参数优化建议的闭环机制，为系统的质量持续改进提供了有效支撑。

## 4.3 系统测试

系统测试是验证数字人生成系统功能正确性和运行稳定性的重要环节。本节通过设计系统化的测试方案，对系统各主要模块进行功能验证，确保系统实现满足设计需求。

### 4.3.1 模块功能测试

本小节围绕系统的主要功能模块，分别从测试环境、测试方法、测试用例设计及测试结果分析等方面进行详细阐述。

**（一）测试环境**

系统测试在以下软硬件环境中进行：操作系统为Ubuntu Linux，GPU为NVIDIA显卡（显存十六GB），CUDA版本为11.3及以上，Python版本为3.8及以上，PyTorch版本为1.10及以上，前端运行环境为Node.js。后端服务部署于本地开发服务器，前端通过本地开发环境访问后端API接口。测试过程中使用了PyTest作为主要的自动化测试框架。

**（二）评价模型模块测试**

评价模型模块的测试主要采用单元测试和冒烟测试相结合的方法，验证模型的前向推理、损失计算、模型序列化与反序列化等核心功能。

如表4-2所示，模型模块共设计了五组核心测试用例。

**表4-2 评价模型模块测试用例**

| 测试编号 | 测试项目 | 测试方法 | 测试输入 | 预期结果 | 实际结果 |
|---------|---------|---------|---------|---------|---------|
| T-M-01 | 模型前向推理 | 单元测试 | 随机生成的多模态特征张量（批量大小为2，序列长度为10） | 输出五个维度的评分张量，每个形状为(2,1) | 通过，输出形状与预期一致 |
| T-M-02 | 损失函数计算 | 单元测试 | 模型输出与随机目标标签 | 返回有效的标量损失值，数值有限且非空 | 通过，损失值正常 |
| T-M-03 | 模型保存与加载 | 单元测试 | 训练后的模型检查点文件 | 加载后模型参数与保存前完全一致 | 通过，参数一致性验证成功 |
| T-M-04 | 高级模型冒烟测试 | 冒烟测试 | 标准配置下的模型实例化与前向推理 | 模型正常实例化，推理无异常 | 通过 |
| T-M-05 | 基础前向传播冒烟测试 | 冒烟测试 | 最小化输入的前向传播 | 模型正常完成前向传播 | 通过 |

在模型前向推理测试中，测试程序根据配置文件中定义的各模态特征维度（视觉特征二千零四十八维、音频特征七百六十八维、面部关键点特征一千四百零四维、动作单元特征若干维），构造符合模型输入规格的随机张量，送入模型进行前向推理。测试验证模型能够正确输出五个维度的评分结果，每个评分输出的张量形状均为批量大小乘以一，且所有输出值均为有限实数。

在损失函数计算测试中，测试程序将模型前向推理的输出与随机生成的目标标签一同传入多任务加权损失函数，验证损失值的计算结果为有效的标量值，不存在数值溢出或空值异常。

在模型保存与加载测试中，测试程序首先将模型参数保存至临时检查点文件，随后创建新的模型实例并从检查点文件加载参数，逐一对比两个模型实例的所有参数张量，验证其数值完全一致，确保模型的序列化与反序列化过程不会引入参数偏差。

**（三）视频生成流水线集成测试**

视频生成流水线的集成测试重点验证各阶段模块之间的接口调用与数据传递的正确性。测试方案如表4-3所示。

**表4-3 视频生成流水线集成测试用例**

| 测试编号 | 测试项目 | 测试方法 | 测试输入 | 预期结果 | 实际结果 |
|---------|---------|---------|---------|---------|---------|
| T-P-01 | 图像上传与预处理 | 功能测试 | 标准正面人脸图像 | 成功存储并返回文件标识符 | 通过 |
| T-P-02 | 语音合成接口 | 功能测试 | 中文文本与参考音频 | 生成语音文件，音频时长合理 | 通过 |
| T-P-03 | SadTalker推理 | 集成测试 | 人脸图像与语音文件 | 生成MP4视频文件，时长与音频一致 | 通过 |
| T-P-04 | Wav2Lip推理 | 集成测试 | 人脸图像与语音文件 | 生成MP4视频文件，唇形同步 | 通过 |
| T-P-05 | 完整流水线端到端 | 端到端测试 | 人脸图像与文本输入 | 完成全流程并输出带评分的视频 | 通过 |

端到端测试覆盖了从用户上传图像到最终获取带有质量评分的生成视频的完整流程，验证了前端调用链路中十一个步骤的顺序执行与数据传递的正确性，包括图像上传、配置设定、音频上传、推理触发、任务状态轮询、视频下载等环节。

**（四）前端功能测试**

前端功能测试主要通过手工测试方式验证用户界面的交互功能与信息展示的正确性。测试内容包括：人脸图像上传界面的文件格式校验与预览功能、参数配置面板的选项切换与输入验证、视频生成进度条的实时更新与时间估算、质量评分结果的可视化展示以及视频播放功能的流畅性。

如图4-4所示，前端界面能够正确展示视频生成进度信息，并在生成完成后显示各维度的质量评分结果。

**图4-4 前端质量评分结果展示界面截图**（建议插入一张前端界面截图，展示视频播放区域和各维度评分数值的展示面板，包括唇形同步、表情质量、音频质量、跨模态一致性和综合评分的具体分数。）

**（五）测试结果分析**

经过上述多层次的系统测试，各模块功能测试用例全部通过，系统主要功能运行正常。模型模块的单元测试覆盖了前向推理、损失计算和模型持久化三项核心功能，冒烟测试进一步确认了模型在标准配置下的基本可用性。集成测试验证了视频生成流水线各阶段的接口兼容性与数据一致性，端到端测试证实了系统在完整业务场景下的功能正确性。

当前测试仍存在以下不足：一是自动化测试的覆盖范围主要集中在模型模块，视频生成流水线和前端功能尚以手工测试为主，后续可引入自动化集成测试框架进一步提升测试效率；二是性能测试方面尚未进行大规模并发场景的压力测试，系统在高并发条件下的稳定性有待进一步验证；三是边界条件测试可进一步完善，例如异常格式输入、超大文件上传、网络中断恢复等场景的测试用例可适当补充。

综上所述，本节通过系统化的测试方案对数字人生成系统的各主要模块进行了功能验证，测试结果表明系统实现达到了预期的设计目标，各模块功能正确、接口协调、运行稳定，能够满足数字人视频生成与质量评价的业务需求。
