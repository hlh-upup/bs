@article{rakesh2025survey,
  author    = {Rakesh, V K and others},
  title     = {Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions},
  journal   = {arXiv preprint},
  year      = {2025},
  url       = {https://arxiv.org/abs/2507.02900},
  note      = {(2025-06-23)[2026-03-01]}
}

@article{netland2025comparing,
  author    = {Netland, T and others},
  title     = {Comparing human-made and AI-generated teaching videos: An experimental study on learning effects},
  journal   = {Computers \& Education},
  year      = {2025},
  volume    = {194},
  pages     = {104703}
}

@article{zhen2023hci,
  author    = {Zhen, R and others},
  title     = {Human-Computer Interaction System: A Survey of Talking-Head Generation},
  journal   = {Electronics},
  year      = {2023},
  volume    = {12},
  number    = {1},
  pages     = {218}
}

@inproceedings{zhang2023sadtalker,
  author    = {Zhang, W and Cun, X and Wang, X and others},
  title     = {SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2023},
  pages     = {8652--8661}
}

@inproceedings{su2025quality,
  author    = {Su, M and Liu, Y and Zhang, H and others},
  title     = {Quality Assessment for Talking Head Videos via Multi-modal Feature Representation},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  year      = {2025}
}

@inproceedings{zhou2025talker,
  author    = {Zhou, Y and Li, X and Wang, Z and others},
  title     = {Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year      = {2025}
}

@misc{moe2024digital,
  author    = {{Ministry of Education of the People's Republic of China}},
  title     = {Overview of work on digital education in China},
  year      = {2024},
  url       = {http://en.moe.gov.cn/features/2024WorldDigitalEducationConference/News/202402/t20240201_1113777.html},
  note      = {(2024-02-01)[2026-03-01]}
}

@article{rakesh2026advancements,
  author    = {Rakesh, V K and others},
  title     = {Advancements in talking head generation: a comprehensive review of techniques, metrics, and challenges},
  journal   = {The Visual Computer},
  year      = {2026},
  volume    = {42},
  pages     = {9},
  doi       = {10.1007/s00371-025-04232-w}
}

@article{lezheng2025survey,
  author    = {Le Zheng and Hu Yongting and Xu Yong},
  title     = {Survey of Audio-Driven Talking Face Video Generation and Identification},
  journal   = {Journal of Computer Research and Development},
  year      = {2025},
  volume    = {62},
  number    = {10},
  pages     = {2523--2544}
}

@article{bai2025survey,
  author    = {Bai, X and others},
  title     = {A Survey on Audio-Driven Talking Face Generation},
  journal   = {IEEE Transactions on Multimedia},
  year      = {2025}
}

@article{wu2023audio,
  author    = {Wu, R and others},
  title     = {Audio-driven talking face generation with diverse yet realistic facial animations},
  journal   = {Pattern Recognition},
  year      = {2023},
  volume    = {144},
  pages     = {109856}
}

@article{jiang2024audio,
  author    = {Jiang, D and others},
  title     = {Audio-Driven Facial Animation with Deep Learning: A Survey},
  journal   = {Information},
  year      = {2024},
  volume    = {15},
  number    = {11},
  pages     = {675}
}

@inproceedings{prajwal2023lipsync3d,
  author    = {Prajwal, K R and others},
  title     = {LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2023}
}

@article{zhou2026mi3s,
  author    = {Zhou, Y and others},
  title     = {MI3S: A multimodal large language model assisted quality assessment framework for AI-generated talking heads},
  journal   = {Information Processing \& Management},
  year      = {2026},
  volume    = {63},
  number    = {3},
  pages     = {103456}
}

@misc{gptsovits2024,
  author    = {{GPT-SoVITS Team}},
  title     = {GPT-SoVITS: Few-shot Voice Cloning with GPT and SoVITS},
  year      = {2024},
  url       = {https://github.com/RVC-Boss/GPT-SoVITS},
  note      = {(2024)[2026-03-01]}
}

@inproceedings{wang2025emotivetalk,
  author    = {Wang, X and others},
  title     = {EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2025}
}

@inproceedings{thies2016face2face,
  author    = {Thies, J and Zollhofer, M and Stamminger, M and others},
  title     = {Face2Face: Real-time Face Capture and Reenactment of RGB Videos},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2016},
  pages     = {2387--2395}
}

@article{suwajanakorn2017obama,
  author    = {Suwajanakorn, S and Seitz, S M and Kemelmacher-Shlizerman, I},
  title     = {Synthesizing Obama: Learning Lip Sync from Audio},
  journal   = {ACM Transactions on Graphics},
  year      = {2017},
  volume    = {36},
  number    = {4},
  pages     = {95}
}

@inproceedings{prajwal2020wav2lip,
  author    = {Prajwal, K R and Mukhopadhyay, R and Namboodiri, V P and others},
  title     = {A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild},
  booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
  year      = {2020},
  pages     = {1505--1514}
}

@inproceedings{wang2021audio2head,
  author    = {Wang, S and Li, L and Ding, Z and others},
  title     = {Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
  year      = {2021},
  pages     = {1098--1105}
}

@inproceedings{vaswani2017attention,
  author    = {Vaswani, A and Shazeer, N and Parmar, N and others},
  title     = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {5998--6008}
}

@inproceedings{lu2019vilbert,
  author    = {Lu, J and Batra, D and Parikh, D and others},
  title     = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2019},
  pages     = {13--23}
}

@inproceedings{chen2020uniter,
  author    = {Chen, Y C and Li, L and Yu, L and others},
  title     = {UNITER: UNiversal Image-TExt Representation Learning},
  booktitle = {European Conference on Computer Vision},
  year      = {2020},
  pages     = {104--120}
}

@inproceedings{tsai2019multimodal,
  author    = {Tsai, Y H H and Bai, S and Liang, P P and others},
  title     = {Multimodal Transformer for Unaligned Multimodal Language Sequences},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year      = {2019},
  pages     = {6558--6569}
}

@inproceedings{kendall2018multitask,
  author    = {Kendall, A and Gal, Y and Cipolla, R},
  title     = {Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2018},
  pages     = {7482--7491}
}

@inproceedings{kendall2017uncertainties,
  author    = {Kendall, A and Gal, Y},
  title     = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {5574--5584}
}

@inproceedings{chen2018gradnorm,
  author    = {Chen, Z and Badrinarayanan, V and Lee, C Y and others},
  title     = {GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks},
  booktitle = {International Conference on Machine Learning},
  year      = {2018},
  pages     = {794--803}
}

@inproceedings{liu2019endtoend,
  author    = {Liu, S and Johns, E and Davison, A J},
  title     = {End-to-End Multi-Task Learning with Attention},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2019},
  pages     = {1871--1880}
}

@inproceedings{micikevicius2018mixed,
  author    = {Micikevicius, P and Narang, S and Alben, J and others},
  title     = {Mixed Precision Training},
  booktitle = {International Conference on Learning Representations},
  year      = {2018}
}

@article{zhang2022survey,
  author    = {Zhang, Y and Yang, Q},
  title     = {A Survey on Multi-Task Learning},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  year      = {2022},
  volume    = {34},
  number    = {12},
  pages     = {5586--5609}
}

@article{ruder2017overview,
  author    = {Ruder, S},
  title     = {An Overview of Multi-Task Learning in Deep Neural Networks},
  journal   = {arXiv preprint arXiv:1706.05098},
  year      = {2017}
}
